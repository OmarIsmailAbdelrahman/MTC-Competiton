{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8805495,"sourceType":"datasetVersion","datasetId":5295827},{"sourceId":8814483,"sourceType":"datasetVersion","datasetId":5238724},{"sourceId":8817445,"sourceType":"datasetVersion","datasetId":5304348},{"sourceId":8826380,"sourceType":"datasetVersion","datasetId":5310362},{"sourceId":8827752,"sourceType":"datasetVersion","datasetId":5304606},{"sourceId":8844110,"sourceType":"datasetVersion","datasetId":5323049},{"sourceId":8844356,"sourceType":"datasetVersion","datasetId":5323219}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries Installation & Imports ðŸ“¦ðŸ”Œ","metadata":{}},{"cell_type":"markdown","source":"## Libraries Installations ðŸ› ï¸\n### Installing necessary libraries required for the notebook's operations.","metadata":{}},{"cell_type":"code","source":"!pip install wget\n!apt-get -y install sox libsndfile1 ffmpeg\n!pip install text-unidecode\n!pip install matplotlib>=3.3.2\n!pip install aiohttp==3.9.2\n!pip install boto3 --upgrade","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T22:08:54.315266Z","iopub.execute_input":"2024-07-02T22:08:54.316151Z","iopub.status.idle":"2024-07-02T22:09:42.718953Z","shell.execute_reply.started":"2024-07-02T22:08:54.316113Z","shell.execute_reply":"2024-07-02T22:09:42.717902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BRANCH = 'r2.0.0rc0'\n!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T22:09:42.720826Z","iopub.execute_input":"2024-07-02T22:09:42.721131Z","iopub.status.idle":"2024-07-02T22:12:40.671051Z","shell.execute_reply.started":"2024-07-02T22:09:42.721102Z","shell.execute_reply":"2024-07-02T22:12:40.670052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Libraries ðŸ“š\n### Importing the installed libraries into the notebook to use their functionalities.","metadata":{}},{"cell_type":"code","source":"import os\nimport nemo\nimport json\nimport wandb\nimport librosa\nimport pandas as pd\nimport nemo.collections.asr as nemo_asr\nfrom omegaconf import OmegaConf, open_dict","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:12:40.672380Z","iopub.execute_input":"2024-07-02T22:12:40.672691Z","iopub.status.idle":"2024-07-02T22:12:41.048758Z","shell.execute_reply.started":"2024-07-02T22:12:40.672662Z","shell.execute_reply":"2024-07-02T22:12:41.047855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper-Parameter Tuning ðŸŽ›ï¸âš™ï¸","metadata":{}},{"cell_type":"markdown","source":"## Tuning Tokenizer ðŸ”§ðŸ“\n### Adjusting the tokenizer settings to optimize its performance for the given task.","metadata":{}},{"cell_type":"code","source":"if not os.path.exists(\"scripts/tokenizers/process_asr_text_tokenizer.py\"):\n    !mkdir scripts\n    !wget -P scripts/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tokenizers/process_asr_text_tokenizer.py","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:12:41.050622Z","iopub.execute_input":"2024-07-02T22:12:41.050991Z","iopub.status.idle":"2024-07-02T22:12:43.140171Z","shell.execute_reply.started":"2024-07-02T22:12:41.050965Z","shell.execute_reply":"2024-07-02T22:12:43.139102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ./scripts/process_asr_text_tokenizer.py \\\n  --manifest='/kaggle/input/aic-manifests/train_manifest.json' \\\n  --data_root=\"/kaggle/working/tokinzers/sus\" \\\n  --vocab_size=128 \\\n  --tokenizer=\"spe\" \\\n  --no_lower_case \\\n  --spe_type=\"unigram\" \\\n  --log","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T22:12:43.141633Z","iopub.execute_input":"2024-07-02T22:12:43.141928Z","iopub.status.idle":"2024-07-02T22:13:04.207813Z","shell.execute_reply.started":"2024-07-02T22:12:43.141901Z","shell.execute_reply":"2024-07-02T22:13:04.206673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir configs\nBRANCH = 'r2.0.0rc0'\n!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/conformer/conformer_ctc_bpe.yaml","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:14:26.675446Z","iopub.execute_input":"2024-07-02T22:14:26.675938Z","iopub.status.idle":"2024-07-02T22:14:28.738453Z","shell.execute_reply.started":"2024-07-02T22:14:26.675911Z","shell.execute_reply":"2024-07-02T22:14:28.737508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git clone https://github.com/NVIDIA/NeMo.git","metadata":{"execution":{"iopub.status.busy":"2024-07-02T16:37:53.319695Z","iopub.execute_input":"2024-07-02T16:37:53.320574Z","iopub.status.idle":"2024-07-02T16:38:06.962254Z","shell.execute_reply.started":"2024-07-02T16:37:53.320534Z","shell.execute_reply":"2024-07-02T16:38:06.961114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Folders to Working ðŸ“‚âž¡ï¸ðŸ—ï¸\n### Setting up the directory structure and loading necessary folders for the project.","metadata":{}},{"cell_type":"code","source":"!cp -r /kaggle/input/sussy-baka/NeMo \"/kaggle/working/\"\n!cp -r /kaggle/input/sussy-baka/configs \"/kaggle/working/\"\n!cp -r /kaggle/input/sussy-baka/results \"/kaggle/working/\"\n!cp -r /kaggle/input/sussy-baka/scripts \"/kaggle/working/\"\n!cp -r /kaggle/input/sussy-baka/tokinzers \"/kaggle/working/\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:23:39.062631Z","iopub.execute_input":"2024-07-02T22:23:39.063585Z","iopub.status.idle":"2024-07-02T22:25:20.150932Z","shell.execute_reply.started":"2024-07-02T22:23:39.063550Z","shell.execute_reply":"2024-07-02T22:25:20.149405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning conformer_ctc_bpe ðŸ”§ðŸ§©\n### Fine-tuning the Conformer CTC model with BPE (Byte-Pair Encoding) to enhance its performance.","metadata":{}},{"cell_type":"code","source":"params = OmegaConf.load(\"/kaggle/working/configs/conformer_ctc_bpe.yaml\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:50:18.892705Z","iopub.execute_input":"2024-07-02T23:50:18.893444Z","iopub.status.idle":"2024-07-02T23:50:18.944414Z","shell.execute_reply.started":"2024-07-02T23:50:18.893412Z","shell.execute_reply":"2024-07-02T23:50:18.943723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuration updates\nparams.trainer.precision=32 # this set precision of the model to values [32,16]\nparams.model.encoder.dropout = 0.2\nparams.model.encoder.dropout_pre_encoder = 0.2\nparams.model.encoder.dropout_emb = 0.1\nparams.model.train_ds.batch_size=4","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:50:25.010224Z","iopub.execute_input":"2024-07-02T23:50:25.010616Z","iopub.status.idle":"2024-07-02T23:50:25.017419Z","shell.execute_reply.started":"2024-07-02T23:50:25.010585Z","shell.execute_reply":"2024-07-02T23:50:25.016524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OmegaConf.save(params, \"/kaggle/working/NeMo/examples/asr/conf/conformer/conformer_ctc_bpe.yaml\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:50:26.874244Z","iopub.execute_input":"2024-07-02T23:50:26.874629Z","iopub.status.idle":"2024-07-02T23:50:26.895581Z","shell.execute_reply.started":"2024-07-02T23:50:26.874599Z","shell.execute_reply":"2024-07-02T23:50:26.894296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Opening WanDB ðŸŒðŸ“Š\n### Initiating a connection with Weights and Biases (WanDB) for experiment tracking and visualization.","metadata":{}},{"cell_type":"code","source":"WANDB_API = 'ffbdfe896293a7e939b6775de3fe55e2abd1a0fc'\nwandb.login(key=WANDB_API)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:20.176399Z","iopub.execute_input":"2024-07-02T22:25:20.176687Z","iopub.status.idle":"2024-07-02T22:25:26.005845Z","shell.execute_reply.started":"2024-07-02T22:25:20.176663Z","shell.execute_reply":"2024-07-02T22:25:26.004925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding FAdam âž•âš¡\n### Integrating the FAdam optimizer to improve the training efficiency of the model.","metadata":{}},{"cell_type":"code","source":"write = '''import types\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nimport pytorch_lightning as L\nfrom pytorch_lightning.utilities.types import OptimizerLRScheduler\nfrom torch.optim import Optimizer\n\nfrom nemo.lightning.megatron_parallel import CallbackMethods\n\n\nclass LRSchedulerModule(L.Callback, CallbackMethods, ABC):\n    \"\"\"A module to standardize the learning rate scheduler setup and configuration.\n\n    This class decouples the learning rate scheduler from the model, similar to how the LightningDataModule\n    decouples data handling. It also acts as a Callback to hook into the training loop, which can be useful\n    for adding custom all-reduces, logging, early stopping, etc. Next to that standard Lightning callback-event,\n    this also supports hooking into the Megatron forward-backward function at a granular level.\n\n    Example::\n\n        class MyLRSchedulerModule(LRSchedulerModule):\n            def setup(self, model, optimizer):\n                # Custom setup logic\n                ...\n\n            def scheduler(self, model, optimizers):\n                # Define and return the learning rate scheduler\n                ...\n\n    Methods:\n        setup(model, optimizer): Sets up the learning rate scheduler.\n        scheduler(model, optimizers): Abstract method to define the learning rate scheduler.\n        __call__(model, optimizers): Calls the setup and scheduler methods.\n    \"\"\"\n\n    def connect(self, model, optimizer) -> None:\n        \"\"\"Sets up the learning rate scheduler.\n\n        Args:\n            model: The model for which the scheduler is being set up.\n            optimizer: The optimizer for which the scheduler is being set up.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def scheduler(self, model, optimizers) -> OptimizerLRScheduler:\n        \"\"\"Abstract method to define the learning rate scheduler.\n\n        Args:\n            model: The model for which the scheduler is being defined.\n            optimizers: The optimizers for which the scheduler is being defined.\n\n        Returns:\n            OptimizerLRScheduler: The learning rate scheduler.\n        \"\"\"\n        raise NotImplementedError(\"The scheduler method should be implemented by subclasses.\")\n\n    def __call__(self, model, optimizers):\n        \"\"\"Calls the setup and scheduler methods.\n\n        Args:\n            model: The model for which the scheduler is being called.\n            optimizers: The optimizers for which the scheduler is being called.\n\n        Returns:\n            OptimizerLRScheduler: The learning rate scheduler.\n        \"\"\"\n\n        self.connect(model, optimizers)\n\n        self._scheduler = self.scheduler(model, optimizers)\n\n        if not isinstance(self._scheduler, (dict, tuple)):\n            return optimizers, self._scheduler\n\n        return self._scheduler\n\n\nclass OptimizerModule(L.Callback, CallbackMethods, ABC):\n    \"\"\"A module to standardize the optimizer setup and configuration.\n\n    This class decouples the optimizer from the model, similar to how the LightningDataModule\n    decouples data handling. It also acts as a Callback to hook into the training loop, which can be useful\n    for adding custom all-reduces, logging, early stopping, etc. Next to that standard Lightning callback-event,\n    this also supports hooking into the Megatron forward-backward function at a granular level.\n\n    Attributes:\n        lr_scheduler (Optional[LRSchedulerModule]): The learning rate scheduler module.\n\n    Example::\n\n        class MyOptimizerModule(OptimizerModule):\n            def __init__(self, lr_scheduler=None):\n                super().__init__(lr_scheduler)\n\n            def setup(self, model):\n                # Custom setup logic\n                ...\n\n            def optimizers(self, model):\n                # Define and return the optimizers\n                ...\n\n    Methods:\n        connect(model, trainer): Connects the optimizer module to the model and trainer.\n        setup(model): Sets up the optimizer.\n        optimizers(model): Abstract method to define the optimizers.\n        __call__(model, megatron_parallel): Calls the setup and optimizers methods.\n    \"\"\"\n\n    def __init__(self, lr_scheduler: Optional[LRSchedulerModule]):\n        \"\"\"Initializes the OptimizerModule.\n\n        Args:\n            lr_scheduler (Optional[LRSchedulerModule]): The learning rate scheduler module.\n        \"\"\"\n        self.lr_scheduler = lr_scheduler\n\n    def connect(self, model: L.LightningModule) -> None:\n        \"\"\"Connects the optimizer module to the model and trainer.\n\n        Args:\n            model (L.LightningModule): The model to which the optimizer module is being connected.\n        \"\"\"\n\n        def custom_configure_optimizers(lightning_module_self, megatron_parallel=None):\n            opt = self(lightning_module_self, megatron_parallel=megatron_parallel)\n            return opt\n\n        model.configure_optimizers = types.MethodType(custom_configure_optimizers, model)\n        model.optim = self\n\n        if hasattr(self, \"__io__\") and hasattr(model, \"__io__\"):\n            if hasattr(model.__io__, \"optim\"):\n                model.__io__.optim = self.__io__\n\n    @abstractmethod\n    def optimizers(self, model) -> List[Optimizer]:\n        \"\"\"Abstract method to define the optimizers.\n\n        Args:\n            model: The model for which the optimizers are being defined.\n\n        Returns:\n            List[Optimizer]: The list of optimizers.\n        \"\"\"\n        raise NotImplementedError(\"The optimizers method should be implemented by subclasses.\")\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx) -> None:\n        if self._optimizers is not None:\n            lr = self._optimizers[0].param_groups[0]['lr']\n            pl_module.log('lr', lr, rank_zero_only=True, batch_size=1)\n\n    def __call__(self, model: L.LightningModule, megatron_parallel=None) -> OptimizerLRScheduler:\n        \"\"\"Calls the setup and optimizers methods.\n\n        Args:\n            model (L.LightningModule): The model for which the optimizers are being called.\n            megatron_parallel: Optional parallel model.\n\n        Returns:\n            OptimizerLRScheduler: The optimizers and optionally the learning rate scheduler.\n        \"\"\"\n        _model = model if megatron_parallel is None else megatron_parallel\n        callbacks = _model.trainer.callbacks\n        if self not in callbacks:\n            callbacks.append(self)\n        if self.lr_scheduler is not None and self.lr_scheduler not in callbacks:\n            callbacks.append(self.lr_scheduler)\n\n        self._optimizers = self.optimizers(_model)\n\n        _opt = self._optimizers[0] if len(self._optimizers) == 1 else self._optimizers\n\n        if self.lr_scheduler is not None:\n            with_scheduler = self.lr_scheduler(_model, _opt)\n\n            return with_scheduler\n\n        return self._optimizers\n\n\nclass MyOptimizerModule(OptimizerModule):\n    def optimizers(self, model) -> List[Optimizer]:\n        optimizer = fAdam(model.parameters(), lr=5.0, weight_decay=5e-4, betas=(0.9, 0.98))\n        print(\"From fAdam\")\n        return [optimizer]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/lightning/pytorch/optim/base.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.008588Z","iopub.execute_input":"2024-07-02T22:25:26.009123Z","iopub.status.idle":"2024-07-02T22:25:26.021102Z","shell.execute_reply.started":"2024-07-02T22:25:26.009096Z","shell.execute_reply":"2024-07-02T22:25:26.020232Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write = '''from typing import Any, Callable, List, Mapping, Optional\n\nimport pytorch_lightning as pl\nfrom megatron.core.distributed import finalize_model_grads\nfrom megatron.core.optimizer import OptimizerConfig, get_megatron_optimizer\nfrom megatron.core.utils import get_model_config\nfrom torch.optim import Optimizer\n\nfrom nemo.lightning.megatron_parallel import MegatronParallel\nfrom nemo.lightning.pytorch.optim.base import LRSchedulerModule, MyOptimizerModule\n\n\nclass MegatronOptimizerModule(MyOptimizerModule):\n    \"\"\"A MyOptimizerModule for the megatron optimizers.\n\n    Attributes:\n        config (OptimizerConfig): Configuration for the optimizer.\n        no_weight_decay_cond (Optional[Callable]): Condition for no weight decay.\n        scale_lr_cond (Optional[Callable]): Condition for scaling learning rate.\n        lr_mult (float): Learning rate multiplier.\n\n    Example::\n\n        config = OptimizerConfig(...)\n        lr_scheduler = MyLRSchedulerModule(...)\n        optimizer_module = MegatronOptimizerModule(config, lr_scheduler)\n\n    Methods:\n        setup(model): Sets up the optimizer.\n        optimizers(model): Defines the optimizers.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: OptimizerConfig,\n        lr_scheduler: Optional[LRSchedulerModule] = None,\n        no_weight_decay_cond: Optional[Callable] = None,\n        scale_lr_cond: Optional[Callable] = None,\n        lr_mult: float = 1.0,\n    ):\n        \"\"\"Initializes the MegatronOptimizerModule.\n\n        Args:\n            config (OptimizerConfig): Configuration for the optimizer.\n            lr_scheduler (Optional[LRSchedulerModule]): The learning rate scheduler module.\n            no_weight_decay_cond (Optional[Callable]): Condition for no weight decay.\n            scale_lr_cond (Optional[Callable]): Condition for scaling learning rate.\n            lr_mult (float): Learning rate multiplier.\n        \"\"\"\n\n        super().__init__(lr_scheduler=lr_scheduler)\n        self.config = config\n        self.no_weight_decay_cond = no_weight_decay_cond\n        self.scale_lr_cond = scale_lr_cond\n        self.lr_mult = lr_mult\n\n    def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str):\n        \"\"\"We will add the finalize_model_grads function to the model config.\n\n        Args:\n            model: The model for which the optimizer is being set up.\n        \"\"\"\n\n        def finalize_model_grads_func(*args, **kwargs):\n            return self.finalize_model_grads(*args, **kwargs)\n\n        get_model_config(pl_module).finalize_model_grads_func = finalize_model_grads_func\n\n    def optimizers(self, model: MegatronParallel) -> List[Optimizer]:\n        \"\"\"Defines the optimizers.\n\n        Args:\n            model (MegatronParallel): The model for which the optimizers are being defined.\n\n        Returns:\n            List[Optimizer]: The list of optimizers.\n\n        Raises:\n            ValueError: If the model is not an instance of MegatronParallel.\n        \"\"\"\n\n        if not isinstance(model, MegatronParallel):\n            raise ValueError(\"Model must be an instance of MegatronParallel\")\n\n        from nemo.core.optim import McoreDistributedOptimizer\n\n        class McoreOpt(McoreDistributedOptimizer):\n            def sharded_state_dict(\n                self,\n                model_sharded_state_dict,\n                optimizer_state_dict=None,\n                is_loading=False,\n                # dist_ckpt_parallel_save=False, ## TODO: fix!\n            ):\n                # sharding_type = 'fully_sharded_model_space' if dist_ckpt_parallel_save else 'dp_zero_gather_scatter'\n                sharding_type = 'dp_zero_gather_scatter'\n                state_dict = self.mcore_optimizer.sharded_state_dict(\n                    model_sharded_state_dict, is_loading=is_loading, sharding_type=sharding_type\n                )\n                return state_dict\n\n        mcore_opt = get_megatron_optimizer(\n            self.config,\n            list(model),\n            no_weight_decay_cond=self.no_weight_decay_cond,\n            scale_lr_cond=self.scale_lr_cond,\n            lr_mult=self.lr_mult,\n        )\n\n        return [McoreOpt(mcore_opt)]\n\n    def finalize_model_grads(self, *args, **kwargs):\n        return finalize_model_grads(*args, **kwargs)\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/lightning/pytorch/optim/megatron.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.022275Z","iopub.execute_input":"2024-07-02T22:25:26.022580Z","iopub.status.idle":"2024-07-02T22:25:26.038583Z","shell.execute_reply.started":"2024-07-02T22:25:26.022547Z","shell.execute_reply":"2024-07-02T22:25:26.037835Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write = '''from typing import Union\n\nfrom lightning_fabric.plugins.environments import slurm\nfrom pytorch_lightning import plugins as _pl_plugins\n\n# This is here to import it once, which improves the speed of launch when in debug-mode\ntry:\n    import transformer_engine  # noqa\nexcept ImportError:\n    pass\n\nfrom nemo.lightning.base import get_vocab_size, teardown\nfrom nemo.lightning.nemo_logger import NeMoLogger\nfrom nemo.lightning.pytorch.callbacks.megatron_model_checkpoint import ModelCheckpoint\nfrom nemo.lightning.pytorch.optim import LRSchedulerModule, MegatronOptimizerModule, MyOptimizerModule\nfrom nemo.lightning.pytorch.plugins import MegatronDataSampler, MegatronMixedPrecision\nfrom nemo.lightning.pytorch.plugins import data_sampler as _data_sampler\nfrom nemo.lightning.pytorch.strategies import MegatronStrategy\nfrom nemo.lightning.pytorch.trainer import Trainer\nfrom nemo.lightning.resume import AutoResume\n\n\n# We monkey patch because nvidia uses a naming convention for SLURM jobs\ndef _is_slurm_interactive_mode():\n    job_name = slurm.SLURMEnvironment.job_name()\n    return job_name is None or job_name.endswith(\"bash\") or job_name.endswith(\"interactive\")\n\n\nslurm._is_slurm_interactive_mode = _is_slurm_interactive_mode  # noqa: SLF001\n\n\n_pl_plugins._PLUGIN_INPUT = Union[_pl_plugins._PLUGIN_INPUT, _data_sampler.DataSampler]  # noqa: SLF001\n\n\n__all__ = [\n    \"AutoResume\",\n    \"LRSchedulerModule\",\n    \"MegatronStrategy\",\n    \"MegatronDataSampler\",\n    \"MegatronMixedPrecision\",\n    \"MegatronOptimizerModule\",\n    \"NeMoLogger\",\n    \"ModelCheckpoint\",\n    \"MyOptimizerModule\",\n    \"Trainer\",\n    \"get_vocab_size\",\n    \"teardown\",\n]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/lightning/__init__.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.039564Z","iopub.execute_input":"2024-07-02T22:25:26.039794Z","iopub.status.idle":"2024-07-02T22:25:26.052588Z","shell.execute_reply.started":"2024-07-02T22:25:26.039773Z","shell.execute_reply":"2024-07-02T22:25:26.051845Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write = '''from pathlib import Path\nfrom typing import Callable, Optional\n\nimport pytorch_lightning as pl\nfrom typing_extensions import Annotated\n\nfrom nemo.collections.llm.utils import Config, task\nfrom nemo.lightning import AutoResume, MegatronStrategy, NeMoLogger, MyOptimizerModule, Trainer, io, teardown\n\n\n@task(namespace=\"llm\")\ndef train(\n    model: pl.LightningModule,\n    data: pl.LightningDataModule,\n    trainer: Trainer,\n    log: Annotated[Optional[NeMoLogger], Config[NeMoLogger]] = None,\n    resume: Annotated[Optional[AutoResume], Config[AutoResume]] = None,\n    optim: Optional[MyOptimizerModule] = None,\n    tokenizer: Optional[str] = None,\n    # TODO: Fix export export: Optional[str] = None,\n) -> Path:\n    \"\"\"\n    Trains a model using the specified data and trainer, with optional tokenizer, source, and export.\n\n    Args:\n        model (pl.LightningModule): The model to be trained.\n        data (pl.LightningDataModule): The data module containing training data.\n        trainer (Trainer): The trainer instance configured with a MegatronStrategy.\n        log (NeMoLogger): A nemologger instance.\n        resume (Optional[Union[AutoResume, Resume]]): Resume training from a checkpoint.\n        optim (Optional[OptimizerModule]): The optimizer module to be used. If not provided, the default optimizer\n            from the model will be used.\n        tokenizer (Optional[str]): Tokenizer setting to be applied. Can be 'data' or 'model'.\n        export (Optional[str]): Filename to save the exported checkpoint after training.\n\n    Returns\n    -------\n        Path: The directory path where training artifacts are saved.\n\n    Raises\n    ------\n        ValueError: If the trainer's strategy is not MegatronStrategy.\n\n    Examples\n    --------\n        >>> model = MyModel()\n        >>> data = MyDataModule()\n        >>> trainer = Trainer(strategy=MegatronStrategy())\n        >>> train(model, data, trainer, tokenizer='data', source='path/to/ckpt.ckpt', export='final.ckpt')\n        PosixPath('/path/to/log_dir')\n    \"\"\"\n    _log = log or NeMoLogger()\n    app_state = _log.setup(\n        trainer,\n        resume_if_exists=getattr(resume, \"resume_if_exists\", False),\n        task_config=getattr(train, \"__io__\", None),\n    )\n    if resume is not None:\n        resume.setup(model, trainer)\n    if optim:\n        optim.connect(model)\n    if tokenizer:  # TODO: Improve this\n        _use_tokenizer(model, data, tokenizer)\n\n    trainer.fit(model, data)\n\n    _log.teardown()\n\n    return app_state.exp_dir\n\n\n@task(namespace=\"llm\")\ndef pretrain(\n    model: pl.LightningModule,\n    data: pl.LightningDataModule,\n    trainer: Trainer,\n    source: Optional[str] = None,\n    # export: Optional[str] = None\n) -> Path:\n    return train(model=model, data=data, trainer=trainer, tokenizer=\"data\", source=source)\n\n\n@task(namespace=\"llm\")\ndef validate(\n    model: pl.LightningModule,\n    data: pl.LightningDataModule,\n    trainer: Trainer,\n    tokenizer: Optional[str] = None,\n    source: Optional[str] = None,\n    export: Optional[str] = None,\n) -> Path:\n    if not isinstance(trainer.strategy, MegatronStrategy):\n        raise ValueError(\"Only MegatronStrategy is supported\")\n\n    validate_kwargs = {}\n    run_dir = Path(trainer.logger.log_dir)\n    export_dir = run_dir / \"export\"\n\n    if tokenizer:  # TODO: Improve this\n        _use_tokenizer(model, data, tokenizer)\n    if source:\n        _add_ckpt_path(source, model, validate_kwargs)\n\n    trainer.validate(model, data, **validate_kwargs)\n    trainer.save_checkpoint(export_dir)\n    if export:\n        teardown(trainer)\n        del trainer, model, data\n        export_ckpt(export_dir, export)\n\n    return run_dir\n\n\n@task(name=\"import\", namespace=\"llm\")\ndef import_ckpt(\n    model: pl.LightningModule,\n    source: str,\n    output_path: Optional[Path] = None,\n    overwrite: bool = False,\n) -> Path:\n    return io.import_ckpt(model=model, source=source, output_path=output_path, overwrite=overwrite)\n\n\ndef load_connector_from_trainer_ckpt(path: Path, target: str) -> io.ModelConnector:\n    return io.load_ckpt(path).model.exporter(target, path)\n\n\n@task(name=\"export\", namespace=\"llm\")\ndef export_ckpt(\n    path: Path,\n    target: str,\n    output_path: Optional[Path] = None,\n    overwrite: bool = False,\n    load_connector: Callable[[Path, str], io.ModelConnector] = load_connector_from_trainer_ckpt,\n) -> Path:\n    return io.export_ckpt(path, target, output_path, overwrite, load_connector)\n\n\ndef _use_tokenizer(model: pl.LightningModule, data: pl.LightningDataModule, tokenizer: str) -> None:\n    if tokenizer == \"data\":\n        model.tokenizer = data.tokenizer\n    elif tokenizer == \"model\":\n        data.tokenizer = model.tokenizer\n\n\ndef _add_ckpt_path(source, model, kwargs) -> None:\n    if io.is_distributed_ckpt(source):\n        kwargs[\"ckpt_path\"] = source\n    else:\n        kwargs[\"ckpt_path\"] = model.import_ckpt(source)\n\n\ndef _save_config_img(*args, **kwargs):\n    try:\n        from nemo_sdk.utils import save_config_img\n\n        save_config_img(*args, **kwargs)\n    except ImportError:\n        pass\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/api.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.053950Z","iopub.execute_input":"2024-07-02T22:25:26.054203Z","iopub.status.idle":"2024-07-02T22:25:26.067528Z","shell.execute_reply.started":"2024-07-02T22:25:26.054181Z","shell.execute_reply":"2024-07-02T22:25:26.066810Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write = '''\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Annotated, Callable, Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom nemo.collections.llm.gpt.model.base import GPTConfig, GPTModel\nfrom nemo.collections.llm.utils import Config\nfrom nemo.lightning import MyOptimizerModule, io, teardown\n\nif TYPE_CHECKING:\n    from transformers import LlamaConfig as HFLlamaConfig\n    from transformers import LlamaForCausalLM\n\n    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n    from nemo.collections.common.tokenizers.tokenizer_spec import TokenizerSpec\n\n\n# Note: these Llama configs are copied from the corresponding HF model. You may need to modify the parameter for\n# your own needs, in particular: seq_length and rotary_base.\n@dataclass\nclass LlamaConfig(GPTConfig):\n    # configs that are common across model sizes\n    normalization: str = \"RMSNorm\"\n    activation_func: Callable = F.silu\n    gated_linear_unit: bool = True\n    position_embedding_type: str = \"rope\"\n    add_bias_linear: bool = False\n    seq_length: int = 4096\n\n\n@dataclass\nclass Llama2Config7B(LlamaConfig):\n    num_layers: int = 32\n    hidden_size: int = 4096\n    num_attention_heads: int = 32\n    num_query_groups: int = 32\n    ffn_hidden_size: int = 11008\n\n\n@dataclass\nclass Llama2Config13B(LlamaConfig):\n    num_layers: int = 40\n    hidden_size: int = 5120\n    num_attention_heads: int = 40\n    num_query_groups: int = 40\n    ffn_hidden_size: int = 13824\n\n\n@dataclass\nclass Llama2Config70B(LlamaConfig):\n    num_layers: int = 80\n    hidden_size: int = 8192\n    num_attention_heads: int = 64\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 28672\n\n\n@dataclass\nclass Llama3Config8B(Llama2Config7B):\n    seq_length: int = 8192\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 14336\n\n\n@dataclass\nclass Llama3Config70B(Llama2Config70B):\n    seq_length: int = 8192\n\n\n@dataclass\nclass CodeLlamaConfig7B(Llama2Config7B):\n    rotary_base: int = 1_000_000\n    seq_length: int = 16384\n\n\n@dataclass\nclass CodeLlamaConfig13B(Llama2Config13B):\n    rotary_base: int = 1_000_000\n    seq_length: int = 16384\n\n\n@dataclass\nclass CodeLlamaConfig34B(LlamaConfig):\n    num_layers: int = 48\n    hidden_size: int = 8192\n    num_attention_heads: int = 64\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 22016\n    rotary_base: int = 1_000_000\n    seq_length: int = 16384\n\n\n@dataclass\nclass CodeLlamaConfig70B(Llama2Config70B):\n    pass\n\n\nclass LlamaModel(GPTModel):\n    def __init__(\n        self,\n        config: Annotated[Optional[LlamaConfig], Config[LlamaConfig]] = None,\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__(config or LlamaConfig(), optim=optim, tokenizer=tokenizer)\n\n\n@io.model_importer(LlamaModel, \"hf\")\nclass HFLlamaImporter(io.ModelConnector[\"LlamaForCausalLM\", LlamaModel]):\n    def init(self) -> LlamaModel:\n        return LlamaModel(self.config, tokenizer=self.tokenizer)\n\n    def apply(self, output_path: Path) -> Path:\n        from transformers import LlamaForCausalLM\n\n        source = LlamaForCausalLM.from_pretrained(str(self))\n        target = self.init()\n        trainer = self.nemo_setup(target)\n        self.convert_state(source, target)\n        self.nemo_save(output_path, trainer)\n\n        print(f\"Converted Llama model to Nemo, model saved to {output_path}\")\n\n        teardown(trainer, target)\n        del trainer, target\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"model.embed_tokens.weight\": \"embedding.word_embeddings.weight\",\n            \"model.layers.*.self_attn.o_proj.weight\": \"decoder.layers.*.self_attention.linear_proj.weight\",\n            \"model.layers.*.mlp.down_proj.weight\": \"decoder.layers.*.mlp.linear_fc2.weight\",\n            \"model.layers.*.input_layernorm.weight\": \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n            \"model.layers.*.post_attention_layernorm.weight\": \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\",\n            \"model.norm.weight\": \"decoder.final_layernorm.weight\",\n            \"lm_head.weight\": \"output_layer.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_import_qkv, _import_linear_fc1])\n\n    @property\n    def tokenizer(self) -> \"AutoTokenizer\":\n        from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n        return AutoTokenizer(str(self))\n\n    @property\n    def config(self) -> LlamaConfig:\n        from transformers import LlamaConfig as HFLlamaConfig\n\n        source = HFLlamaConfig.from_pretrained(str(self))\n\n        def make_vocab_size_divisible_by(vocab_size):\n            base = 128\n            while vocab_size % base != 0:\n                base //= 2\n            return base\n\n        output = LlamaConfig(\n            num_layers=source.num_hidden_layers,\n            hidden_size=source.hidden_size,\n            ffn_hidden_size=source.intermediate_size,\n            num_attention_heads=source.num_attention_heads,\n            init_method_std=source.initializer_range,\n            layernorm_epsilon=source.rms_norm_eps,\n            num_query_groups=source.num_key_value_heads,\n            rotary_base=source.rope_theta,\n            gated_linear_unit=True,\n            make_vocab_size_divisible_by=make_vocab_size_divisible_by(source.vocab_size),\n            share_embeddings_and_output_weights=False,\n        )\n\n        return output\n\n\n@io.model_exporter(LlamaModel, \"hf\")\nclass HFLlamaExporter(io.ModelConnector[LlamaModel, \"LlamaForCausalLM\"]):\n    def init(self) -> \"LlamaForCausalLM\":\n        from transformers import AutoModelForCausalLM\n\n        return AutoModelForCausalLM.from_config(self.config)\n\n    def apply(self, output_path: Path) -> Path:\n        target = self.init()\n        source, _ = self.nemo_load(str(self))\n        target = self.convert_state(source, target)\n\n        target = target.cpu()\n        target.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"embedding.word_embeddings.weight\": \"model.embed_tokens.weight\",\n            \"decoder.layers.*.self_attention.linear_proj.weight\": \"model.layers.*.self_attn.o_proj.weight\",\n            \"decoder.layers.*.mlp.linear_fc2.weight\": \"model.layers.*.mlp.down_proj.weight\",\n            \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\": \"model.layers.*.input_layernorm.weight\",\n            \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\": \"model.layers.*.post_attention_layernorm.weight\",\n            \"decoder.final_layernorm.weight\": \"model.norm.weight\",\n            \"output_layer.weight\": \"lm_head.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_export_qkv, _export_linear_fc1])\n\n    @property\n    def tokenizer(self):\n        return io.load_ckpt(str(self)).model.tokenizer.tokenizer\n\n    @property\n    def config(self) -> \"HFLlamaConfig\":\n        source: LlamaConfig = io.load_ckpt(str(self)).model.config\n\n        from transformers import LlamaConfig as HFLlamaConfig\n\n        return HFLlamaConfig(\n            num_hidden_layers=source.num_layers,\n            hidden_size=source.hidden_size,\n            intermediate_size=source.ffn_hidden_size,\n            num_attention_heads=source.num_attention_heads,\n            max_position_embeddings=source.seq_length,\n            initializer_range=source.init_method_std,\n            rms_norm_eps=source.layernorm_epsilon,\n            num_key_value_heads=source.num_query_groups,\n            rope_theta=source.rotary_base,\n            vocab_size=self.tokenizer.vocab_size,\n        )\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n    target_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n)\ndef _import_qkv(ctx: io.TransformCTX, q, k, v):\n    megatron_config = ctx.target.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n\n    old_tensor_shape = q.size()\n    new_q_tensor_shape = (head_num, head_size) + old_tensor_shape[1:]\n    new_kv_tensor_shape = (num_query_groups, head_size) + old_tensor_shape[1:]\n\n    q = q.view(*new_q_tensor_shape)\n    k = k.view(*new_kv_tensor_shape)\n    v = v.view(*new_kv_tensor_shape)\n\n    qkv_weights_l = []\n    for i in range(num_query_groups):\n        qkv_weights_l.append(q[i * heads_per_group : (i + 1) * heads_per_group, :, :])\n        qkv_weights_l.append(k[i : i + 1, :, :])\n        qkv_weights_l.append(v[i : i + 1, :, :])\n    qkv_weights = torch.cat(qkv_weights_l)\n    assert qkv_weights.ndim == 3, qkv_weights.shape\n    assert qkv_weights.shape[0] == (heads_per_group + 2) * num_query_groups, qkv_weights.shape\n    assert qkv_weights.shape[1] == head_size, qkv_weights.shape\n    assert qkv_weights.shape[2] == old_tensor_shape[1], qkv_weights.shape\n\n    qkv_weights = qkv_weights.reshape([head_size * (head_num + 2 * num_query_groups), hidden_size])\n\n    return qkv_weights\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n    target_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n)\ndef _export_qkv(ctx: io.TransformCTX, linear_qkv):\n    megatron_config = ctx.source.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n    qkv_total_dim = head_num + 2 * num_query_groups\n\n    linear_qkv = linear_qkv.reshape([qkv_total_dim, head_size, hidden_size])\n    q_slice = torch.cat(\n        [\n            torch.arange((heads_per_group + 2) * i, (heads_per_group + 2) * i + heads_per_group)\n            for i in range(num_query_groups)\n        ]\n    )\n    k_slice = torch.arange(heads_per_group, qkv_total_dim, (heads_per_group + 2))\n    v_slice = torch.arange(heads_per_group + 1, qkv_total_dim, (heads_per_group + 2))\n\n    q_proj = linear_qkv[q_slice].reshape(-1, hidden_size).cpu()\n    k_proj = linear_qkv[k_slice].reshape(-1, hidden_size).cpu()\n    v_proj = linear_qkv[v_slice].reshape(-1, hidden_size).cpu()\n\n    return q_proj, k_proj, v_proj\n\n\n@io.state_transform(\n    source_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n    target_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n)\ndef _import_linear_fc1(down, gate):\n    return torch.cat((down, gate), axis=0).float()\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n    target_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n)\ndef _export_linear_fc1(linear_fc1):\n    gate_proj, up_proj = torch.chunk(linear_fc1, 2, dim=0)\n\n    return gate_proj, up_proj\n\n\n__all__ = [\n    \"LlamaConfig\",\n    \"Llama2Config7B\",\n    \"Llama2Config13B\",\n    \"Llama2Config70B\",\n    \"Llama3Config8B\",\n    \"Llama3Config70B\",\n    \"CodeLlamaConfig7B\",\n    \"CodeLlamaConfig13B\",\n    \"CodeLlamaConfig34B\",\n    \"CodeLlamaConfig70B\",\n    \"LlamaModel\",\n]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/llama.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.068751Z","iopub.execute_input":"2024-07-02T22:25:26.069008Z","iopub.status.idle":"2024-07-02T22:25:26.088037Z","shell.execute_reply.started":"2024-07-02T22:25:26.068986Z","shell.execute_reply":"2024-07-02T22:25:26.087165Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write = '''from dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Annotated, Callable, Optional\n\nimport torch\n\nfrom nemo.collections.llm.fn.activation import openai_gelu\nfrom nemo.collections.llm.gpt.model.base import GPTConfig, GPTModel\nfrom nemo.collections.llm.utils import Config\nfrom nemo.lightning import MyOptimizerModule, io, teardown\n\nif TYPE_CHECKING:\n    from transformers import GemmaForCausalLM\n\n    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n    from nemo.collections.common.tokenizers.tokenizer_spec import TokenizerSpec\n\n\n# Note: Gemma requires huggingface transformers >= 4.38\n# Note: these Gemma configs are copied from the corresponding HF model. You may need to modify the parameter for\n# your own needs, in particular: seq_length and rotary_base.\n@dataclass\nclass GemmaConfig(GPTConfig):\n    # configs that are common across model sizes\n    normalization: str = \"RMSNorm\"\n    activation_func: Callable = openai_gelu\n    gated_linear_unit: bool = True\n    position_embedding_type: str = \"rope\"\n    add_bias_linear: bool = False\n    seq_length: int = 8192\n    kv_channels: int = 256\n    share_embeddings_and_output_weights: bool = True\n    # Note: different behavior compared to Legacy NeMo\n    # Legacy NeMo does not set layernorm_zero_centered_gamma and instead adds 1 in the HF -> NeMo conversion script\n    # The present implementation is more in line with the official implementation\n    layernorm_zero_centered_gamma: bool = True\n\n\n@dataclass\nclass GemmaConfig2B(GemmaConfig):\n    num_layers: int = 18\n    hidden_size: int = 2048\n    num_attention_heads: int = 8\n    num_query_groups: int = 1\n    ffn_hidden_size: int = 16384\n\n\n@dataclass\nclass GemmaConfig7B(GemmaConfig):\n    num_layers: int = 28\n    hidden_size: int = 3072\n    num_attention_heads: int = 16\n    num_query_groups: int = 16\n    ffn_hidden_size: int = 24576\n\n\nclass CodeGemmaConfig2B(GemmaConfig2B):\n    pass\n\n\nclass CodeGemmaConfig7B(GemmaConfig7B):\n    pass\n\n\nclass GemmaModel(GPTModel):\n    def __init__(\n        self,\n        config: Annotated[Optional[GemmaConfig], Config[GemmaConfig]] = None,\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__(config or GemmaConfig(), optim=optim, tokenizer=tokenizer)\n\n\n@io.model_importer(GemmaModel, \"hf\")\nclass HFGemmaImporter(io.ModelConnector[\"GemmaForCausalLM\", GemmaModel]):\n    def init(self) -> GemmaModel:\n        return GemmaModel(self.config, tokenizer=self.tokenizer)\n\n    def apply(self, output_path: Path) -> Path:\n        from transformers import GemmaForCausalLM\n\n        source = GemmaForCausalLM.from_pretrained(str(self))\n        target = self.init()\n        trainer = self.nemo_setup(target)\n        self.convert_state(source, target)\n        self.nemo_save(output_path, trainer)\n\n        print(f\"Converted Gemma model to Nemo, model saved to {output_path}\")\n\n        teardown(trainer, target)\n        del trainer, target\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"model.embed_tokens.weight\": \"embedding.word_embeddings.weight\",\n            \"model.layers.*.self_attn.o_proj.weight\": \"decoder.layers.*.self_attention.linear_proj.weight\",\n            \"model.layers.*.mlp.down_proj.weight\": \"decoder.layers.*.mlp.linear_fc2.weight\",\n            \"model.layers.*.input_layernorm.weight\": \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n            \"model.layers.*.post_attention_layernorm.weight\": \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\",\n            \"model.norm.weight\": \"decoder.final_layernorm.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_import_qkv, _import_linear_fc1])\n\n    @property\n    def tokenizer(self) -> \"AutoTokenizer\":\n        from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n        return AutoTokenizer(str(self))\n\n    @property\n    def config(self) -> GemmaConfig:\n        from transformers import GemmaConfig as HFGemmaConfig\n\n        source = HFGemmaConfig.from_pretrained(str(self))\n\n        def make_vocab_size_divisible_by(vocab_size):\n            base = 128\n            while vocab_size % base != 0:\n                base //= 2\n            return base\n\n        output = GemmaConfig(\n            num_layers=source.num_hidden_layers,\n            hidden_size=source.hidden_size,\n            ffn_hidden_size=source.intermediate_size,\n            num_attention_heads=source.num_attention_heads,\n            init_method_std=source.initializer_range,\n            layernorm_epsilon=source.rms_norm_eps,\n            num_query_groups=source.num_key_value_heads,\n            rotary_base=source.rope_theta,\n            gated_linear_unit=True,\n            make_vocab_size_divisible_by=make_vocab_size_divisible_by(source.vocab_size),\n            share_embeddings_and_output_weights=False,\n        )\n\n        return output\n\n\n@io.model_exporter(GemmaModel, \"hf\")\nclass HFGemmaExporter(io.ModelConnector[GemmaModel, \"GemmaForCausalLM\"]):\n    def init(self) -> \"GemmaForCausalLM\":\n        from transformers import AutoModelForCausalLM\n\n        return AutoModelForCausalLM.from_config(self.config)\n\n    def apply(self, output_path: Path) -> Path:\n        target = self.init()\n        source, _ = self.nemo_load(str(self))\n        target = self.convert_state(source, target)\n\n        target = target.cpu()\n        target.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"embedding.word_embeddings.weight\": \"model.embed_tokens.weight\",\n            \"decoder.layers.*.self_attention.linear_proj.weight\": \"model.layers.*.self_attn.o_proj.weight\",\n            \"decoder.layers.*.mlp.linear_fc2.weight\": \"model.layers.*.mlp.down_proj.weight\",\n            \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\": \"model.layers.*.input_layernorm.weight\",\n            \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\": \"model.layers.*.post_attention_layernorm.weight\",\n            \"decoder.final_layernorm.weight\": \"model.norm.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_export_qkv, _export_linear_fc1])\n\n    @property\n    def tokenizer(self):\n        return io.load_ckpt(str(self)).model.tokenizer.tokenizer\n\n    @property\n    def config(self) -> \"GemmaConfig\":\n        source: GemmaConfig = io.load_ckpt(str(self)).model.config\n\n        from transformers import GemmaConfig as HFGemmaConfig\n\n        return HFGemmaConfig(\n            num_hidden_layers=source.num_layers,\n            hidden_size=source.hidden_size,\n            intermediate_size=source.ffn_hidden_size,\n            num_attention_heads=source.num_attention_heads,\n            max_position_embeddings=source.seq_length,\n            initializer_range=source.init_method_std,\n            rms_norm_eps=source.layernorm_epsilon,\n            num_key_value_heads=source.num_query_groups,\n            vocab_size=self.tokenizer.vocab_size,\n        )\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n    target_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n)\ndef _import_qkv(ctx: io.TransformCTX, q, k, v):\n    megatron_config = ctx.target.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n\n    old_tensor_shape = q.size()\n    new_q_tensor_shape = (head_num, head_size) + old_tensor_shape[1:]\n    new_kv_tensor_shape = (num_query_groups, head_size) + old_tensor_shape[1:]\n\n    q = q.view(*new_q_tensor_shape)\n    k = k.view(*new_kv_tensor_shape)\n    v = v.view(*new_kv_tensor_shape)\n\n    qkv_weights_l = []\n    for i in range(num_query_groups):\n        qkv_weights_l.append(q[i * heads_per_group : (i + 1) * heads_per_group, :, :])\n        qkv_weights_l.append(k[i : i + 1, :, :])\n        qkv_weights_l.append(v[i : i + 1, :, :])\n    qkv_weights = torch.cat(qkv_weights_l)\n    assert qkv_weights.ndim == 3, qkv_weights.shape\n    assert qkv_weights.shape[0] == (heads_per_group + 2) * num_query_groups, qkv_weights.shape\n    assert qkv_weights.shape[1] == head_size, qkv_weights.shape\n    assert qkv_weights.shape[2] == old_tensor_shape[1], qkv_weights.shape\n\n    qkv_weights = qkv_weights.reshape([head_size * (head_num + 2 * num_query_groups), hidden_size])\n\n    return qkv_weights\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n    target_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n)\ndef _export_qkv(ctx: io.TransformCTX, linear_qkv):\n    megatron_config = ctx.source.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n    qkv_total_dim = head_num + 2 * num_query_groups\n\n    linear_qkv = linear_qkv.reshape([qkv_total_dim, head_size, hidden_size])\n    q_slice = torch.cat(\n        [\n            torch.arange((heads_per_group + 2) * i, (heads_per_group + 2) * i + heads_per_group)\n            for i in range(num_query_groups)\n        ]\n    )\n    k_slice = torch.arange(heads_per_group, qkv_total_dim, (heads_per_group + 2))\n    v_slice = torch.arange(heads_per_group + 1, qkv_total_dim, (heads_per_group + 2))\n\n    q_proj = linear_qkv[q_slice].reshape(-1, hidden_size).cpu()\n    k_proj = linear_qkv[k_slice].reshape(-1, hidden_size).cpu()\n    v_proj = linear_qkv[v_slice].reshape(-1, hidden_size).cpu()\n\n    return q_proj, k_proj, v_proj\n\n\n@io.state_transform(\n    source_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n    target_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n)\ndef _import_linear_fc1(down, gate):\n    return torch.cat((down, gate), axis=0).float()\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n    target_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n)\ndef _export_linear_fc1(linear_fc1):\n    gate_proj, up_proj = torch.chunk(linear_fc1, 2, dim=0)\n\n    return gate_proj, up_proj\n\n\n__all__ = [\n    \"GemmaConfig\",\n    \"GemmaConfig2B\",\n    \"GemmaConfig7B\",\n    \"CodeGemmaConfig2B\",\n    \"CodeGemmaConfig7B\",\n    \"GemmaModel\",\n]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/gemma.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.089205Z","iopub.execute_input":"2024-07-02T22:25:26.089489Z","iopub.status.idle":"2024-07-02T22:25:26.106625Z","shell.execute_reply.started":"2024-07-02T22:25:26.089447Z","shell.execute_reply":"2024-07-02T22:25:26.105907Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write = '''\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Callable, List, Optional\n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn.functional as F\nfrom typing_extensions import Annotated\n\nfrom nemo.collections.llm.gpt.model.base import GPTConfig, GPTModel\nfrom nemo.collections.llm.utils import Config\nfrom nemo.lightning import io, teardown\nfrom nemo.lightning.pytorch.optim import MyOptimizerModule\n\nif TYPE_CHECKING:\n    from transformers import MistralConfig, MistralForCausalLM\n\n    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n    from nemo.collections.common.tokenizers.tokenizer_spec import TokenizerSpec\n\n\n@dataclass\nclass MistralConfig7B(GPTConfig):\n    normalization: str = \"RMSNorm\"\n    activation_func: Callable = F.silu\n    position_embedding_type: str = \"rope\"\n    add_bias_linear: bool = False\n    gated_linear_unit: bool = True\n    apply_query_key_layer_scaling: bool = False  # TODO: Should this be True?\n\n    num_layers: int = 32\n    hidden_size: int = 4096\n    num_attention_heads: int = 32\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 14336\n    seq_length: int = 32768\n\n    init_method_std: float = 0.02\n    layernorm_epsilon: float = 1e-5\n    window_size: List[int] = field(default_factory=lambda: [4096, 0])\n\n\nclass MistralModel(GPTModel):\n    def __init__(\n        self,\n        config: Annotated[Optional[MistralConfig7B], Config[MistralConfig7B]] = None,\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__(config or MistralConfig7B(), optim=optim, tokenizer=tokenizer)\n\n\n@io.model_importer(MistralModel, \"hf\")\nclass HFMistralImporter(io.ModelConnector[\"MistralForCausalLM\", MistralModel]):\n    def init(self) -> MistralModel:\n        return MistralModel(self.config, tokenizer=self.tokenizer)\n\n    def apply(self, output_path: Path) -> Path:\n        from transformers import MistralForCausalLM\n\n        source = MistralForCausalLM.from_pretrained(str(self))\n        target = self.init()\n        trainer = self.nemo_setup(target)\n        self.convert_state(source, target)\n        self.nemo_save(output_path, trainer)\n\n        print(f\"Converted Mistral 7B model to Nemo, model saved to {output_path}\")\n\n        teardown(trainer, target)\n        del trainer, target\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"model.embed_tokens.weight\": \"embedding.word_embeddings.weight\",\n            \"model.layers.*.self_attn.o_proj.weight\": \"decoder.layers.*.self_attention.linear_proj.weight\",\n            \"model.layers.*.mlp.down_proj.weight\": \"decoder.layers.*.mlp.linear_fc2.weight\",\n            \"model.layers.*.input_layernorm.weight\": \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n            \"model.layers.*.post_attention_layernorm.weight\": \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\",\n            \"model.norm.weight\": \"decoder.final_layernorm.weight\",\n            \"lm_head.weight\": \"output_layer.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_import_qkv, _import_linear_fc1])\n\n    @property\n    def tokenizer(self) -> \"AutoTokenizer\":\n        from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n        return AutoTokenizer(str(self))\n\n    @property\n    def config(self) -> MistralConfig7B:\n        from transformers import MistralConfig\n\n        source = MistralConfig.from_pretrained(str(self))\n\n        def make_vocab_size_divisible_by(mistral_vocab_size):\n            base = 128\n            while mistral_vocab_size % base != 0:\n                base //= 2\n            return base\n\n        output = MistralConfig7B(\n            seq_length=source.sliding_window,\n            num_layers=source.num_hidden_layers,\n            hidden_size=source.hidden_size,\n            ffn_hidden_size=source.intermediate_size,\n            num_attention_heads=source.num_attention_heads,\n            # max_position_embeddings=source.max_position_embeddings,\n            init_method_std=source.initializer_range,\n            layernorm_epsilon=source.rms_norm_eps,\n            num_query_groups=source.num_key_value_heads,\n            rotary_base=source.rope_theta,\n            gated_linear_unit=True,\n            make_vocab_size_divisible_by=make_vocab_size_divisible_by(source.vocab_size),\n            window_size=[source.sliding_window, 0],\n            share_embeddings_and_output_weights=False,\n        )\n\n        return output\n\n\n@io.model_exporter(MistralModel, \"hf\")\nclass HFMistralExporter(io.ModelConnector[MistralModel, \"MistralForCausalLM\"]):\n    def init(self) -> \"MistralForCausalLM\":\n        from transformers import AutoModelForCausalLM\n\n        return AutoModelForCausalLM.from_config(self.config)\n\n    def apply(self, output_path: Path) -> Path:\n        # TODO: Make it work with lazy init\n        # with torch.device(\"meta\"):\n        #     target = self.init()\n        target = self.init()\n        source, _ = self.nemo_load(str(self))\n        target = self.convert_state(source, target)\n\n        # TODO: Make sure we don't need to do this\n        target = target.cpu()\n        target.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"embedding.word_embeddings.weight\": \"model.embed_tokens.weight\",\n            \"decoder.layers.*.self_attention.linear_proj.weight\": \"model.layers.*.self_attn.o_proj.weight\",\n            \"decoder.layers.*.mlp.linear_fc2.weight\": \"model.layers.*.mlp.down_proj.weight\",\n            \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\": \"model.layers.*.input_layernorm.weight\",\n            \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\": \"model.layers.*.post_attention_layernorm.weight\",\n            \"decoder.final_layernorm.weight\": \"model.norm.weight\",\n            \"output_layer.weight\": \"lm_head.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_export_qkv, _export_linear_fc1])\n\n    @property\n    def tokenizer(self):\n        return io.load_ckpt(str(self)).model.tokenizer.tokenizer\n\n    @property\n    def config(self) -> \"MistralConfig\":\n        source: MistralConfig7B = io.load_ckpt(str(self)).model.config\n\n        from transformers import MistralConfig as HfMistralConfig\n\n        return HfMistralConfig(\n            sliding_window=source.window_size[0],\n            num_hidden_layers=source.num_layers,\n            hidden_size=source.hidden_size,\n            intermediate_size=source.ffn_hidden_size,\n            num_attention_heads=source.num_attention_heads,\n            max_position_embeddings=source.seq_length,\n            initializer_range=source.init_method_std,\n            rms_norm_eps=source.layernorm_epsilon,\n            num_key_value_heads=source.num_query_groups,\n            rope_theta=source.rotary_base,\n            vocab_size=self.tokenizer.vocab_size,\n        )\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n    target_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n)\ndef _import_qkv(ctx: io.TransformCTX, q, k, v):\n    megatron_config = ctx.target.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n\n    old_tensor_shape = q.size()\n    new_q_tensor_shape = (head_num, head_size) + old_tensor_shape[1:]\n    new_kv_tensor_shape = (num_query_groups, head_size) + old_tensor_shape[1:]\n\n    q = q.view(*new_q_tensor_shape)\n    k = k.view(*new_kv_tensor_shape)\n    v = v.view(*new_kv_tensor_shape)\n\n    qkv_weights_l = []\n    for i in range(num_query_groups):\n        qkv_weights_l.append(q[i * heads_per_group : (i + 1) * heads_per_group, :, :])\n        qkv_weights_l.append(k[i : i + 1, :, :])\n        qkv_weights_l.append(v[i : i + 1, :, :])\n    qkv_weights = torch.cat(qkv_weights_l)\n    assert qkv_weights.ndim == 3, qkv_weights.shape\n    assert qkv_weights.shape[0] == (heads_per_group + 2) * num_query_groups, qkv_weights.shape\n    assert qkv_weights.shape[1] == head_size, qkv_weights.shape\n    assert qkv_weights.shape[2] == old_tensor_shape[1], qkv_weights.shape\n\n    qkv_weights = qkv_weights.reshape([head_size * (head_num + 2 * num_query_groups), hidden_size])\n\n    return qkv_weights\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n    target_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n)\ndef _export_qkv(ctx: io.TransformCTX, linear_qkv):\n    megatron_config = ctx.source.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n    qkv_total_dim = head_num + 2 * num_query_groups\n\n    linear_qkv = linear_qkv.reshape([qkv_total_dim, head_size, hidden_size])\n    q_slice = torch.cat(\n        [\n            torch.arange((heads_per_group + 2) * i, (heads_per_group + 2) * i + heads_per_group)\n            for i in range(num_query_groups)\n        ]\n    )\n    k_slice = torch.arange(heads_per_group, qkv_total_dim, (heads_per_group + 2))\n    v_slice = torch.arange(heads_per_group + 1, qkv_total_dim, (heads_per_group + 2))\n\n    q_proj = linear_qkv[q_slice].reshape(-1, hidden_size).cpu()\n    k_proj = linear_qkv[k_slice].reshape(-1, hidden_size).cpu()\n    v_proj = linear_qkv[v_slice].reshape(-1, hidden_size).cpu()\n\n    return q_proj, k_proj, v_proj\n\n\n@io.state_transform(\n    source_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n    target_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n)\ndef _import_linear_fc1(down, gate):\n    return torch.cat((down, gate), axis=0).float()\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n    target_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n)\ndef _export_linear_fc1(linear_fc1):\n    gate_proj, up_proj = torch.chunk(linear_fc1, 2, dim=0)\n\n    return gate_proj, up_proj\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/mistral.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.111064Z","iopub.execute_input":"2024-07-02T22:25:26.111348Z","iopub.status.idle":"2024-07-02T22:25:26.127089Z","shell.execute_reply.started":"2024-07-02T22:25:26.111325Z","shell.execute_reply":"2024-07-02T22:25:26.126207Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write = '''\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Dict, Literal, Optional\n\nimport pytorch_lightning as L\nimport torch\nimport torch.distributed\nfrom megatron.core.optimizer import OptimizerConfig\nfrom megatron.core.transformer.transformer_config import TransformerConfig\n\nfrom nemo.collections.llm import fn\nfrom nemo.lightning import get_vocab_size, io\nfrom nemo.lightning.megatron_parallel import MaskedTokenLossReduction\nfrom nemo.lightning.pytorch.optim import MegatronOptimizerModule, MyOptimizerModule\n\nif TYPE_CHECKING:\n    from megatron.core.models.gpt.gpt_model import GPTModel as MCoreGPTModel\n\n    from nemo.collections.common.tokenizers.tokenizer_spec import TokenizerSpec\n\n\n@dataclass\nclass GPTConfig(TransformerConfig, io.IOMixin):\n    # From megatron.core.models.gpt.gpt_model.GPTModel\n    fp16_lm_cross_entropy: bool = False\n    parallel_output: bool = True\n    share_embeddings_and_output_weights: bool = True\n    make_vocab_size_divisible_by: int = 128\n    position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\"\n    rotary_base: int = 10000\n    rotary_percent: float = 1.0\n    seq_len_interpolation_factor: Optional[float] = None\n    seq_length: int = 1024\n\n    # TODO: Move this to better places?\n    get_attention_mask_from_fusion: bool = False\n\n    def configure_model(self, tokenizer) -> \"MCoreGPTModel\":\n        vp_size = self.virtual_pipeline_model_parallel_size\n        if vp_size:\n            p_size = self.pipeline_model_parallel_size\n            assert (\n                self.num_layers // p_size\n            ) % vp_size == 0, \"Make sure the number of model chunks is the same across all pipeline stages.\"\n\n        from megatron.core import parallel_state\n        from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_with_transformer_engine_spec\n        from megatron.core.models.gpt.gpt_model import GPTModel as MCoreGPTModel\n\n        return MCoreGPTModel(\n            self,\n            transformer_layer_spec=get_gpt_layer_with_transformer_engine_spec(self.num_moe_experts),\n            vocab_size=get_vocab_size(self, tokenizer.vocab_size, self.make_vocab_size_divisible_by),\n            max_sequence_length=self.seq_length,\n            fp16_lm_cross_entropy=self.fp16_lm_cross_entropy,\n            parallel_output=self.parallel_output,\n            share_embeddings_and_output_weights=self.share_embeddings_and_output_weights,\n            position_embedding_type=self.position_embedding_type,\n            rotary_percent=self.rotary_percent,\n            rotary_base=self.rotary_base,\n            seq_len_interpolation_factor=self.seq_len_interpolation_factor,\n            pre_process=parallel_state.is_pipeline_first_stage(),\n            post_process=parallel_state.is_pipeline_last_stage(),\n        )\n\n\nclass GPTModel(L.LightningModule, io.IOMixin, io.ConnectorMixin, fn.FNMixin):\n    def __init__(\n        self,\n        config: GPTConfig,\n        # TODO: Add transformer_layer_spec when we update mcore\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.tokenizer = tokenizer\n        self.optim = optim or MegatronOptimizerModule(config=OptimizerConfig(lr=1e-4, use_distributed_optimizer=True))\n        self.optim.connect(self)  # This will bind the `configure_optimizers` method\n\n    def configure_model(self) -> None:\n        if not hasattr(self, \"module\"):\n            self.module = self.config.configure_model(self.tokenizer)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        labels: Optional[torch.Tensor] = None,\n        decoder_input: Optional[torch.Tensor] = None,\n        inference_params=None,\n    ) -> torch.Tensor:\n        output_tensor = self.module(\n            input_ids,\n            position_ids,\n            attention_mask,\n            decoder_input=decoder_input,\n            labels=labels,\n            inference_params=inference_params,\n        )\n\n        return output_tensor\n\n    def data_step(self, dataloader_iter) -> Dict[str, torch.Tensor]:\n        return gpt_data_step(dataloader_iter)\n\n    def forward_step(self, batch) -> torch.Tensor:\n        return gpt_forward_step(self, batch)\n\n    def training_step(self, batch, batch_idx=None) -> torch.Tensor:\n        # In mcore the loss-function is part of the forward-pass (when labels are provided)\n\n        return self.forward_step(batch)\n\n    def validation_step(self, batch, batch_idx=None) -> torch.Tensor:\n        # In mcore the loss-function is part of the forward-pass (when labels are provided)\n\n        return self.forward_step(batch)\n\n    def training_loss_reduction(self) -> MaskedTokenLossReduction:\n        return MaskedTokenLossReduction()\n\n    def validation_loss_reduction(self) -> MaskedTokenLossReduction:\n        return MaskedTokenLossReduction(validation_step=True)\n\n\ndef gpt_data_step(dataloader_iter) -> Dict[str, torch.Tensor]:\n    from megatron.core import parallel_state\n\n    # Based on: https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py#L87\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L828-L842\n\n    batch = next(dataloader_iter)\n\n    _batch: dict\n    if isinstance(batch, tuple) and len(batch) == 3:\n        _batch = batch[0]\n    else:\n        _batch = batch\n\n    required_keys = set()\n    required_keys.add(\"attention_mask\")\n    if parallel_state.is_pipeline_first_stage():\n        required_keys.update((\"tokens\", \"position_ids\"))\n    if parallel_state.is_pipeline_last_stage():\n        required_keys.update((\"labels\", \"loss_mask\"))\n    # if self.get_attention_mask_from_fusion:\n    #     required_keys.remove('attention_mask')\n\n    _batch = {key: val.cuda(non_blocking=True) if key in required_keys else None for key, val in _batch.items()}\n    # slice batch along sequence dimension for context parallelism\n    output = get_batch_on_this_context_parallel_rank(_batch)\n\n    return output\n\n\ndef gpt_forward_step(model, batch) -> torch.Tensor:\n    forward_args = {\n        \"input_ids\": batch[\"tokens\"],\n        \"position_ids\": batch[\"position_ids\"],\n        \"attention_mask\": batch[\"attention_mask\"],\n        \"labels\": batch[\"labels\"],\n    }\n\n    if 'cu_seqlens' in batch:\n        forward_args['packed_seq_params'] = get_packed_seq_params(batch)\n\n    return model(**forward_args)\n\n\ndef get_batch_on_this_context_parallel_rank(batch):\n    from megatron.core import parallel_state\n\n    if (cp_size := parallel_state.get_context_parallel_world_size()) > 1:\n        num_valid_tokens_in_ub = None\n        if 'loss_mask' in batch and batch['loss_mask'] is not None:\n            num_valid_tokens_in_ub = batch['loss_mask'].sum()\n\n        cp_rank = parallel_state.get_context_parallel_rank()\n        for key, val in batch.items():\n            if val is not None:\n                seq_dim = 1 if key != 'attention_mask' else 2\n                _val = val.view(\n                    *val.shape[0:seq_dim],\n                    2 * cp_size,\n                    val.shape[seq_dim] // (2 * cp_size),\n                    *val.shape[(seq_dim + 1) :],\n                )\n                index = torch.tensor([cp_rank, (2 * cp_size - cp_rank - 1)], device=\"cpu\", pin_memory=True).cuda(\n                    non_blocking=True\n                )\n                _val = _val.index_select(seq_dim, index)\n                _val = _val.view(*val.shape[0:seq_dim], -1, *_val.shape[(seq_dim + 2) :])\n                batch[key] = _val\n        batch['num_valid_tokens_in_ub'] = num_valid_tokens_in_ub\n    return batch\n\n\ndef get_packed_seq_params(batch):\n    from megatron.core.packed_seq_params import PackedSeqParams\n\n    cu_seqlens = batch['cu_seqlens'].squeeze()  # remove batch size dimension (mbs=1)\n    # remove -1 \"paddings\" added in collate_fn\n    if (cu_seqlens_argmin := batch.get('cu_seqlens_argmin', None)) is not None:\n        # pre-compute cu_seqlens_argmin in dataset class for perf\n        cu_seqlens = cu_seqlens[: cu_seqlens_argmin.item()]\n    else:\n        cu_seqlens = cu_seqlens[: torch.argmin(cu_seqlens)]\n\n    # pre-compute max_seqlens in dataset class for perf\n    max_seqlen = batch['max_seqlen'].squeeze() if 'max_seqlen' in batch else None\n\n    # these args are passed eventually into TEDotProductAttention.forward()\n    return PackedSeqParams(\n        cu_seqlens_q=cu_seqlens,\n        cu_seqlens_kv=cu_seqlens,\n        max_seqlen_q=max_seqlen,\n        max_seqlen_kv=max_seqlen,\n        qkv_format='thd',\n    )\n\n\n__all__ = [\"GPTModel\", \"GPTConfig\", \"gpt_data_step\", \"gpt_forward_step\"]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/base.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.128285Z","iopub.execute_input":"2024-07-02T22:25:26.128571Z","iopub.status.idle":"2024-07-02T22:25:26.143237Z","shell.execute_reply.started":"2024-07-02T22:25:26.128548Z","shell.execute_reply":"2024-07-02T22:25:26.142503Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write = '''\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Callable, Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom nemo.collections.llm.gpt.model.base import GPTConfig, GPTModel\nfrom nemo.lightning import io, teardown\nfrom nemo.lightning.pytorch.optim import MyOptimizerModule\n\nif TYPE_CHECKING:\n    from transformers import MistralConfig, MistralForCausalLM\n\n    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n\n@dataclass\nclass MixtralConfig8x7B(GPTConfig):\n    \"\"\"\n    Config for Mixtral-8x7B model\n    Official announcement: https://mistral.ai/news/mixtral-of-experts/\n    \"\"\"\n\n    normalization: str = \"RMSNorm\"\n    activation_func: Callable = F.silu\n    position_embedding_type: str = \"rope\"\n    add_bias_linear: bool = False\n    gated_linear_unit: bool = True\n    apply_query_key_layer_scaling: bool = False  # TODO: Should this be True?\n\n    num_layers: int = 32\n    hidden_size: int = 4096\n    num_attention_heads: int = 32\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 14336\n    max_position_embeddings: int = 4096  # 32768\n    seq_length: int = 4096  # 32768\n    # MoE\n    num_moe_experts: int = 8\n    moe_router_topk: int = 1\n\n    init_method_std: float = 0.02\n    layernorm_epsilon: float = 1e-5\n    # rotary\n    rotary_percent: float = 0.5\n    rotary_base: float = 10000\n\n\nclass MixtralModel(GPTModel):\n    def __init__(\n        self,\n        config: Optional[MixtralConfig8x7B] = None,\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__(config or MixtralConfig8x7B(), optim=optim, tokenizer=tokenizer)\n\n\n@io.model_importer(MixtralModel, ext=\"hf\")\nclass HFMixtralImporter(io.ModelConnector[\"MixtralForCausalLM\", MixtralModel]):\n    def init(self) -> MixtralModel:\n        return MixtralModel(self.config, tokenizer=self.tokenizer)\n\n    def apply(self, output_path: Path) -> Path:\n        from transformers import MixtralForCausalLM\n\n        source = MixtralForCausalLM.from_pretrained(str(self))\n        target = self.init()\n        trainer = self.nemo_setup(target)\n        self.convert_state(source, target)\n        self.nemo_save(output_path, trainer)\n\n        teardown(trainer, target)\n        del trainer, target\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"model.embed_tokens.weight\": \"embedding.word_embeddings.weight\",\n            \"model.layers.*.self_attn.o_proj.weight\": \"decoder.layers.*.self_attention.linear_proj.weight\",\n            \"model.layers.*.input_layernorm.weight\": \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n            \"model.layers.*.post_attention_layernorm.weight\": \"decoder.layers.*.pre_mlp_layernorm.weight\",\n            # MoE\n            \"model.layers.*.block_sparse_moe.experts.*.w2.weight\": \"decoder.layers.*.mlp.experts.local_experts.*.linear_fc2.weight\",\n            \"model.layers.*.block_sparse_moe.gate.weight\": \"decoder.layers.*.mlp.router.weight\",\n            # lm-head\n            \"model.norm.weight\": \"decoder.final_layernorm.weight\",\n            \"lm_head.weight\": \"output_layer.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_import_qkv, _import_moe_w1_w3])\n\n    @property\n    def tokenizer(self) -> \"AutoTokenizer\":\n        from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n        return AutoTokenizer(str(self))\n\n    @property\n    def config(self) -> MixtralConfig8x7B:\n        from transformers import MixtralConfig as HfMixtralConfig\n\n        config = HfMixtralConfig.from_pretrained(str(self))\n        return MixtralConfig8x7B(\n            activation_func=F.silu,\n            # network\n            num_layers=config.num_hidden_layers,\n            hidden_size=config.hidden_size,\n            ffn_hidden_size=config.intermediate_size,\n            max_position_embeddings=config.max_position_embeddings,  # TODO\n            seq_length=config.max_position_embeddings,\n            # RoPE\n            position_embedding_type='rope',\n            rotary_base=config.rope_theta,\n            # Transformer config\n            num_attention_heads=config.num_attention_heads,\n            num_query_groups=config.num_key_value_heads,\n            num_moe_experts=config.num_local_experts,\n            moe_router_topk=config.num_experts_per_tok,\n            # norm\n            normalization='RMSNorm',\n            layernorm_epsilon=config.rms_norm_eps,\n            # Init\n            init_method_std=config.initializer_range,\n            gated_linear_unit=True,\n            # Vocab\n            make_vocab_size_divisible_by=128,\n        )\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n    target_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n)\ndef _import_qkv(ctx: io.TransformCTX, q, k, v):\n    megatron_config = ctx.target.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n\n    old_tensor_shape = q.size()\n    new_q_tensor_shape = (head_num, head_size) + old_tensor_shape[1:]\n    new_kv_tensor_shape = (num_query_groups, head_size) + old_tensor_shape[1:]\n\n    q = q.view(*new_q_tensor_shape)\n    k = k.view(*new_kv_tensor_shape)\n    v = v.view(*new_kv_tensor_shape)\n\n    qkv_weights_l = []\n    for i in range(num_query_groups):\n        qkv_weights_l.append(q[i * heads_per_group : (i + 1) * heads_per_group, :, :])\n        qkv_weights_l.append(k[i : i + 1, :, :])\n        qkv_weights_l.append(v[i : i + 1, :, :])\n    qkv_weights = torch.cat(qkv_weights_l)\n    assert qkv_weights.ndim == 3, qkv_weights.shape\n    assert qkv_weights.shape[0] == (heads_per_group + 2) * num_query_groups, qkv_weights.shape\n    assert qkv_weights.shape[1] == head_size, qkv_weights.shape\n    assert qkv_weights.shape[2] == old_tensor_shape[1], qkv_weights.shape\n\n    qkv_weights = qkv_weights.reshape([head_size * (head_num + 2 * num_query_groups), hidden_size])\n\n    return qkv_weights\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.block_sparse_moe.experts.*.w1.weight\",\n        \"model.layers.*.block_sparse_moe.experts.*.w3.weight\",\n    ),\n    target_key=\"decoder.layers.*.mlp.experts.local_experts.*.linear_fc1.weight\",\n)\ndef _import_moe_w1_w3(gate_proj, up_proj):\n    return torch.cat((gate_proj, up_proj), axis=0)\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/mixtral.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.144761Z","iopub.execute_input":"2024-07-02T22:25:26.145074Z","iopub.status.idle":"2024-07-02T22:25:26.158583Z","shell.execute_reply.started":"2024-07-02T22:25:26.145045Z","shell.execute_reply":"2024-07-02T22:25:26.157826Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write = '''from nemo.lightning.pytorch.optim.base import LRSchedulerModule, MyOptimizerModule\nfrom nemo.lightning.pytorch.optim.lr_scheduler import (\n    CosineAnnealingScheduler,\n    InverseSquareRootAnnealingScheduler,\n    NoamAnnealingScheduler,\n    NoamHoldAnnealingScheduler,\n    PolynomialDecayAnnealingScheduler,\n    PolynomialHoldDecayAnnealingScheduler,\n    SquareAnnealingScheduler,\n    SquareRootAnnealingScheduler,\n    T5InverseSquareRootAnnealingScheduler,\n    WarmupAnnealingScheduler,\n    WarmupHoldPolicyScheduler,\n    WarmupPolicyScheduler,\n)\nfrom nemo.lightning.pytorch.optim.megatron import MegatronOptimizerModule\n\n__all__ = [\n    \"MyOptimizerModule\",\n    \"LRSchedulerModule\",\n    \"MegatronOptimizerModule\",\n    \"WarmupPolicyScheduler\",\n    \"WarmupHoldPolicyScheduler\",\n    \"SquareAnnealingScheduler\",\n    \"SquareRootAnnealingScheduler\",\n    \"NoamAnnealingScheduler\",\n    \"NoamHoldAnnealingScheduler\",\n    \"WarmupAnnealingScheduler\",\n    \"InverseSquareRootAnnealingScheduler\",\n    \"T5InverseSquareRootAnnealingScheduler\",\n    \"PolynomialDecayAnnealingScheduler\",\n    \"PolynomialHoldDecayAnnealingScheduler\",\n    \"CosineAnnealingScheduler\",\n]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/lightning/pytorch/optim/__init__.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:25:26.159751Z","iopub.execute_input":"2024-07-02T22:25:26.159996Z","iopub.status.idle":"2024-07-02T22:25:26.171730Z","shell.execute_reply.started":"2024-07-02T22:25:26.159974Z","shell.execute_reply":"2024-07-02T22:25:26.171002Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Review CheckPoint ðŸ•µï¸â€â™‚ï¸âœ…\n### Reviewing a saved checkpoint of the model to continue training from a previous state.","metadata":{}},{"cell_type":"code","source":"!ls \"/kaggle/input/sussy-baka/results/Some name of our experiment/checkpoints\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:48:08.260880Z","iopub.execute_input":"2024-07-02T23:48:08.261765Z","iopub.status.idle":"2024-07-02T23:48:09.258767Z","shell.execute_reply.started":"2024-07-02T23:48:08.261726Z","shell.execute_reply":"2024-07-02T23:48:09.257812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!rm -r \"/kaggle/working/results/Some name of our experiment/checkpoints/Some name of our experiment--val_wer=0.5307-epoch=42.ckpt\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:40:35.499561Z","iopub.execute_input":"2024-07-02T21:40:35.500875Z","iopub.status.idle":"2024-07-02T21:40:36.648177Z","shell.execute_reply.started":"2024-07-02T21:40:35.500823Z","shell.execute_reply":"2024-07-02T21:40:36.646018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!mv \"/kaggle/working/results/Some name of our experiment/checkpoints/Some name of our experiment--val_wer=0.4992-epoch=40-last.ckpt\" \"/kaggle/working/results/Some name of our experiment/checkpoints/Some name of our experiment--val_wer=0.4992-epoch=40.ckpt\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T21:39:59.275916Z","iopub.execute_input":"2024-07-02T21:39:59.276501Z","iopub.status.idle":"2024-07-02T21:40:00.417016Z","shell.execute_reply.started":"2024-07-02T21:39:59.276455Z","shell.execute_reply":"2024-07-02T21:40:00.415310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model ðŸš‚ðŸ’»\n### Training the model on the given dataset using the defined configurations and hyperparameters.","metadata":{}},{"cell_type":"code","source":"TOKENIZER='/kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128'\nTRAIN_MANIFEST='/kaggle/input/aic-manifests/train_manifest.json'\nVAL_MANIFEST='/kaggle/input/aic-manifests/test_manifest.json'\nNEMO_ROOT='/kaggle/working/NeMo'\n\n! HYDRA_FULL_ERROR=1 python /kaggle/working/NeMo/examples/asr/asr_ctc/speech_to_text_ctc_bpe.py \\\n  --config-path=../conf/conformer/ \\\n  --config-name=conformer_ctc_bpe \\\n  exp_manager.name=\"Some name of our experiment\" \\\n  exp_manager.resume_if_exists=true \\\n  exp_manager.resume_ignore_no_checkpoint=true \\\n  exp_manager.exp_dir=results/ \\\n  model.tokenizer.dir=$TOKENIZER \\\n  model.train_ds.manifest_filepath=$TRAIN_MANIFEST \\\n  model.validation_ds.manifest_filepath=$VAL_MANIFEST \\\n  exp_manager.create_wandb_logger=True \\\n  exp_manager.wandb_logger_kwargs.project=\"'ASR'\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T23:52:00.398755Z","iopub.execute_input":"2024-07-02T23:52:00.399068Z","iopub.status.idle":"2024-07-02T23:54:23.894888Z","shell.execute_reply.started":"2024-07-02T23:52:00.399045Z","shell.execute_reply":"2024-07-02T23:54:23.893697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning on Adapt dataset ðŸ› ï¸ðŸ“ˆ\n### Performing fine-tuning on a specialized dataset (Adapt dataset) to further refine the modelâ€™s accuracy and performance.","metadata":{}},{"cell_type":"code","source":"TOKENIZER='/kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128'\nTRAIN_MANIFEST='/kaggle/input/adapt-split-manifest/data_85.json'\nVAL_MANIFEST='/kaggle/input/adapt-split-manifest/data_15.json'\nNEMO_ROOT='/kaggle/working/NeMo'\n\n! HYDRA_FULL_ERROR=1 python /kaggle/working/NeMo/examples/asr/asr_ctc/speech_to_text_ctc_bpe.py \\\n  --config-path=../conf/conformer/ \\\n  --config-name=conformer_ctc_bpe \\\n  exp_manager.name=\"Some name of our experiment\" \\\n  exp_manager.resume_if_exists=true \\\n  exp_manager.resume_ignore_no_checkpoint=true \\\n  exp_manager.exp_dir=results/ \\\n  model.tokenizer.dir=$TOKENIZER \\\n  model.train_ds.manifest_filepath=$TRAIN_MANIFEST \\\n  model.validation_ds.manifest_filepath=$VAL_MANIFEST \\\n  exp_manager.create_wandb_logger=True \\\n  exp_manager.wandb_logger_kwargs.project=\"'ASR'\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T23:47:24.216169Z","iopub.execute_input":"2024-07-02T23:47:24.216542Z","iopub.status.idle":"2024-07-02T23:47:24.220956Z","shell.execute_reply.started":"2024-07-02T23:47:24.216514Z","shell.execute_reply":"2024-07-02T23:47:24.219837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating CSV file ðŸ“„âž¡ï¸ðŸ“Š\n### Creating a CSV file with the results, predictions, or any other relevant data produced by the model.","metadata":{}},{"cell_type":"code","source":"checkpoint_path = '/kaggle/working/results/Some name of our experiment/checkpoints/Some name of our experiment.nemo'","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:54:33.204530Z","iopub.execute_input":"2024-07-02T23:54:33.204925Z","iopub.status.idle":"2024-07-02T23:54:33.210147Z","shell.execute_reply.started":"2024-07-02T23:54:33.204896Z","shell.execute_reply":"2024-07-02T23:54:33.209104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_asr_model = nemo_asr.models.EncDecCTCModelBPE.restore_from(checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:54:37.449755Z","iopub.execute_input":"2024-07-02T23:54:37.450649Z","iopub.status.idle":"2024-07-02T23:54:40.396161Z","shell.execute_reply.started":"2024-07-02T23:54:37.450614Z","shell.execute_reply":"2024-07-02T23:54:40.395196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport csv\n\n# Placeholder for your ASR model initialization\n# first_asr_model = YourASRModel()\n\n# Path to the directory containing .wav files\ndata_dir = '/kaggle/input/test-data/test'\n\n# List all .wav files in the directory\nwav_files = [f for f in os.listdir(data_dir) if f.endswith('.wav')]\n\n# Prepare the list of audio paths\naudio_paths = [os.path.join(data_dir, wav) for wav in wav_files]\n\n# Transcribe the audio files in batches (assuming batch_size=4)\nbatch_size = 4\ntranscriptions = []\n\nfor i in range(0, len(audio_paths), batch_size):\n    batch_paths = audio_paths[i:i + batch_size]\n    transcripts = first_asr_model.transcribe(audio=batch_paths, batch_size=len(batch_paths))\n    transcriptions.extend(transcripts)\n\n# Prepare data for CSV\ncsv_data = []\nfor wav, transcript in zip(wav_files, transcriptions):\n    audio_name = os.path.splitext(wav)[0]\n    csv_data.append([audio_name, transcript])\n\n# Write to CSV\ncsv_file = 'transcriptions.csv'\nwith open(csv_file, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['audio', 'transcript'])\n    writer.writerows(csv_data)\n\nprint(f\"Transcriptions saved to {csv_file}\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T23:54:40.739994Z","iopub.execute_input":"2024-07-02T23:54:40.740327Z"},"trusted":true},"execution_count":null,"outputs":[]}]}