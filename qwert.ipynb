{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8805495,"sourceType":"datasetVersion","datasetId":5295827},{"sourceId":8817445,"sourceType":"datasetVersion","datasetId":5304348},{"sourceId":8826380,"sourceType":"datasetVersion","datasetId":5310362},{"sourceId":8827752,"sourceType":"datasetVersion","datasetId":5304606},{"sourceId":8843668,"sourceType":"datasetVersion","datasetId":5322767},{"sourceId":8842783,"sourceType":"datasetVersion","datasetId":5322120},{"sourceId":8750045,"sourceType":"datasetVersion","datasetId":5238724,"isSourceIdPinned":true},{"sourceId":8844110,"sourceType":"datasetVersion","datasetId":5323049}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wget\n!apt-get -y install sox libsndfile1 ffmpeg\n!pip install text-unidecode\n!pip install matplotlib>=3.3.2","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T19:47:35.295846Z","iopub.execute_input":"2024-07-02T19:47:35.296199Z","iopub.status.idle":"2024-07-02T19:48:22.652661Z","shell.execute_reply.started":"2024-07-02T19:47:35.296171Z","shell.execute_reply":"2024-07-02T19:48:22.651584Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=8b1ba4979599049717d6419fc3a2483f59604267b06ad5c360a87ae502c767c2\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nlibsndfile1 is already the newest version (1.0.28-7ubuntu0.2).\nlibsndfile1 set to manually installed.\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\nThe following additional packages will be installed:\n  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n  libsox-fmt-base libsox3\nSuggested packages:\n  file libsox-fmt-all\nThe following NEW packages will be installed:\n  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n  libsox-fmt-base libsox3 sox\n0 upgraded, 8 newly installed, 0 to remove and 75 not upgraded.\nNeed to get 807 kB of archives.\nAfter this operation, 7653 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic-mgc amd64 1:5.38-4 [218 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic1 amd64 1:5.38-4 [75.9 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\nGet:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\nGet:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1 [225 kB]\nGet:6 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1 [10.5 kB]\nGet:7 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1 [31.4 kB]\nGet:8 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 sox amd64 14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1 [102 kB]\nFetched 807 kB in 1s (1318 kB/s)\nSelecting previously unselected package libmagic-mgc.\n(Reading database ... 113807 files and directories currently installed.)\nPreparing to unpack .../0-libmagic-mgc_1%3a5.38-4_amd64.deb ...\nUnpacking libmagic-mgc (1:5.38-4) ...\nSelecting previously unselected package libmagic1:amd64.\nPreparing to unpack .../1-libmagic1_1%3a5.38-4_amd64.deb ...\nUnpacking libmagic1:amd64 (1:5.38-4) ...\nSelecting previously unselected package libopencore-amrnb0:amd64.\nPreparing to unpack .../2-libopencore-amrnb0_0.1.5-1_amd64.deb ...\nUnpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\nSelecting previously unselected package libopencore-amrwb0:amd64.\nPreparing to unpack .../3-libopencore-amrwb0_0.1.5-1_amd64.deb ...\nUnpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\nSelecting previously unselected package libsox3:amd64.\nPreparing to unpack .../4-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1_amd64.deb ...\nUnpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1) ...\nSelecting previously unselected package libsox-fmt-alsa:amd64.\nPreparing to unpack .../5-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1_amd64.deb ...\nUnpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1) ...\nSelecting previously unselected package libsox-fmt-base:amd64.\nPreparing to unpack .../6-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1_amd64.deb ...\nUnpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1) ...\nSelecting previously unselected package sox.\nPreparing to unpack .../7-sox_14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1_amd64.deb ...\nUnpacking sox (14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1) ...\nSetting up libmagic-mgc (1:5.38-4) ...\nSetting up libmagic1:amd64 (1:5.38-4) ...\nSetting up libopencore-amrwb0:amd64 (0.1.5-1) ...\nSetting up libopencore-amrnb0:amd64 (0.1.5-1) ...\nSetting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1) ...\nSetting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1) ...\nSetting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1) ...\nSetting up sox (14.4.2+git20190427-2+deb11u2ubuntu0.20.04.1) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.14) ...\nProcessing triggers for man-db (2.9.1-1) ...\nProcessing triggers for mime-support (3.64ubuntu1) ...\nRequirement already satisfied: text-unidecode in /opt/conda/lib/python3.10/site-packages (1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"BRANCH = 'r2.0.0rc0'\n!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T19:48:22.654815Z","iopub.execute_input":"2024-07-02T19:48:22.655135Z","iopub.status.idle":"2024-07-02T19:51:17.324073Z","shell.execute_reply.started":"2024-07-02T19:48:22.655106Z","shell.execute_reply":"2024-07-02T19:51:17.323139Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mDEPRECATION: git+https://github.com/NVIDIA/NeMo.git@r2.0.0rc0#egg=nemo_toolkit[all] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n\u001b[0mCollecting nemo_toolkit (from nemo_toolkit[all])\n  Cloning https://github.com/NVIDIA/NeMo.git (to revision r2.0.0rc0) to /tmp/pip-install-jwyx1rh2/nemo-toolkit_d3f1de749cb942cb971322ed70ac0cde\n  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/NeMo.git /tmp/pip-install-jwyx1rh2/nemo-toolkit_d3f1de749cb942cb971322ed70ac0cde\n  Running command git checkout -b r2.0.0rc0 --track origin/r2.0.0rc0\n  Switched to a new branch 'r2.0.0rc0'\n  Branch 'r2.0.0rc0' set up to track remote branch 'r2.0.0rc0' from 'origin'.\n  Resolved https://github.com/NVIDIA/NeMo.git to commit d02bb32a36eecab15e2782839eb5b6838df5cb88\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting fiddle (from nemo_toolkit->nemo_toolkit[all])\n  Downloading fiddle-0.3.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: huggingface-hub>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.23.2)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.58.1)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (1.26.4)\nRequirement already satisfied: onnx>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (1.16.1)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (2.9.0.post0)\nRequirement already satisfied: ruamel.yaml in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.18.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (1.2.2)\nRequirement already satisfied: setuptools>=65.5.1 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (69.0.3)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (2.15.1)\nRequirement already satisfied: text-unidecode in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (1.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (2.1.2)\nRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (4.66.4)\nRequirement already satisfied: wget in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (3.2)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (1.14.1)\nCollecting black~=24.3 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading black-24.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting click==8.0.2 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading click-8.0.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: isort<6.0.0,>5.1.0 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (5.13.2)\nCollecting parameterized (from nemo_toolkit->nemo_toolkit[all])\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (8.2.1)\nCollecting pytest-mock (from nemo_toolkit->nemo_toolkit[all])\n  Downloading pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting pytest-runner (from nemo_toolkit->nemo_toolkit[all])\n  Downloading pytest_runner-6.0.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting sphinx (from nemo_toolkit->nemo_toolkit[all])\n  Downloading sphinx-7.3.7-py3-none-any.whl.metadata (6.0 kB)\nCollecting sphinxcontrib-bibtex (from nemo_toolkit->nemo_toolkit[all])\n  Downloading sphinxcontrib_bibtex-2.6.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.17.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (2.2.1)\nCollecting hydra-core<=1.3.2,>1.3 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting omegaconf<=2.3 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: pytorch-lightning>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (2.2.5)\nRequirement already satisfied: torchmetrics>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (1.4.0.post0)\nCollecting transformers<=4.40.2,>=4.36.0 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting webdataset>=0.2.86 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading webdataset-0.2.86-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (2.19.2)\nCollecting inflect (from nemo_toolkit->nemo_toolkit[all])\n  Downloading inflect-7.3.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (2.2.1)\nCollecting sacremoses>=0.0.43 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: sentencepiece<1.0.0 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.2.0)\nCollecting braceexpand (from nemo_toolkit->nemo_toolkit[all])\n  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting editdistance (from nemo_toolkit->nemo_toolkit[all])\n  Downloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nCollecting einops (from nemo_toolkit->nemo_toolkit[all])\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting g2p-en (from nemo_toolkit->nemo_toolkit[all])\n  Downloading g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (7.7.1)\nCollecting jiwer (from nemo_toolkit->nemo_toolkit[all])\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nCollecting kaldi-python-io (from nemo_toolkit->nemo_toolkit[all])\n  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting kaldiio (from nemo_toolkit->nemo_toolkit[all])\n  Downloading kaldiio-2.18.0-py3-none-any.whl.metadata (13 kB)\nCollecting lhotse>=1.22.0 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading lhotse-1.24.2-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: librosa>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.10.2.post1)\nRequirement already satisfied: marshmallow in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (3.21.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (3.7.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (21.3)\nCollecting pyannote.core (from nemo_toolkit->nemo_toolkit[all])\n  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting pyannote.metrics (from nemo_toolkit->nemo_toolkit[all])\n  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.25.1)\nCollecting pyloudnorm (from nemo_toolkit->nemo_toolkit[all])\n  Downloading pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\nCollecting resampy (from nemo_toolkit->nemo_toolkit[all])\n  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (1.11.4)\nRequirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.12.1)\nCollecting sox (from nemo_toolkit->nemo_toolkit[all])\n  Downloading sox-1.5.0.tar.gz (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting texterrors (from nemo_toolkit->nemo_toolkit[all])\n  Downloading texterrors-0.5.1.tar.gz (23 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting accelerated-scan (from nemo_toolkit->nemo_toolkit[all])\n  Downloading accelerated_scan-0.2.0-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (1.26.100)\nCollecting causal-conv1d==1.2.0.post2 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading causal_conv1d-1.2.0.post2.tar.gz (7.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting faiss-cpu (from nemo_toolkit->nemo_toolkit[all])\n  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: fasttext in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.9.2)\nCollecting flask-restful (from nemo_toolkit->nemo_toolkit[all])\n  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl.metadata (1.0 kB)\nCollecting ftfy (from nemo_toolkit->nemo_toolkit[all])\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nCollecting gdown (from nemo_toolkit->nemo_toolkit[all])\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (3.10.0)\nCollecting ijson (from nemo_toolkit->nemo_toolkit[all])\n  Downloading ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.42.1)\nCollecting markdown2 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading markdown2-2.4.13-py2.py3-none-any.whl.metadata (2.0 kB)\nCollecting nltk>=3.6.5 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nCollecting opencc<1.1.7 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading OpenCC-1.1.6-cp310-cp310-manylinux1_x86_64.whl.metadata (12 kB)\nCollecting pangu (from nemo_toolkit->nemo_toolkit[all])\n  Downloading pangu-4.0.6.1-py3-none-any.whl.metadata (5.3 kB)\nCollecting rapidfuzz (from nemo_toolkit->nemo_toolkit[all])\n  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting rouge-score (from nemo_toolkit->nemo_toolkit[all])\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting sacrebleu (from nemo_toolkit->nemo_toolkit[all])\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentence-transformers (from nemo_toolkit->nemo_toolkit[all])\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nCollecting tensorstore<0.1.46 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading tensorstore-0.1.45-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\nCollecting zarr (from nemo_toolkit->nemo_toolkit[all])\n  Downloading zarr-2.18.2-py3-none-any.whl.metadata (5.7 kB)\nCollecting attrdict (from nemo_toolkit->nemo_toolkit[all])\n  Downloading attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: kornia in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.7.2)\nCollecting pypinyin (from nemo_toolkit->nemo_toolkit[all])\n  Downloading pypinyin-0.51.0-py2.py3-none-any.whl.metadata (12 kB)\nCollecting pypinyin-dict (from nemo_toolkit->nemo_toolkit[all])\n  Downloading pypinyin_dict-0.8.0-py2.py3-none-any.whl.metadata (3.6 kB)\nCollecting progress>=1.5 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading progress-1.6.tar.gz (7.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tabulate>=0.8.7 in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (0.9.0)\nCollecting textdistance>=4.1.5 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading textdistance-4.6.2-py3-none-any.whl.metadata (18 kB)\nCollecting addict (from nemo_toolkit->nemo_toolkit[all])\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nCollecting clip (from nemo_toolkit->nemo_toolkit[all])\n  Downloading clip-0.2.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting decord (from nemo_toolkit->nemo_toolkit[all])\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nCollecting diffusers>=0.19.3 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading diffusers-0.29.2-py3-none-any.whl.metadata (19 kB)\nCollecting einops-exts (from nemo_toolkit->nemo_toolkit[all])\n  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.10/site-packages (from nemo_toolkit->nemo_toolkit[all]) (2.33.1)\nCollecting nerfacc>=0.5.3 (from nemo_toolkit->nemo_toolkit[all])\n  Downloading nerfacc-0.5.3-py3-none-any.whl.metadata (915 bytes)\nCollecting open-clip-torch (from nemo_toolkit->nemo_toolkit[all])\n  Downloading open_clip_torch-2.24.0-py3-none-any.whl.metadata (30 kB)\nCollecting PyMCubes (from nemo_toolkit->nemo_toolkit[all])\n  Downloading PyMCubes-0.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (863 bytes)\nCollecting taming-transformers (from nemo_toolkit->nemo_toolkit[all])\n  Downloading taming_transformers-0.0.1-py3-none-any.whl.metadata (499 bytes)\nCollecting torchdiffeq (from nemo_toolkit->nemo_toolkit[all])\n  Downloading torchdiffeq-0.2.4-py3-none-any.whl.metadata (440 bytes)\nCollecting torchsde (from nemo_toolkit->nemo_toolkit[all])\n  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\nCollecting trimesh (from nemo_toolkit->nemo_toolkit[all])\n  Downloading trimesh-4.4.1-py3-none-any.whl.metadata (18 kB)\nCollecting nemo-text-processing (from nemo_toolkit->nemo_toolkit[all])\n  Downloading nemo_text_processing-1.0.2-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from causal-conv1d==1.2.0.post2->nemo_toolkit->nemo_toolkit[all]) (1.11.1.1)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from black~=24.3->nemo_toolkit->nemo_toolkit[all]) (1.0.0)\nCollecting packaging (from nemo_toolkit->nemo_toolkit[all])\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting pathspec>=0.9.0 (from black~=24.3->nemo_toolkit->nemo_toolkit[all])\n  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black~=24.3->nemo_toolkit->nemo_toolkit[all]) (3.11.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black~=24.3->nemo_toolkit->nemo_toolkit[all]) (2.0.1)\nRequirement already satisfied: typing-extensions>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from black~=24.3->nemo_toolkit->nemo_toolkit[all]) (4.9.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (6.11.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (3.13.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (0.4.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (9.5.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.3->nemo_toolkit->nemo_toolkit[all]) (2024.3.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.3->nemo_toolkit->nemo_toolkit[all]) (6.0.1)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core<=1.3.2,>1.3->nemo_toolkit->nemo_toolkit[all])\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hINFO: pip is looking at multiple versions of jiwer to determine which version is compatible with other requirements. This could take a while.\nCollecting jiwer (from nemo_toolkit->nemo_toolkit[all])\n  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\n  Downloading jiwer-3.0.2-py3-none-any.whl.metadata (2.6 kB)\n  Downloading jiwer-3.0.1-py3-none-any.whl.metadata (2.6 kB)\n  Downloading jiwer-3.0.0-py3-none-any.whl.metadata (2.6 kB)\n  Downloading jiwer-2.6.0-py3-none-any.whl.metadata (14 kB)\n  Downloading jiwer-2.5.2-py3-none-any.whl.metadata (11 kB)\nCollecting rapidfuzz (from nemo_toolkit->nemo_toolkit[all])\n  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[all]) (3.0.1)\nRequirement already satisfied: cytoolz>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[all]) (0.12.3)\nCollecting intervaltree>=3.1.0 (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[all])\n  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting lilcom>=1.1.0 (from lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[all])\n  Downloading lilcom-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[all]) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[all]) (5.1.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[all]) (1.8.1)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[all]) (0.3.7)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[all]) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.10.0->nemo_toolkit->nemo_toolkit[all]) (1.0.7)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nemo_toolkit->nemo_toolkit[all]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nemo_toolkit->nemo_toolkit[all]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nemo_toolkit->nemo_toolkit[all]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nemo_toolkit->nemo_toolkit[all]) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nemo_toolkit->nemo_toolkit[all]) (3.1.1)\nRequirement already satisfied: rich>=12 in /opt/conda/lib/python3.10/site-packages (from nerfacc>=0.5.3->nemo_toolkit->nemo_toolkit[all]) (13.7.0)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->nemo_toolkit->nemo_toolkit[all]) (0.41.1)\nRequirement already satisfied: protobuf>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from onnx>=1.7.0->nemo_toolkit->nemo_toolkit[all]) (3.20.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil->nemo_toolkit->nemo_toolkit[all]) (1.16.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning>=2.2.1->nemo_toolkit->nemo_toolkit[all]) (0.11.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->nemo_toolkit->nemo_toolkit[all]) (3.2.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile->nemo_toolkit->nemo_toolkit[all]) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->nemo_toolkit->nemo_toolkit[all]) (3.1.2)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.40.2,>=4.36.0->nemo_toolkit->nemo_toolkit[all]) (0.19.1)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3->nemo_toolkit->nemo_toolkit[all])\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->nemo_toolkit->nemo_toolkit[all]) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->nemo_toolkit->nemo_toolkit[all]) (0.6.2)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->nemo_toolkit->nemo_toolkit[all]) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->nemo_toolkit->nemo_toolkit[all]) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->nemo_toolkit->nemo_toolkit[all]) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->nemo_toolkit->nemo_toolkit[all]) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->nemo_toolkit->nemo_toolkit[all]) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->nemo_toolkit->nemo_toolkit[all]) (3.9.1)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext->nemo_toolkit->nemo_toolkit[all]) (2.12.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from fiddle->nemo_toolkit->nemo_toolkit[all]) (1.4.0)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from fiddle->nemo_toolkit->nemo_toolkit[all]) (0.20.3)\nCollecting libcst (from fiddle->nemo_toolkit->nemo_toolkit[all])\n  Downloading libcst-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\nCollecting aniso8601>=0.82 (from flask-restful->nemo_toolkit->nemo_toolkit[all])\n  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: Flask>=0.8 in /opt/conda/lib/python3.10/site-packages (from flask-restful->nemo_toolkit->nemo_toolkit[all]) (3.0.3)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from flask-restful->nemo_toolkit->nemo_toolkit[all]) (2023.3.post1)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->nemo_toolkit->nemo_toolkit[all]) (0.2.13)\nCollecting distance>=0.1.3 (from g2p-en->nemo_toolkit->nemo_toolkit[all])\n  Downloading Distance-0.1.3.tar.gz (180 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: more-itertools>=8.5.0 in /opt/conda/lib/python3.10/site-packages (from inflect->nemo_toolkit->nemo_toolkit[all]) (10.2.0)\nRequirement already satisfied: typeguard>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from inflect->nemo_toolkit->nemo_toolkit[all]) (4.1.5)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown->nemo_toolkit->nemo_toolkit[all]) (4.12.2)\nRequirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[all]) (6.28.0)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.2.0)\nRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[all]) (5.9.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[all]) (3.6.6)\nRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[all]) (8.20.0)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->nemo_toolkit->nemo_toolkit[all]) (3.0.9)\nRequirement already satisfied: kornia-rs>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from kornia->nemo_toolkit->nemo_toolkit[all]) (0.1.3)\nCollecting cdifflib (from nemo-text-processing->nemo_toolkit->nemo_toolkit[all])\n  Downloading cdifflib-1.2.6.tar.gz (11 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting pynini==2.1.5 (from nemo-text-processing->nemo_toolkit->nemo_toolkit[all])\n  Downloading pynini-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: Cython>=0.29 in /opt/conda/lib/python3.10/site-packages (from pynini==2.1.5->nemo-text-processing->nemo_toolkit->nemo_toolkit[all]) (3.0.8)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open-clip-torch->nemo_toolkit->nemo_toolkit[all]) (0.16.2)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open-clip-torch->nemo_toolkit->nemo_toolkit[all]) (1.0.3)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->nemo_toolkit->nemo_toolkit[all]) (2023.4)\nRequirement already satisfied: sortedcontainers>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from pyannote.core->nemo_toolkit->nemo_toolkit[all]) (2.4.0)\nCollecting pyannote.database>=4.0.1 (from pyannote.metrics->nemo_toolkit->nemo_toolkit[all])\n  Downloading pyannote.database-5.1.0-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: docopt>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from pyannote.metrics->nemo_toolkit->nemo_toolkit[all]) (0.6.2)\nRequirement already satisfied: future>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from pyloudnorm->nemo_toolkit->nemo_toolkit[all]) (1.0.0)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest->nemo_toolkit->nemo_toolkit[all]) (2.0.0)\nRequirement already satisfied: pluggy<2.0,>=1.5 in /opt/conda/lib/python3.10/site-packages (from pytest->nemo_toolkit->nemo_toolkit[all]) (1.5.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest->nemo_toolkit->nemo_toolkit[all]) (1.2.0)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from ruamel.yaml->nemo_toolkit->nemo_toolkit[all]) (0.2.7)\nCollecting portalocker (from sacrebleu->nemo_toolkit->nemo_toolkit[all])\n  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->nemo_toolkit->nemo_toolkit[all]) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->nemo_toolkit->nemo_toolkit[all]) (5.2.2)\nCollecting sphinxcontrib-applehelp (from sphinx->nemo_toolkit->nemo_toolkit[all])\n  Downloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-devhelp (from sphinx->nemo_toolkit->nemo_toolkit[all])\n  Downloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-jsmath (from sphinx->nemo_toolkit->nemo_toolkit[all])\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx->nemo_toolkit->nemo_toolkit[all])\n  Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx->nemo_toolkit->nemo_toolkit[all])\n  Downloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl.metadata (2.4 kB)\nCollecting sphinxcontrib-qthelp (from sphinx->nemo_toolkit->nemo_toolkit[all])\n  Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.17.2)\nRequirement already satisfied: docutils<0.22,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (0.21.2)\nRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx->nemo_toolkit->nemo_toolkit[all]) (2.14.0)\nCollecting alabaster~=0.7.14 (from sphinx->nemo_toolkit->nemo_toolkit[all])\n  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\nCollecting imagesize>=1.3 (from sphinx->nemo_toolkit->nemo_toolkit[all])\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting pybtex>=0.24 (from sphinxcontrib-bibtex->nemo_toolkit->nemo_toolkit[all])\n  Downloading pybtex-0.24.0-py2.py3-none-any.whl.metadata (2.0 kB)\nCollecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex->nemo_toolkit->nemo_toolkit[all])\n  Downloading pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->nemo_toolkit->nemo_toolkit[all]) (1.59.3)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->nemo_toolkit->nemo_toolkit[all]) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->nemo_toolkit->nemo_toolkit[all]) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->nemo_toolkit->nemo_toolkit[all]) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->nemo_toolkit->nemo_toolkit[all]) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->nemo_toolkit->nemo_toolkit[all]) (3.0.3)\nCollecting plac (from texterrors->nemo_toolkit->nemo_toolkit[all])\n  Downloading plac-1.4.3-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: loguru in /opt/conda/lib/python3.10/site-packages (from texterrors->nemo_toolkit->nemo_toolkit[all]) (0.7.2)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from texterrors->nemo_toolkit->nemo_toolkit[all]) (2.4.0)\nCollecting Levenshtein (from texterrors->nemo_toolkit->nemo_toolkit[all])\n  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\nCollecting trampoline>=0.1.2 (from torchsde->nemo_toolkit->nemo_toolkit[all])\n  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->nemo_toolkit->nemo_toolkit[all]) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->nemo_toolkit->nemo_toolkit[all]) (3.1.41)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->nemo_toolkit->nemo_toolkit[all]) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->nemo_toolkit->nemo_toolkit[all]) (2.3.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->nemo_toolkit->nemo_toolkit[all]) (1.3.3)\nCollecting asciitree (from zarr->nemo_toolkit->nemo_toolkit[all])\n  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting numcodecs>=0.10.0 (from zarr->nemo_toolkit->nemo_toolkit[all])\n  Downloading numcodecs-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\nRequirement already satisfied: fasteners in /opt/conda/lib/python3.10/site-packages (from zarr->nemo_toolkit->nemo_toolkit[all]) (0.19)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->nemo_toolkit->nemo_toolkit[all]) (1.26.18)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile->nemo_toolkit->nemo_toolkit[all]) (2.21)\nRequirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from cytoolz>=0.10.1->lhotse>=1.22.0->nemo_toolkit->nemo_toolkit[all]) (0.12.1)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.8->flask-restful->nemo_toolkit->nemo_toolkit[all]) (2.2.0)\nINFO: pip is looking at multiple versions of flask to determine which version is compatible with other requirements. This could take a while.\nCollecting Flask>=0.8 (from flask-restful->nemo_toolkit->nemo_toolkit[all])\n  Downloading flask-3.0.2-py3-none-any.whl.metadata (3.6 kB)\n  Downloading flask-3.0.1-py3-none-any.whl.metadata (3.6 kB)\n  Downloading flask-3.0.0-py3-none-any.whl.metadata (3.6 kB)\n  Downloading flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\n  Downloading Flask-2.3.2-py3-none-any.whl.metadata (3.7 kB)\n  Downloading Flask-2.3.1-py3-none-any.whl.metadata (3.7 kB)\n  Downloading Flask-2.3.0-py3-none-any.whl.metadata (3.7 kB)\nINFO: pip is still looking at multiple versions of flask to determine which version is compatible with other requirements. This could take a while.\n  Downloading Flask-2.2.5-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->nemo_toolkit->nemo_toolkit[all]) (4.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit->nemo_toolkit[all]) (4.0.11)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit->nemo_toolkit[all]) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit->nemo_toolkit[all]) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit->nemo_toolkit[all]) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->nemo_toolkit->nemo_toolkit[all]) (1.3.1)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.2.1)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.8.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (5.7.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.5.8)\nRequirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (24.0.1)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (6.3.3)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.19.1)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (3.0.42)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.6.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (4.8.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->nemo_toolkit->nemo_toolkit[all]) (2.1.3)\nCollecting typer>=0.12.1 (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit->nemo_toolkit[all])\n  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\nCollecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex->nemo_toolkit->nemo_toolkit[all])\n  Downloading latexcodec-3.0.0-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12->nerfacc>=0.5.3->nemo_toolkit->nemo_toolkit[all]) (3.0.0)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->nemo_toolkit->nemo_toolkit[all]) (1.3.0)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (6.5.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown->nemo_toolkit->nemo_toolkit[all]) (2.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers>=0.19.3->nemo_toolkit->nemo_toolkit[all]) (3.17.0)\nINFO: pip is looking at multiple versions of levenshtein to determine which version is compatible with other requirements. This could take a while.\nCollecting Levenshtein (from texterrors->nemo_toolkit->nemo_toolkit[all])\n  Downloading Levenshtein-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n  Downloading Levenshtein-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n  Downloading Levenshtein-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->nemo_toolkit->nemo_toolkit[all]) (1.7.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit->nemo_toolkit[all]) (5.0.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.8.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.4)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12->nerfacc>=0.5.3->nemo_toolkit->nemo_toolkit[all]) (0.1.2)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (23.1.0)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (5.9.2)\nRequirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (6.4.5)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.18.0)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.19.0)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.0.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.7.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->nemo_toolkit->nemo_toolkit[all]) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->nemo_toolkit->nemo_toolkit[all]) (3.2.2)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit->nemo_toolkit[all]) (1.5.4)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.2.2)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (2.12.5)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.2.3)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.3.0)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (6.1.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.7.1)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.5.13)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (2.19.1)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (4.20.0)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (21.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.16.2)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (4.2.0)\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.5.1)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (7.4.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.7.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.5.1)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.3.0)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (2.0.7)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (2.4)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit->nemo_toolkit[all]) (2.8.19.20240106)\nDownloading click-8.0.2-py3-none-any.whl (97 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading black-24.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading diffusers-0.29.2-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiwer-2.5.2-py3-none-any.whl (15 kB)\nDownloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading lhotse-1.24.2-py3-none-any.whl (787 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m787.2/787.2 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nerfacc-0.5.3-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading OpenCC-1.1.6-cp310-cp310-manylinux1_x86_64.whl (778 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.3/778.3 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorstore-0.1.45-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading textdistance-4.6.2-py3-none-any.whl (31 kB)\nDownloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading webdataset-0.2.86-py3-none-any.whl (70 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerated_scan-0.2.0-py3-none-any.whl (11 kB)\nDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nDownloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\nDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\nDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\nDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fiddle-0.3.0-py3-none-any.whl (419 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\nDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading inflect-7.3.1-py3-none-any.whl (34 kB)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nDownloading ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\nDownloading markdown2-2.4.13-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nemo_text_processing-1.0.2-py3-none-any.whl (2.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pynini-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading open_clip_torch-2.24.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\nDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\nDownloading PyMCubes-0.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.3/274.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pypinyin-0.51.0-py2.py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pypinyin_dict-0.8.0-py2.py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\nDownloading pytest_runner-6.0.1-py3-none-any.whl (7.2 kB)\nDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinx-7.3.7-py3-none-any.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_bibtex-2.6.2-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading taming_transformers-0.0.1-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchdiffeq-0.2.4-py3-none-any.whl (32 kB)\nDownloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trimesh-4.4.1-py3-none-any.whl (694 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m694.7/694.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading zarr-2.18.2-py3-none-any.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.2/210.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\nDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading Flask-2.2.5-py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nDownloading lilcom-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numcodecs-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\nDownloading pyannote.database-5.1.0-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\nDownloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\nDownloading Levenshtein-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.9/172.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading libcst-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading plac-1.4.3-py2.py3-none-any.whl (22 kB)\nDownloading portalocker-2.10.0-py3-none-any.whl (18 kB)\nDownloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl (120 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nDownloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading latexcodec-3.0.0-py3-none-any.whl (18 kB)\nDownloading typer-0.12.3-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: nemo_toolkit, causal-conv1d, antlr4-python3-runtime, progress, clip, kaldi-python-io, rouge-score, sox, texterrors, distance, intervaltree, asciitree, cdifflib\n  Building wheel for nemo_toolkit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nemo_toolkit: filename=nemo_toolkit-2.0.0rc0-py3-none-any.whl size=3638332 sha256=de47ecb62ce16b3d8fae2332ea888e9fa38e2b696ce34e74b6b980e4e56af80c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mw3kfw1a/wheels/0e/bb/ab/27e77a2c5e6f5b7f87468678c238256a315ec40fd5fb4deeae\n  Building wheel for causal-conv1d (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for causal-conv1d: filename=causal_conv1d-1.2.0.post2-cp310-cp310-linux_x86_64.whl size=45447847 sha256=25203ec43fb3bfdebb8170c244e49c0ced9e7eed177d11b2fac3a71f58bb9805\n  Stored in directory: /root/.cache/pip/wheels/eb/8f/52/646c36c652677964016059dd0845960d162668451aeaaf4533\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=71f6ea0c790fb0fc42f4b5c2a4fa097453045e049b8b6e782960f2fd13f6218a\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for progress (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9612 sha256=47143f12adcd11426ad234cf0c16f388096ac43f0257b12d1d4a1b75500b339b\n  Stored in directory: /root/.cache/pip/wheels/a2/68/5f/c339b20a41659d856c93ccdce6a33095493eb82c3964aac5a1\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=6989 sha256=bbf1065b64a4bea652e0e4e74086c469f9cbfdfcabe3a599a4e24b469b3d5500\n  Stored in directory: /root/.cache/pip/wheels/7f/5c/e6/2c0fdb453a3569188864b17e9676bea8b3b7e160c037117869\n  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=8949 sha256=9b85dec48cbd87c335906243bec48541796910e296665e83565556c377e3ae91\n  Stored in directory: /root/.cache/pip/wheels/b7/23/5f/49d3a826be576faf61d84e8028e1914bb36a5586ee2613b087\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=af8c407f5a96d58d0ae8aea6ef4b0598700521b32ed8c587be3dd2706482d687\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n  Building wheel for sox (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sox: filename=sox-1.5.0-py3-none-any.whl size=40037 sha256=3f4fb7c49eb5059d5c09f42d632f0dffe966072cf2ea6ea879ff06f9e6d0ab04\n  Stored in directory: /root/.cache/pip/wheels/74/e7/7b/8033be3ec5e4994595d01269fc9657c8fd83a0dcbf8536666a\n  Building wheel for texterrors (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for texterrors: filename=texterrors-0.5.1-cp310-cp310-linux_x86_64.whl size=100064 sha256=d76ed510d380ef8f5e86ec1c6b8e1adcfcebd8e308d4642924173a3e978e4f06\n  Stored in directory: /root/.cache/pip/wheels/64/b3/44/612e60c5d0d1f956213e08ae2c3f73443fde51878c778a9a38\n  Building wheel for distance (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=d4756a7035b5abd983e702db9bac2469f40b9881075972ff3fe02fb1a5b236dc\n  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26095 sha256=2aa4951ec9d0c659d55cc0ec83b2dc10d31e4cf175685db5a456591283028896\n  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n  Building wheel for asciitree (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5033 sha256=cfb74973ff6963d27db111dd01bdaa30610c691ee750c92b4e9bcf3651dec87a\n  Stored in directory: /root/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\n  Building wheel for cdifflib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for cdifflib: filename=cdifflib-1.2.6-cp310-cp310-linux_x86_64.whl size=12391 sha256=68555a222a906ead18904557365a7fe4f42fa672d89ce3f098106ac803a8baff\n  Stored in directory: /root/.cache/pip/wheels/87/a7/fd/8061e24ed08689045cb6d1ca303768dc463b20a5a338174841\nSuccessfully built nemo_toolkit causal-conv1d antlr4-python3-runtime progress clip kaldi-python-io rouge-score sox texterrors distance intervaltree asciitree cdifflib\nInstalling collected packages: trampoline, progress, plac, pangu, opencc, ijson, distance, clip, braceexpand, asciitree, antlr4-python3-runtime, aniso8601, addict, webdataset, trimesh, textdistance, tensorstore, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, sox, rapidfuzz, pytest-runner, pypinyin, pynini, portalocker, pathspec, parameterized, packaging, omegaconf, numcodecs, markdown2, lilcom, libcst, latexcodec, kaldiio, kaldi-python-io, intervaltree, imagesize, ftfy, einops, editdistance, decord, click, cdifflib, attrdict, alabaster, zarr, sphinx, sacremoses, sacrebleu, resampy, pypinyin-dict, PyMCubes, pyloudnorm, pybtex, pyannote.core, nltk, Levenshtein, jiwer, inflect, hydra-core, Flask, fiddle, faiss-cpu, einops-exts, botocore, black, typer, torchsde, torchdiffeq, texterrors, rouge-score, pytest-mock, pybtex-docutils, nerfacc, lhotse, gdown, g2p-en, flask-restful, diffusers, causal-conv1d, accelerated-scan, transformers, sphinxcontrib-bibtex, pyannote.database, taming-transformers, sentence-transformers, pyannote.metrics, open-clip-torch, nemo_toolkit, nemo-text-processing\n  Attempting uninstall: tensorstore\n    Found existing installation: tensorstore 0.1.60\n    Uninstalling tensorstore-0.1.60:\n      Successfully uninstalled tensorstore-0.1.60\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: click\n    Found existing installation: click 8.1.7\n    Uninstalling click-8.1.7:\n      Successfully uninstalled click-8.1.7\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: Flask\n    Found existing installation: Flask 3.0.3\n    Uninstalling Flask-3.0.3:\n      Successfully uninstalled Flask-3.0.3\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.34.106\n    Uninstalling botocore-1.34.106:\n      Successfully uninstalled botocore-1.34.106\n  Attempting uninstall: typer\n    Found existing installation: typer 0.9.0\n    Uninstalling typer-0.9.0:\n      Successfully uninstalled typer-0.9.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\naiobotocore 2.13.0 requires aiohttp<4.0.0,>=3.9.2, but you have aiohttp 3.9.1 which is incompatible.\naiobotocore 2.13.0 requires botocore<1.34.107,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndask 2024.5.2 requires click>=8.1, but you have click 8.0.2 which is incompatible.\ndask-cuda 24.4.0 requires click>=8.1, but you have click 8.0.2 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nfitter 1.7.0 requires click<9.0.0,>=8.1.6, but you have click 8.0.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\norbax-checkpoint 0.5.15 requires tensorstore>=0.1.51, but you have tensorstore 0.1.45 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Flask-2.2.5 Levenshtein-0.22.0 PyMCubes-0.1.4 accelerated-scan-0.2.0 addict-2.4.0 alabaster-0.7.16 aniso8601-9.0.1 antlr4-python3-runtime-4.9.3 asciitree-0.3.3 attrdict-2.0.1 black-24.4.2 botocore-1.29.165 braceexpand-0.1.7 causal-conv1d-1.2.0.post2 cdifflib-1.2.6 click-8.0.2 clip-0.2.0 decord-0.6.0 diffusers-0.29.2 distance-0.1.3 editdistance-0.8.1 einops-0.8.0 einops-exts-0.0.4 faiss-cpu-1.8.0.post1 fiddle-0.3.0 flask-restful-0.3.10 ftfy-6.2.0 g2p-en-2.1.0 gdown-5.2.0 hydra-core-1.3.2 ijson-3.3.0 imagesize-1.4.1 inflect-7.3.1 intervaltree-3.1.0 jiwer-2.5.2 kaldi-python-io-1.2.2 kaldiio-2.18.0 latexcodec-3.0.0 lhotse-1.24.2 libcst-1.4.0 lilcom-1.8.0 markdown2-2.4.13 nemo-text-processing-1.0.2 nemo_toolkit-2.0.0rc0 nerfacc-0.5.3 nltk-3.8.1 numcodecs-0.12.1 omegaconf-2.3.0 open-clip-torch-2.24.0 opencc-1.1.6 packaging-24.1 pangu-4.0.6.1 parameterized-0.9.0 pathspec-0.12.1 plac-1.4.3 portalocker-2.10.0 progress-1.6 pyannote.core-5.0.0 pyannote.database-5.1.0 pyannote.metrics-3.2.1 pybtex-0.24.0 pybtex-docutils-1.0.3 pyloudnorm-0.1.1 pynini-2.1.5 pypinyin-0.51.0 pypinyin-dict-0.8.0 pytest-mock-3.14.0 pytest-runner-6.0.1 rapidfuzz-2.13.7 resampy-0.4.3 rouge-score-0.1.2 sacrebleu-2.4.2 sacremoses-0.1.1 sentence-transformers-3.0.1 sox-1.5.0 sphinx-7.3.7 sphinxcontrib-applehelp-1.0.8 sphinxcontrib-bibtex-2.6.2 sphinxcontrib-devhelp-1.0.6 sphinxcontrib-htmlhelp-2.0.5 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.7 sphinxcontrib-serializinghtml-1.1.10 taming-transformers-0.0.1 tensorstore-0.1.45 textdistance-4.6.2 texterrors-0.5.1 torchdiffeq-0.2.4 torchsde-0.2.6 trampoline-0.1.2 transformers-4.40.2 trimesh-4.4.1 typer-0.12.3 webdataset-0.2.86 zarr-2.18.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport os\nimport pandas as pd\nimport librosa","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:51:17.325449Z","iopub.execute_input":"2024-07-02T19:51:17.325783Z","iopub.status.idle":"2024-07-02T19:51:17.698288Z","shell.execute_reply.started":"2024-07-02T19:51:17.325751Z","shell.execute_reply":"2024-07-02T19:51:17.697548Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nif not os.path.exists(\"scripts/tokenizers/process_asr_text_tokenizer.py\"):\n    !mkdir scripts\n    !wget -P scripts/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tokenizers/process_asr_text_tokenizer.py","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:51:17.700833Z","iopub.execute_input":"2024-07-02T19:51:17.701712Z","iopub.status.idle":"2024-07-02T19:51:19.763552Z","shell.execute_reply.started":"2024-07-02T19:51:17.701678Z","shell.execute_reply":"2024-07-02T19:51:19.762691Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-07-02 19:51:19--  https://raw.githubusercontent.com/NVIDIA/NeMo/r2.0.0rc0/scripts/tokenizers/process_asr_text_tokenizer.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 16631 (16K) [text/plain]\nSaving to: 'scripts/process_asr_text_tokenizer.py'\n\nprocess_asr_text_to 100%[===================>]  16.24K  --.-KB/s    in 0.001s  \n\n2024-07-02 19:51:19 (27.5 MB/s) - 'scripts/process_asr_text_tokenizer.py' saved [16631/16631]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!python ./scripts/process_asr_text_tokenizer.py \\\n  --manifest='/kaggle/input/aic-manifests/train_manifest.json' \\\n  --data_root=\"/kaggle/working/tokinzers/sus\" \\\n  --vocab_size=128 \\\n  --tokenizer=\"spe\" \\\n  --no_lower_case \\\n  --spe_type=\"unigram\" \\\n  --log","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T19:51:19.764927Z","iopub.execute_input":"2024-07-02T19:51:19.765467Z","iopub.status.idle":"2024-07-02T19:51:40.783997Z","shell.execute_reply.started":"2024-07-02T19:51:19.765436Z","shell.execute_reply":"2024-07-02T19:51:40.782976Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[NeMo I 2024-07-02 19:51:28 sentencepiece_tokenizer:317] Processing /kaggle/working/tokinzers/sus/text_corpus/document.txt and store at /kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128\nsentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/kaggle/working/tokinzers/sus/text_corpus/document.txt --model_prefix=/kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128/tokenizer --vocab_size=128 --shuffle_input_sentence=true --hard_vocab_limit=false --model_type=unigram --character_coverage=1.0 --bos_id=-1 --eos_id=-1\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/tokinzers/sus/text_corpus/document.txt\n  input_format: \n  model_prefix: /kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128/tokenizer\n  model_type: UNIGRAM\n  vocab_size: 128\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 0\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: -1\n  eos_id: -1\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/working/tokinzers/sus/text_corpus/document.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 50709 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=3469719\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=63\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 50709 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=1622735\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 98661 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 50709\ntrainer_interface.cc(609) LOG(INFO) Done! 69061\nunigram_model_trainer.cc(602) LOG(INFO) Using 69061 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=48645 obj=10.4445 num_tokens=118776 num_tokens/piece=2.44169\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=41493 obj=9.02381 num_tokens=119704 num_tokens/piece=2.88492\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=31114 obj=8.99062 num_tokens=126775 num_tokens/piece=4.07453\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=31044 obj=8.95407 num_tokens=126925 num_tokens/piece=4.08855\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=23283 obj=9.06745 num_tokens=137237 num_tokens/piece=5.8943\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=23283 obj=9.02901 num_tokens=137264 num_tokens/piece=5.89546\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17462 obj=9.18606 num_tokens=148286 num_tokens/piece=8.49193\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17462 obj=9.14029 num_tokens=148307 num_tokens/piece=8.49313\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=13096 obj=9.332 num_tokens=159137 num_tokens/piece=12.1516\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=13096 obj=9.27901 num_tokens=159165 num_tokens/piece=12.1537\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9822 obj=9.49985 num_tokens=169102 num_tokens/piece=17.2167\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9822 obj=9.44159 num_tokens=169132 num_tokens/piece=17.2197\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7366 obj=9.69076 num_tokens=178821 num_tokens/piece=24.2765\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7366 obj=9.62656 num_tokens=178856 num_tokens/piece=24.2813\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5524 obj=9.89609 num_tokens=188537 num_tokens/piece=34.1305\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5524 obj=9.82624 num_tokens=188569 num_tokens/piece=34.1363\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4143 obj=10.1191 num_tokens=197577 num_tokens/piece=47.6894\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4143 obj=10.0451 num_tokens=197589 num_tokens/piece=47.6923\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3107 obj=10.3639 num_tokens=207374 num_tokens/piece=66.7441\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3107 obj=10.2844 num_tokens=207431 num_tokens/piece=66.7625\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2330 obj=10.6136 num_tokens=216774 num_tokens/piece=93.0361\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2330 obj=10.5321 num_tokens=217028 num_tokens/piece=93.1451\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1747 obj=10.8756 num_tokens=227192 num_tokens/piece=130.047\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1747 obj=10.7832 num_tokens=227242 num_tokens/piece=130.076\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1310 obj=11.1628 num_tokens=236860 num_tokens/piece=180.809\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1310 obj=11.0654 num_tokens=237459 num_tokens/piece=181.266\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=982 obj=11.4372 num_tokens=247838 num_tokens/piece=252.381\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=982 obj=11.3362 num_tokens=248445 num_tokens/piece=252.999\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=736 obj=11.7027 num_tokens=262201 num_tokens/piece=356.251\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=736 obj=11.5804 num_tokens=262247 num_tokens/piece=356.314\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=552 obj=11.9464 num_tokens=277293 num_tokens/piece=502.342\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=552 obj=11.8201 num_tokens=277355 num_tokens/piece=502.455\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=414 obj=12.2067 num_tokens=291825 num_tokens/piece=704.891\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=414 obj=12.0741 num_tokens=291831 num_tokens/piece=704.906\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=310 obj=12.5139 num_tokens=305768 num_tokens/piece=986.348\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=310 obj=12.3674 num_tokens=305768 num_tokens/piece=986.348\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=232 obj=12.8392 num_tokens=321273 num_tokens/piece=1384.8\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=232 obj=12.6786 num_tokens=321304 num_tokens/piece=1384.93\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=174 obj=13.2047 num_tokens=336140 num_tokens/piece=1931.84\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=174 obj=13.0128 num_tokens=336140 num_tokens/piece=1931.84\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=140 obj=13.4554 num_tokens=353695 num_tokens/piece=2526.39\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=140 obj=13.27 num_tokens=353695 num_tokens/piece=2526.39\ntrainer_interface.cc(687) LOG(INFO) Saving model: /kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128/tokenizer.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: /kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128/tokenizer.vocab\nSerialized tokenizer at location : /kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install aiohttp==3.9.2\n!pip install boto3 --upgrade\n#exit()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T19:51:40.785448Z","iopub.execute_input":"2024-07-02T19:51:40.785790Z","iopub.status.idle":"2024-07-02T19:52:12.653649Z","shell.execute_reply.started":"2024-07-02T19:51:40.785758Z","shell.execute_reply":"2024-07-02T19:52:12.652483Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting aiohttp==3.9.2\n  Downloading aiohttp-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.2) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.2) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.2) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.2) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.2) (1.9.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp==3.9.2) (4.0.3)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp==3.9.2) (3.6)\nDownloading aiohttp-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: aiohttp\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.9.1\n    Uninstalling aiohttp-3.9.1:\n      Successfully uninstalled aiohttp-3.9.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.13.0 requires botocore<1.34.107,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiohttp-3.9.2\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.26.100)\nCollecting boto3\n  Downloading boto3-1.34.138-py3-none-any.whl.metadata (6.6 kB)\nCollecting botocore<1.35.0,>=1.34.138 (from boto3)\n  Downloading botocore-1.34.138-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\nCollecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.138->boto3) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.138->boto3) (1.26.18)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.138->boto3) (1.16.0)\nDownloading boto3-1.34.138-py3-none-any.whl (139 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading botocore-1.34.138-py3-none-any.whl (12.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: botocore, s3transfer, boto3\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.29.165\n    Uninstalling botocore-1.29.165:\n      Successfully uninstalled botocore-1.29.165\n  Attempting uninstall: s3transfer\n    Found existing installation: s3transfer 0.6.2\n    Uninstalling s3transfer-0.6.2:\n      Successfully uninstalled s3transfer-0.6.2\n  Attempting uninstall: boto3\n    Found existing installation: boto3 1.26.100\n    Uninstalling boto3-1.26.100:\n      Successfully uninstalled boto3-1.26.100\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.13.0 requires botocore<1.34.107,>=1.34.70, but you have botocore 1.34.138 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed boto3-1.34.138 botocore-1.34.138 s3transfer-0.10.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ['TOKENIZER'] = '/kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128'\nos.environ['TRAIN_MANIFEST'] = '/kaggle/input/aic-manifests/train_manifest.json'\nos.environ['VAL_MANIFEST'] = '/kaggle/input/aic-manifests/test_manifest.json'\n\n# for adapt\n# os.environ['TRAIN_MANIFEST'] = '/kaggle/input/adapt-split-manifest/data_85.json'\n# os.environ['VAL_MANIFEST'] = '/kaggle/input/adapt-split-manifest/data_15.json'\n\nimport nemo\nfrom omegaconf import OmegaConf, open_dict\nimport nemo.collections.asr as nemo_asr","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:52:12.655187Z","iopub.execute_input":"2024-07-02T19:52:12.655500Z","iopub.status.idle":"2024-07-02T19:52:24.096107Z","shell.execute_reply.started":"2024-07-02T19:52:12.655469Z","shell.execute_reply":"2024-07-02T19:52:24.095332Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!mkdir configs\nBRANCH = 'r2.0.0rc0'\n!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/conformer/conformer_ctc_bpe.yaml","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:52:24.097196Z","iopub.execute_input":"2024-07-02T19:52:24.097686Z","iopub.status.idle":"2024-07-02T19:52:26.129939Z","shell.execute_reply.started":"2024-07-02T19:52:24.097659Z","shell.execute_reply":"2024-07-02T19:52:26.129021Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"[NeMo W 2024-07-02 19:52:24 nemo_logging:349] /opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n      pid, fd = os.forkpty()\n    \n","output_type":"stream"},{"name":"stdout","text":"--2024-07-02 19:52:25--  https://raw.githubusercontent.com/NVIDIA/NeMo/r2.0.0rc0/examples/asr/conf/conformer/conformer_ctc_bpe.yaml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10228 (10.0K) [text/plain]\nSaving to: 'configs/conformer_ctc_bpe.yaml'\n\nconformer_ctc_bpe.y 100%[===================>]   9.99K  --.-KB/s    in 0s      \n\n2024-07-02 19:52:26 (52.4 MB/s) - 'configs/conformer_ctc_bpe.yaml' saved [10228/10228]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from omegaconf import OmegaConf, open_dict\n\nparams = OmegaConf.load(\"/kaggle/working/configs/conformer_ctc_bpe.yaml\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:52:26.131383Z","iopub.execute_input":"2024-07-02T19:52:26.131703Z","iopub.status.idle":"2024-07-02T19:52:26.184661Z","shell.execute_reply.started":"2024-07-02T19:52:26.131673Z","shell.execute_reply":"2024-07-02T19:52:26.183805Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/NeMo.git","metadata":{"execution":{"iopub.status.busy":"2024-07-02T16:37:53.319695Z","iopub.execute_input":"2024-07-02T16:37:53.320574Z","iopub.status.idle":"2024-07-02T16:38:06.962254Z","shell.execute_reply.started":"2024-07-02T16:37:53.320534Z","shell.execute_reply":"2024-07-02T16:38:06.961114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuration updates\nparams.trainer.precision=32 # this set precision of the model to values [32,16]\nparams.model.encoder.dropout = 0.2\nparams.model.encoder.dropout_emb = 0.1","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:53:39.706990Z","iopub.execute_input":"2024-07-02T19:53:39.707706Z","iopub.status.idle":"2024-07-02T19:53:39.712795Z","shell.execute_reply.started":"2024-07-02T19:53:39.707674Z","shell.execute_reply":"2024-07-02T19:53:39.711737Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/final-dataset-35-epoch/NeMo \"/kaggle/working/\"\n!cp -r /kaggle/input/final-dataset-35-epoch/configs \"/kaggle/working/\"\n!cp -r /kaggle/input/final-dataset-35-epoch/results \"/kaggle/working/\"\n!cp -r /kaggle/input/final-dataset-35-epoch/scripts \"/kaggle/working/\"\n!cp -r /kaggle/input/final-dataset-35-epoch/tokinzers \"/kaggle/working/\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:53:43.351434Z","iopub.execute_input":"2024-07-02T19:53:43.352078Z","iopub.status.idle":"2024-07-02T19:55:28.852924Z","shell.execute_reply.started":"2024-07-02T19:53:43.352047Z","shell.execute_reply":"2024-07-02T19:55:28.851437Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"OmegaConf.save(params, \"/kaggle/working/NeMo/examples/asr/conf/conformer/conformer_ctc_bpe.yaml\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:55:28.855002Z","iopub.execute_input":"2024-07-02T19:55:28.855322Z","iopub.status.idle":"2024-07-02T19:55:28.876277Z","shell.execute_reply.started":"2024-07-02T19:55:28.855291Z","shell.execute_reply":"2024-07-02T19:55:28.875532Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/final-dataset-35-epoch \"/kaggle/working/\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T18:51:07.933416Z","iopub.execute_input":"2024-07-02T18:51:07.934278Z","iopub.status.idle":"2024-07-02T18:52:03.124557Z","shell.execute_reply.started":"2024-07-02T18:51:07.934219Z","shell.execute_reply":"2024-07-02T18:52:03.123219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login(key='ffbdfe896293a7e939b6775de3fe55e2abd1a0fc')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T18:53:35.292025Z","iopub.execute_input":"2024-07-02T18:53:35.292883Z","iopub.status.idle":"2024-07-02T18:53:39.168986Z","shell.execute_reply.started":"2024-07-02T18:53:35.292846Z","shell.execute_reply":"2024-07-02T18:53:39.168148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FAdam","metadata":{}},{"cell_type":"code","source":"write = '''import types\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nimport pytorch_lightning as L\nfrom pytorch_lightning.utilities.types import OptimizerLRScheduler\nfrom torch.optim import Optimizer\n\nfrom nemo.lightning.megatron_parallel import CallbackMethods\n\n\nclass LRSchedulerModule(L.Callback, CallbackMethods, ABC):\n    \"\"\"A module to standardize the learning rate scheduler setup and configuration.\n\n    This class decouples the learning rate scheduler from the model, similar to how the LightningDataModule\n    decouples data handling. It also acts as a Callback to hook into the training loop, which can be useful\n    for adding custom all-reduces, logging, early stopping, etc. Next to that standard Lightning callback-event,\n    this also supports hooking into the Megatron forward-backward function at a granular level.\n\n    Example::\n\n        class MyLRSchedulerModule(LRSchedulerModule):\n            def setup(self, model, optimizer):\n                # Custom setup logic\n                ...\n\n            def scheduler(self, model, optimizers):\n                # Define and return the learning rate scheduler\n                ...\n\n    Methods:\n        setup(model, optimizer): Sets up the learning rate scheduler.\n        scheduler(model, optimizers): Abstract method to define the learning rate scheduler.\n        __call__(model, optimizers): Calls the setup and scheduler methods.\n    \"\"\"\n\n    def connect(self, model, optimizer) -> None:\n        \"\"\"Sets up the learning rate scheduler.\n\n        Args:\n            model: The model for which the scheduler is being set up.\n            optimizer: The optimizer for which the scheduler is being set up.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def scheduler(self, model, optimizers) -> OptimizerLRScheduler:\n        \"\"\"Abstract method to define the learning rate scheduler.\n\n        Args:\n            model: The model for which the scheduler is being defined.\n            optimizers: The optimizers for which the scheduler is being defined.\n\n        Returns:\n            OptimizerLRScheduler: The learning rate scheduler.\n        \"\"\"\n        raise NotImplementedError(\"The scheduler method should be implemented by subclasses.\")\n\n    def __call__(self, model, optimizers):\n        \"\"\"Calls the setup and scheduler methods.\n\n        Args:\n            model: The model for which the scheduler is being called.\n            optimizers: The optimizers for which the scheduler is being called.\n\n        Returns:\n            OptimizerLRScheduler: The learning rate scheduler.\n        \"\"\"\n\n        self.connect(model, optimizers)\n\n        self._scheduler = self.scheduler(model, optimizers)\n\n        if not isinstance(self._scheduler, (dict, tuple)):\n            return optimizers, self._scheduler\n\n        return self._scheduler\n\n\nclass OptimizerModule(L.Callback, CallbackMethods, ABC):\n    \"\"\"A module to standardize the optimizer setup and configuration.\n\n    This class decouples the optimizer from the model, similar to how the LightningDataModule\n    decouples data handling. It also acts as a Callback to hook into the training loop, which can be useful\n    for adding custom all-reduces, logging, early stopping, etc. Next to that standard Lightning callback-event,\n    this also supports hooking into the Megatron forward-backward function at a granular level.\n\n    Attributes:\n        lr_scheduler (Optional[LRSchedulerModule]): The learning rate scheduler module.\n\n    Example::\n\n        class MyOptimizerModule(OptimizerModule):\n            def __init__(self, lr_scheduler=None):\n                super().__init__(lr_scheduler)\n\n            def setup(self, model):\n                # Custom setup logic\n                ...\n\n            def optimizers(self, model):\n                # Define and return the optimizers\n                ...\n\n    Methods:\n        connect(model, trainer): Connects the optimizer module to the model and trainer.\n        setup(model): Sets up the optimizer.\n        optimizers(model): Abstract method to define the optimizers.\n        __call__(model, megatron_parallel): Calls the setup and optimizers methods.\n    \"\"\"\n\n    def __init__(self, lr_scheduler: Optional[LRSchedulerModule]):\n        \"\"\"Initializes the OptimizerModule.\n\n        Args:\n            lr_scheduler (Optional[LRSchedulerModule]): The learning rate scheduler module.\n        \"\"\"\n        self.lr_scheduler = lr_scheduler\n\n    def connect(self, model: L.LightningModule) -> None:\n        \"\"\"Connects the optimizer module to the model and trainer.\n\n        Args:\n            model (L.LightningModule): The model to which the optimizer module is being connected.\n        \"\"\"\n\n        def custom_configure_optimizers(lightning_module_self, megatron_parallel=None):\n            opt = self(lightning_module_self, megatron_parallel=megatron_parallel)\n            return opt\n\n        model.configure_optimizers = types.MethodType(custom_configure_optimizers, model)\n        model.optim = self\n\n        if hasattr(self, \"__io__\") and hasattr(model, \"__io__\"):\n            if hasattr(model.__io__, \"optim\"):\n                model.__io__.optim = self.__io__\n\n    @abstractmethod\n    def optimizers(self, model) -> List[Optimizer]:\n        \"\"\"Abstract method to define the optimizers.\n\n        Args:\n            model: The model for which the optimizers are being defined.\n\n        Returns:\n            List[Optimizer]: The list of optimizers.\n        \"\"\"\n        raise NotImplementedError(\"The optimizers method should be implemented by subclasses.\")\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx) -> None:\n        if self._optimizers is not None:\n            lr = self._optimizers[0].param_groups[0]['lr']\n            pl_module.log('lr', lr, rank_zero_only=True, batch_size=1)\n\n    def __call__(self, model: L.LightningModule, megatron_parallel=None) -> OptimizerLRScheduler:\n        \"\"\"Calls the setup and optimizers methods.\n\n        Args:\n            model (L.LightningModule): The model for which the optimizers are being called.\n            megatron_parallel: Optional parallel model.\n\n        Returns:\n            OptimizerLRScheduler: The optimizers and optionally the learning rate scheduler.\n        \"\"\"\n        _model = model if megatron_parallel is None else megatron_parallel\n        callbacks = _model.trainer.callbacks\n        if self not in callbacks:\n            callbacks.append(self)\n        if self.lr_scheduler is not None and self.lr_scheduler not in callbacks:\n            callbacks.append(self.lr_scheduler)\n\n        self._optimizers = self.optimizers(_model)\n\n        _opt = self._optimizers[0] if len(self._optimizers) == 1 else self._optimizers\n\n        if self.lr_scheduler is not None:\n            with_scheduler = self.lr_scheduler(_model, _opt)\n\n            return with_scheduler\n\n        return self._optimizers\n\n\nclass MyOptimizerModule(OptimizerModule):\n    def optimizers(self, model) -> List[Optimizer]:\n        optimizer = fAdam(model.parameters(), lr=5.0, weight_decay=5e-4, betas=(0.9, 0.98))\n        print(\"From fAdam\")\n        return [optimizer]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/lightning/pytorch/optim/base.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:55:52.323398Z","iopub.execute_input":"2024-07-02T19:55:52.323993Z","iopub.status.idle":"2024-07-02T19:55:52.336000Z","shell.execute_reply.started":"2024-07-02T19:55:52.323962Z","shell.execute_reply":"2024-07-02T19:55:52.334977Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"write = '''from typing import Any, Callable, List, Mapping, Optional\n\nimport pytorch_lightning as pl\nfrom megatron.core.distributed import finalize_model_grads\nfrom megatron.core.optimizer import OptimizerConfig, get_megatron_optimizer\nfrom megatron.core.utils import get_model_config\nfrom torch.optim import Optimizer\n\nfrom nemo.lightning.megatron_parallel import MegatronParallel\nfrom nemo.lightning.pytorch.optim.base import LRSchedulerModule, MyOptimizerModule\n\n\nclass MegatronOptimizerModule(MyOptimizerModule):\n    \"\"\"A MyOptimizerModule for the megatron optimizers.\n\n    Attributes:\n        config (OptimizerConfig): Configuration for the optimizer.\n        no_weight_decay_cond (Optional[Callable]): Condition for no weight decay.\n        scale_lr_cond (Optional[Callable]): Condition for scaling learning rate.\n        lr_mult (float): Learning rate multiplier.\n\n    Example::\n\n        config = OptimizerConfig(...)\n        lr_scheduler = MyLRSchedulerModule(...)\n        optimizer_module = MegatronOptimizerModule(config, lr_scheduler)\n\n    Methods:\n        setup(model): Sets up the optimizer.\n        optimizers(model): Defines the optimizers.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: OptimizerConfig,\n        lr_scheduler: Optional[LRSchedulerModule] = None,\n        no_weight_decay_cond: Optional[Callable] = None,\n        scale_lr_cond: Optional[Callable] = None,\n        lr_mult: float = 1.0,\n    ):\n        \"\"\"Initializes the MegatronOptimizerModule.\n\n        Args:\n            config (OptimizerConfig): Configuration for the optimizer.\n            lr_scheduler (Optional[LRSchedulerModule]): The learning rate scheduler module.\n            no_weight_decay_cond (Optional[Callable]): Condition for no weight decay.\n            scale_lr_cond (Optional[Callable]): Condition for scaling learning rate.\n            lr_mult (float): Learning rate multiplier.\n        \"\"\"\n\n        super().__init__(lr_scheduler=lr_scheduler)\n        self.config = config\n        self.no_weight_decay_cond = no_weight_decay_cond\n        self.scale_lr_cond = scale_lr_cond\n        self.lr_mult = lr_mult\n\n    def setup(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", stage: str):\n        \"\"\"We will add the finalize_model_grads function to the model config.\n\n        Args:\n            model: The model for which the optimizer is being set up.\n        \"\"\"\n\n        def finalize_model_grads_func(*args, **kwargs):\n            return self.finalize_model_grads(*args, **kwargs)\n\n        get_model_config(pl_module).finalize_model_grads_func = finalize_model_grads_func\n\n    def optimizers(self, model: MegatronParallel) -> List[Optimizer]:\n        \"\"\"Defines the optimizers.\n\n        Args:\n            model (MegatronParallel): The model for which the optimizers are being defined.\n\n        Returns:\n            List[Optimizer]: The list of optimizers.\n\n        Raises:\n            ValueError: If the model is not an instance of MegatronParallel.\n        \"\"\"\n\n        if not isinstance(model, MegatronParallel):\n            raise ValueError(\"Model must be an instance of MegatronParallel\")\n\n        from nemo.core.optim import McoreDistributedOptimizer\n\n        class McoreOpt(McoreDistributedOptimizer):\n            def sharded_state_dict(\n                self,\n                model_sharded_state_dict,\n                optimizer_state_dict=None,\n                is_loading=False,\n                # dist_ckpt_parallel_save=False, ## TODO: fix!\n            ):\n                # sharding_type = 'fully_sharded_model_space' if dist_ckpt_parallel_save else 'dp_zero_gather_scatter'\n                sharding_type = 'dp_zero_gather_scatter'\n                state_dict = self.mcore_optimizer.sharded_state_dict(\n                    model_sharded_state_dict, is_loading=is_loading, sharding_type=sharding_type\n                )\n                return state_dict\n\n        mcore_opt = get_megatron_optimizer(\n            self.config,\n            list(model),\n            no_weight_decay_cond=self.no_weight_decay_cond,\n            scale_lr_cond=self.scale_lr_cond,\n            lr_mult=self.lr_mult,\n        )\n\n        return [McoreOpt(mcore_opt)]\n\n    def finalize_model_grads(self, *args, **kwargs):\n        return finalize_model_grads(*args, **kwargs)\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/lightning/pytorch/optim/megatron.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:56:07.144388Z","iopub.execute_input":"2024-07-02T19:56:07.144768Z","iopub.status.idle":"2024-07-02T19:56:07.154010Z","shell.execute_reply.started":"2024-07-02T19:56:07.144738Z","shell.execute_reply":"2024-07-02T19:56:07.152884Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"write = '''from typing import Union\n\nfrom lightning_fabric.plugins.environments import slurm\nfrom pytorch_lightning import plugins as _pl_plugins\n\n# This is here to import it once, which improves the speed of launch when in debug-mode\ntry:\n    import transformer_engine  # noqa\nexcept ImportError:\n    pass\n\nfrom nemo.lightning.base import get_vocab_size, teardown\nfrom nemo.lightning.nemo_logger import NeMoLogger\nfrom nemo.lightning.pytorch.callbacks.megatron_model_checkpoint import ModelCheckpoint\nfrom nemo.lightning.pytorch.optim import LRSchedulerModule, MegatronOptimizerModule, MyOptimizerModule\nfrom nemo.lightning.pytorch.plugins import MegatronDataSampler, MegatronMixedPrecision\nfrom nemo.lightning.pytorch.plugins import data_sampler as _data_sampler\nfrom nemo.lightning.pytorch.strategies import MegatronStrategy\nfrom nemo.lightning.pytorch.trainer import Trainer\nfrom nemo.lightning.resume import AutoResume\n\n\n# We monkey patch because nvidia uses a naming convention for SLURM jobs\ndef _is_slurm_interactive_mode():\n    job_name = slurm.SLURMEnvironment.job_name()\n    return job_name is None or job_name.endswith(\"bash\") or job_name.endswith(\"interactive\")\n\n\nslurm._is_slurm_interactive_mode = _is_slurm_interactive_mode  # noqa: SLF001\n\n\n_pl_plugins._PLUGIN_INPUT = Union[_pl_plugins._PLUGIN_INPUT, _data_sampler.DataSampler]  # noqa: SLF001\n\n\n__all__ = [\n    \"AutoResume\",\n    \"LRSchedulerModule\",\n    \"MegatronStrategy\",\n    \"MegatronDataSampler\",\n    \"MegatronMixedPrecision\",\n    \"MegatronOptimizerModule\",\n    \"NeMoLogger\",\n    \"ModelCheckpoint\",\n    \"MyOptimizerModule\",\n    \"Trainer\",\n    \"get_vocab_size\",\n    \"teardown\",\n]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/lightning/__init__.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:56:12.966800Z","iopub.execute_input":"2024-07-02T19:56:12.967433Z","iopub.status.idle":"2024-07-02T19:56:12.973805Z","shell.execute_reply.started":"2024-07-02T19:56:12.967402Z","shell.execute_reply":"2024-07-02T19:56:12.972905Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"write = '''from pathlib import Path\nfrom typing import Callable, Optional\n\nimport pytorch_lightning as pl\nfrom typing_extensions import Annotated\n\nfrom nemo.collections.llm.utils import Config, task\nfrom nemo.lightning import AutoResume, MegatronStrategy, NeMoLogger, MyOptimizerModule, Trainer, io, teardown\n\n\n@task(namespace=\"llm\")\ndef train(\n    model: pl.LightningModule,\n    data: pl.LightningDataModule,\n    trainer: Trainer,\n    log: Annotated[Optional[NeMoLogger], Config[NeMoLogger]] = None,\n    resume: Annotated[Optional[AutoResume], Config[AutoResume]] = None,\n    optim: Optional[MyOptimizerModule] = None,\n    tokenizer: Optional[str] = None,\n    # TODO: Fix export export: Optional[str] = None,\n) -> Path:\n    \"\"\"\n    Trains a model using the specified data and trainer, with optional tokenizer, source, and export.\n\n    Args:\n        model (pl.LightningModule): The model to be trained.\n        data (pl.LightningDataModule): The data module containing training data.\n        trainer (Trainer): The trainer instance configured with a MegatronStrategy.\n        log (NeMoLogger): A nemologger instance.\n        resume (Optional[Union[AutoResume, Resume]]): Resume training from a checkpoint.\n        optim (Optional[OptimizerModule]): The optimizer module to be used. If not provided, the default optimizer\n            from the model will be used.\n        tokenizer (Optional[str]): Tokenizer setting to be applied. Can be 'data' or 'model'.\n        export (Optional[str]): Filename to save the exported checkpoint after training.\n\n    Returns\n    -------\n        Path: The directory path where training artifacts are saved.\n\n    Raises\n    ------\n        ValueError: If the trainer's strategy is not MegatronStrategy.\n\n    Examples\n    --------\n        >>> model = MyModel()\n        >>> data = MyDataModule()\n        >>> trainer = Trainer(strategy=MegatronStrategy())\n        >>> train(model, data, trainer, tokenizer='data', source='path/to/ckpt.ckpt', export='final.ckpt')\n        PosixPath('/path/to/log_dir')\n    \"\"\"\n    _log = log or NeMoLogger()\n    app_state = _log.setup(\n        trainer,\n        resume_if_exists=getattr(resume, \"resume_if_exists\", False),\n        task_config=getattr(train, \"__io__\", None),\n    )\n    if resume is not None:\n        resume.setup(model, trainer)\n    if optim:\n        optim.connect(model)\n    if tokenizer:  # TODO: Improve this\n        _use_tokenizer(model, data, tokenizer)\n\n    trainer.fit(model, data)\n\n    _log.teardown()\n\n    return app_state.exp_dir\n\n\n@task(namespace=\"llm\")\ndef pretrain(\n    model: pl.LightningModule,\n    data: pl.LightningDataModule,\n    trainer: Trainer,\n    source: Optional[str] = None,\n    # export: Optional[str] = None\n) -> Path:\n    return train(model=model, data=data, trainer=trainer, tokenizer=\"data\", source=source)\n\n\n@task(namespace=\"llm\")\ndef validate(\n    model: pl.LightningModule,\n    data: pl.LightningDataModule,\n    trainer: Trainer,\n    tokenizer: Optional[str] = None,\n    source: Optional[str] = None,\n    export: Optional[str] = None,\n) -> Path:\n    if not isinstance(trainer.strategy, MegatronStrategy):\n        raise ValueError(\"Only MegatronStrategy is supported\")\n\n    validate_kwargs = {}\n    run_dir = Path(trainer.logger.log_dir)\n    export_dir = run_dir / \"export\"\n\n    if tokenizer:  # TODO: Improve this\n        _use_tokenizer(model, data, tokenizer)\n    if source:\n        _add_ckpt_path(source, model, validate_kwargs)\n\n    trainer.validate(model, data, **validate_kwargs)\n    trainer.save_checkpoint(export_dir)\n    if export:\n        teardown(trainer)\n        del trainer, model, data\n        export_ckpt(export_dir, export)\n\n    return run_dir\n\n\n@task(name=\"import\", namespace=\"llm\")\ndef import_ckpt(\n    model: pl.LightningModule,\n    source: str,\n    output_path: Optional[Path] = None,\n    overwrite: bool = False,\n) -> Path:\n    return io.import_ckpt(model=model, source=source, output_path=output_path, overwrite=overwrite)\n\n\ndef load_connector_from_trainer_ckpt(path: Path, target: str) -> io.ModelConnector:\n    return io.load_ckpt(path).model.exporter(target, path)\n\n\n@task(name=\"export\", namespace=\"llm\")\ndef export_ckpt(\n    path: Path,\n    target: str,\n    output_path: Optional[Path] = None,\n    overwrite: bool = False,\n    load_connector: Callable[[Path, str], io.ModelConnector] = load_connector_from_trainer_ckpt,\n) -> Path:\n    return io.export_ckpt(path, target, output_path, overwrite, load_connector)\n\n\ndef _use_tokenizer(model: pl.LightningModule, data: pl.LightningDataModule, tokenizer: str) -> None:\n    if tokenizer == \"data\":\n        model.tokenizer = data.tokenizer\n    elif tokenizer == \"model\":\n        data.tokenizer = model.tokenizer\n\n\ndef _add_ckpt_path(source, model, kwargs) -> None:\n    if io.is_distributed_ckpt(source):\n        kwargs[\"ckpt_path\"] = source\n    else:\n        kwargs[\"ckpt_path\"] = model.import_ckpt(source)\n\n\ndef _save_config_img(*args, **kwargs):\n    try:\n        from nemo_sdk.utils import save_config_img\n\n        save_config_img(*args, **kwargs)\n    except ImportError:\n        pass\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/api.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:56:21.902497Z","iopub.execute_input":"2024-07-02T19:56:21.902885Z","iopub.status.idle":"2024-07-02T19:56:21.912492Z","shell.execute_reply.started":"2024-07-02T19:56:21.902856Z","shell.execute_reply":"2024-07-02T19:56:21.911469Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"write = '''\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Annotated, Callable, Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom nemo.collections.llm.gpt.model.base import GPTConfig, GPTModel\nfrom nemo.collections.llm.utils import Config\nfrom nemo.lightning import MyOptimizerModule, io, teardown\n\nif TYPE_CHECKING:\n    from transformers import LlamaConfig as HFLlamaConfig\n    from transformers import LlamaForCausalLM\n\n    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n    from nemo.collections.common.tokenizers.tokenizer_spec import TokenizerSpec\n\n\n# Note: these Llama configs are copied from the corresponding HF model. You may need to modify the parameter for\n# your own needs, in particular: seq_length and rotary_base.\n@dataclass\nclass LlamaConfig(GPTConfig):\n    # configs that are common across model sizes\n    normalization: str = \"RMSNorm\"\n    activation_func: Callable = F.silu\n    gated_linear_unit: bool = True\n    position_embedding_type: str = \"rope\"\n    add_bias_linear: bool = False\n    seq_length: int = 4096\n\n\n@dataclass\nclass Llama2Config7B(LlamaConfig):\n    num_layers: int = 32\n    hidden_size: int = 4096\n    num_attention_heads: int = 32\n    num_query_groups: int = 32\n    ffn_hidden_size: int = 11008\n\n\n@dataclass\nclass Llama2Config13B(LlamaConfig):\n    num_layers: int = 40\n    hidden_size: int = 5120\n    num_attention_heads: int = 40\n    num_query_groups: int = 40\n    ffn_hidden_size: int = 13824\n\n\n@dataclass\nclass Llama2Config70B(LlamaConfig):\n    num_layers: int = 80\n    hidden_size: int = 8192\n    num_attention_heads: int = 64\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 28672\n\n\n@dataclass\nclass Llama3Config8B(Llama2Config7B):\n    seq_length: int = 8192\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 14336\n\n\n@dataclass\nclass Llama3Config70B(Llama2Config70B):\n    seq_length: int = 8192\n\n\n@dataclass\nclass CodeLlamaConfig7B(Llama2Config7B):\n    rotary_base: int = 1_000_000\n    seq_length: int = 16384\n\n\n@dataclass\nclass CodeLlamaConfig13B(Llama2Config13B):\n    rotary_base: int = 1_000_000\n    seq_length: int = 16384\n\n\n@dataclass\nclass CodeLlamaConfig34B(LlamaConfig):\n    num_layers: int = 48\n    hidden_size: int = 8192\n    num_attention_heads: int = 64\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 22016\n    rotary_base: int = 1_000_000\n    seq_length: int = 16384\n\n\n@dataclass\nclass CodeLlamaConfig70B(Llama2Config70B):\n    pass\n\n\nclass LlamaModel(GPTModel):\n    def __init__(\n        self,\n        config: Annotated[Optional[LlamaConfig], Config[LlamaConfig]] = None,\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__(config or LlamaConfig(), optim=optim, tokenizer=tokenizer)\n\n\n@io.model_importer(LlamaModel, \"hf\")\nclass HFLlamaImporter(io.ModelConnector[\"LlamaForCausalLM\", LlamaModel]):\n    def init(self) -> LlamaModel:\n        return LlamaModel(self.config, tokenizer=self.tokenizer)\n\n    def apply(self, output_path: Path) -> Path:\n        from transformers import LlamaForCausalLM\n\n        source = LlamaForCausalLM.from_pretrained(str(self))\n        target = self.init()\n        trainer = self.nemo_setup(target)\n        self.convert_state(source, target)\n        self.nemo_save(output_path, trainer)\n\n        print(f\"Converted Llama model to Nemo, model saved to {output_path}\")\n\n        teardown(trainer, target)\n        del trainer, target\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"model.embed_tokens.weight\": \"embedding.word_embeddings.weight\",\n            \"model.layers.*.self_attn.o_proj.weight\": \"decoder.layers.*.self_attention.linear_proj.weight\",\n            \"model.layers.*.mlp.down_proj.weight\": \"decoder.layers.*.mlp.linear_fc2.weight\",\n            \"model.layers.*.input_layernorm.weight\": \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n            \"model.layers.*.post_attention_layernorm.weight\": \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\",\n            \"model.norm.weight\": \"decoder.final_layernorm.weight\",\n            \"lm_head.weight\": \"output_layer.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_import_qkv, _import_linear_fc1])\n\n    @property\n    def tokenizer(self) -> \"AutoTokenizer\":\n        from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n        return AutoTokenizer(str(self))\n\n    @property\n    def config(self) -> LlamaConfig:\n        from transformers import LlamaConfig as HFLlamaConfig\n\n        source = HFLlamaConfig.from_pretrained(str(self))\n\n        def make_vocab_size_divisible_by(vocab_size):\n            base = 128\n            while vocab_size % base != 0:\n                base //= 2\n            return base\n\n        output = LlamaConfig(\n            num_layers=source.num_hidden_layers,\n            hidden_size=source.hidden_size,\n            ffn_hidden_size=source.intermediate_size,\n            num_attention_heads=source.num_attention_heads,\n            init_method_std=source.initializer_range,\n            layernorm_epsilon=source.rms_norm_eps,\n            num_query_groups=source.num_key_value_heads,\n            rotary_base=source.rope_theta,\n            gated_linear_unit=True,\n            make_vocab_size_divisible_by=make_vocab_size_divisible_by(source.vocab_size),\n            share_embeddings_and_output_weights=False,\n        )\n\n        return output\n\n\n@io.model_exporter(LlamaModel, \"hf\")\nclass HFLlamaExporter(io.ModelConnector[LlamaModel, \"LlamaForCausalLM\"]):\n    def init(self) -> \"LlamaForCausalLM\":\n        from transformers import AutoModelForCausalLM\n\n        return AutoModelForCausalLM.from_config(self.config)\n\n    def apply(self, output_path: Path) -> Path:\n        target = self.init()\n        source, _ = self.nemo_load(str(self))\n        target = self.convert_state(source, target)\n\n        target = target.cpu()\n        target.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"embedding.word_embeddings.weight\": \"model.embed_tokens.weight\",\n            \"decoder.layers.*.self_attention.linear_proj.weight\": \"model.layers.*.self_attn.o_proj.weight\",\n            \"decoder.layers.*.mlp.linear_fc2.weight\": \"model.layers.*.mlp.down_proj.weight\",\n            \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\": \"model.layers.*.input_layernorm.weight\",\n            \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\": \"model.layers.*.post_attention_layernorm.weight\",\n            \"decoder.final_layernorm.weight\": \"model.norm.weight\",\n            \"output_layer.weight\": \"lm_head.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_export_qkv, _export_linear_fc1])\n\n    @property\n    def tokenizer(self):\n        return io.load_ckpt(str(self)).model.tokenizer.tokenizer\n\n    @property\n    def config(self) -> \"HFLlamaConfig\":\n        source: LlamaConfig = io.load_ckpt(str(self)).model.config\n\n        from transformers import LlamaConfig as HFLlamaConfig\n\n        return HFLlamaConfig(\n            num_hidden_layers=source.num_layers,\n            hidden_size=source.hidden_size,\n            intermediate_size=source.ffn_hidden_size,\n            num_attention_heads=source.num_attention_heads,\n            max_position_embeddings=source.seq_length,\n            initializer_range=source.init_method_std,\n            rms_norm_eps=source.layernorm_epsilon,\n            num_key_value_heads=source.num_query_groups,\n            rope_theta=source.rotary_base,\n            vocab_size=self.tokenizer.vocab_size,\n        )\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n    target_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n)\ndef _import_qkv(ctx: io.TransformCTX, q, k, v):\n    megatron_config = ctx.target.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n\n    old_tensor_shape = q.size()\n    new_q_tensor_shape = (head_num, head_size) + old_tensor_shape[1:]\n    new_kv_tensor_shape = (num_query_groups, head_size) + old_tensor_shape[1:]\n\n    q = q.view(*new_q_tensor_shape)\n    k = k.view(*new_kv_tensor_shape)\n    v = v.view(*new_kv_tensor_shape)\n\n    qkv_weights_l = []\n    for i in range(num_query_groups):\n        qkv_weights_l.append(q[i * heads_per_group : (i + 1) * heads_per_group, :, :])\n        qkv_weights_l.append(k[i : i + 1, :, :])\n        qkv_weights_l.append(v[i : i + 1, :, :])\n    qkv_weights = torch.cat(qkv_weights_l)\n    assert qkv_weights.ndim == 3, qkv_weights.shape\n    assert qkv_weights.shape[0] == (heads_per_group + 2) * num_query_groups, qkv_weights.shape\n    assert qkv_weights.shape[1] == head_size, qkv_weights.shape\n    assert qkv_weights.shape[2] == old_tensor_shape[1], qkv_weights.shape\n\n    qkv_weights = qkv_weights.reshape([head_size * (head_num + 2 * num_query_groups), hidden_size])\n\n    return qkv_weights\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n    target_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n)\ndef _export_qkv(ctx: io.TransformCTX, linear_qkv):\n    megatron_config = ctx.source.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n    qkv_total_dim = head_num + 2 * num_query_groups\n\n    linear_qkv = linear_qkv.reshape([qkv_total_dim, head_size, hidden_size])\n    q_slice = torch.cat(\n        [\n            torch.arange((heads_per_group + 2) * i, (heads_per_group + 2) * i + heads_per_group)\n            for i in range(num_query_groups)\n        ]\n    )\n    k_slice = torch.arange(heads_per_group, qkv_total_dim, (heads_per_group + 2))\n    v_slice = torch.arange(heads_per_group + 1, qkv_total_dim, (heads_per_group + 2))\n\n    q_proj = linear_qkv[q_slice].reshape(-1, hidden_size).cpu()\n    k_proj = linear_qkv[k_slice].reshape(-1, hidden_size).cpu()\n    v_proj = linear_qkv[v_slice].reshape(-1, hidden_size).cpu()\n\n    return q_proj, k_proj, v_proj\n\n\n@io.state_transform(\n    source_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n    target_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n)\ndef _import_linear_fc1(down, gate):\n    return torch.cat((down, gate), axis=0).float()\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n    target_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n)\ndef _export_linear_fc1(linear_fc1):\n    gate_proj, up_proj = torch.chunk(linear_fc1, 2, dim=0)\n\n    return gate_proj, up_proj\n\n\n__all__ = [\n    \"LlamaConfig\",\n    \"Llama2Config7B\",\n    \"Llama2Config13B\",\n    \"Llama2Config70B\",\n    \"Llama3Config8B\",\n    \"Llama3Config70B\",\n    \"CodeLlamaConfig7B\",\n    \"CodeLlamaConfig13B\",\n    \"CodeLlamaConfig34B\",\n    \"CodeLlamaConfig70B\",\n    \"LlamaModel\",\n]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/llama.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:56:31.516694Z","iopub.execute_input":"2024-07-02T19:56:31.517024Z","iopub.status.idle":"2024-07-02T19:56:31.533813Z","shell.execute_reply.started":"2024-07-02T19:56:31.517000Z","shell.execute_reply":"2024-07-02T19:56:31.532804Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"write = '''from dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Annotated, Callable, Optional\n\nimport torch\n\nfrom nemo.collections.llm.fn.activation import openai_gelu\nfrom nemo.collections.llm.gpt.model.base import GPTConfig, GPTModel\nfrom nemo.collections.llm.utils import Config\nfrom nemo.lightning import MyOptimizerModule, io, teardown\n\nif TYPE_CHECKING:\n    from transformers import GemmaForCausalLM\n\n    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n    from nemo.collections.common.tokenizers.tokenizer_spec import TokenizerSpec\n\n\n# Note: Gemma requires huggingface transformers >= 4.38\n# Note: these Gemma configs are copied from the corresponding HF model. You may need to modify the parameter for\n# your own needs, in particular: seq_length and rotary_base.\n@dataclass\nclass GemmaConfig(GPTConfig):\n    # configs that are common across model sizes\n    normalization: str = \"RMSNorm\"\n    activation_func: Callable = openai_gelu\n    gated_linear_unit: bool = True\n    position_embedding_type: str = \"rope\"\n    add_bias_linear: bool = False\n    seq_length: int = 8192\n    kv_channels: int = 256\n    share_embeddings_and_output_weights: bool = True\n    # Note: different behavior compared to Legacy NeMo\n    # Legacy NeMo does not set layernorm_zero_centered_gamma and instead adds 1 in the HF -> NeMo conversion script\n    # The present implementation is more in line with the official implementation\n    layernorm_zero_centered_gamma: bool = True\n\n\n@dataclass\nclass GemmaConfig2B(GemmaConfig):\n    num_layers: int = 18\n    hidden_size: int = 2048\n    num_attention_heads: int = 8\n    num_query_groups: int = 1\n    ffn_hidden_size: int = 16384\n\n\n@dataclass\nclass GemmaConfig7B(GemmaConfig):\n    num_layers: int = 28\n    hidden_size: int = 3072\n    num_attention_heads: int = 16\n    num_query_groups: int = 16\n    ffn_hidden_size: int = 24576\n\n\nclass CodeGemmaConfig2B(GemmaConfig2B):\n    pass\n\n\nclass CodeGemmaConfig7B(GemmaConfig7B):\n    pass\n\n\nclass GemmaModel(GPTModel):\n    def __init__(\n        self,\n        config: Annotated[Optional[GemmaConfig], Config[GemmaConfig]] = None,\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__(config or GemmaConfig(), optim=optim, tokenizer=tokenizer)\n\n\n@io.model_importer(GemmaModel, \"hf\")\nclass HFGemmaImporter(io.ModelConnector[\"GemmaForCausalLM\", GemmaModel]):\n    def init(self) -> GemmaModel:\n        return GemmaModel(self.config, tokenizer=self.tokenizer)\n\n    def apply(self, output_path: Path) -> Path:\n        from transformers import GemmaForCausalLM\n\n        source = GemmaForCausalLM.from_pretrained(str(self))\n        target = self.init()\n        trainer = self.nemo_setup(target)\n        self.convert_state(source, target)\n        self.nemo_save(output_path, trainer)\n\n        print(f\"Converted Gemma model to Nemo, model saved to {output_path}\")\n\n        teardown(trainer, target)\n        del trainer, target\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"model.embed_tokens.weight\": \"embedding.word_embeddings.weight\",\n            \"model.layers.*.self_attn.o_proj.weight\": \"decoder.layers.*.self_attention.linear_proj.weight\",\n            \"model.layers.*.mlp.down_proj.weight\": \"decoder.layers.*.mlp.linear_fc2.weight\",\n            \"model.layers.*.input_layernorm.weight\": \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n            \"model.layers.*.post_attention_layernorm.weight\": \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\",\n            \"model.norm.weight\": \"decoder.final_layernorm.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_import_qkv, _import_linear_fc1])\n\n    @property\n    def tokenizer(self) -> \"AutoTokenizer\":\n        from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n        return AutoTokenizer(str(self))\n\n    @property\n    def config(self) -> GemmaConfig:\n        from transformers import GemmaConfig as HFGemmaConfig\n\n        source = HFGemmaConfig.from_pretrained(str(self))\n\n        def make_vocab_size_divisible_by(vocab_size):\n            base = 128\n            while vocab_size % base != 0:\n                base //= 2\n            return base\n\n        output = GemmaConfig(\n            num_layers=source.num_hidden_layers,\n            hidden_size=source.hidden_size,\n            ffn_hidden_size=source.intermediate_size,\n            num_attention_heads=source.num_attention_heads,\n            init_method_std=source.initializer_range,\n            layernorm_epsilon=source.rms_norm_eps,\n            num_query_groups=source.num_key_value_heads,\n            rotary_base=source.rope_theta,\n            gated_linear_unit=True,\n            make_vocab_size_divisible_by=make_vocab_size_divisible_by(source.vocab_size),\n            share_embeddings_and_output_weights=False,\n        )\n\n        return output\n\n\n@io.model_exporter(GemmaModel, \"hf\")\nclass HFGemmaExporter(io.ModelConnector[GemmaModel, \"GemmaForCausalLM\"]):\n    def init(self) -> \"GemmaForCausalLM\":\n        from transformers import AutoModelForCausalLM\n\n        return AutoModelForCausalLM.from_config(self.config)\n\n    def apply(self, output_path: Path) -> Path:\n        target = self.init()\n        source, _ = self.nemo_load(str(self))\n        target = self.convert_state(source, target)\n\n        target = target.cpu()\n        target.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"embedding.word_embeddings.weight\": \"model.embed_tokens.weight\",\n            \"decoder.layers.*.self_attention.linear_proj.weight\": \"model.layers.*.self_attn.o_proj.weight\",\n            \"decoder.layers.*.mlp.linear_fc2.weight\": \"model.layers.*.mlp.down_proj.weight\",\n            \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\": \"model.layers.*.input_layernorm.weight\",\n            \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\": \"model.layers.*.post_attention_layernorm.weight\",\n            \"decoder.final_layernorm.weight\": \"model.norm.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_export_qkv, _export_linear_fc1])\n\n    @property\n    def tokenizer(self):\n        return io.load_ckpt(str(self)).model.tokenizer.tokenizer\n\n    @property\n    def config(self) -> \"GemmaConfig\":\n        source: GemmaConfig = io.load_ckpt(str(self)).model.config\n\n        from transformers import GemmaConfig as HFGemmaConfig\n\n        return HFGemmaConfig(\n            num_hidden_layers=source.num_layers,\n            hidden_size=source.hidden_size,\n            intermediate_size=source.ffn_hidden_size,\n            num_attention_heads=source.num_attention_heads,\n            max_position_embeddings=source.seq_length,\n            initializer_range=source.init_method_std,\n            rms_norm_eps=source.layernorm_epsilon,\n            num_key_value_heads=source.num_query_groups,\n            vocab_size=self.tokenizer.vocab_size,\n        )\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n    target_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n)\ndef _import_qkv(ctx: io.TransformCTX, q, k, v):\n    megatron_config = ctx.target.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n\n    old_tensor_shape = q.size()\n    new_q_tensor_shape = (head_num, head_size) + old_tensor_shape[1:]\n    new_kv_tensor_shape = (num_query_groups, head_size) + old_tensor_shape[1:]\n\n    q = q.view(*new_q_tensor_shape)\n    k = k.view(*new_kv_tensor_shape)\n    v = v.view(*new_kv_tensor_shape)\n\n    qkv_weights_l = []\n    for i in range(num_query_groups):\n        qkv_weights_l.append(q[i * heads_per_group : (i + 1) * heads_per_group, :, :])\n        qkv_weights_l.append(k[i : i + 1, :, :])\n        qkv_weights_l.append(v[i : i + 1, :, :])\n    qkv_weights = torch.cat(qkv_weights_l)\n    assert qkv_weights.ndim == 3, qkv_weights.shape\n    assert qkv_weights.shape[0] == (heads_per_group + 2) * num_query_groups, qkv_weights.shape\n    assert qkv_weights.shape[1] == head_size, qkv_weights.shape\n    assert qkv_weights.shape[2] == old_tensor_shape[1], qkv_weights.shape\n\n    qkv_weights = qkv_weights.reshape([head_size * (head_num + 2 * num_query_groups), hidden_size])\n\n    return qkv_weights\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n    target_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n)\ndef _export_qkv(ctx: io.TransformCTX, linear_qkv):\n    megatron_config = ctx.source.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n    qkv_total_dim = head_num + 2 * num_query_groups\n\n    linear_qkv = linear_qkv.reshape([qkv_total_dim, head_size, hidden_size])\n    q_slice = torch.cat(\n        [\n            torch.arange((heads_per_group + 2) * i, (heads_per_group + 2) * i + heads_per_group)\n            for i in range(num_query_groups)\n        ]\n    )\n    k_slice = torch.arange(heads_per_group, qkv_total_dim, (heads_per_group + 2))\n    v_slice = torch.arange(heads_per_group + 1, qkv_total_dim, (heads_per_group + 2))\n\n    q_proj = linear_qkv[q_slice].reshape(-1, hidden_size).cpu()\n    k_proj = linear_qkv[k_slice].reshape(-1, hidden_size).cpu()\n    v_proj = linear_qkv[v_slice].reshape(-1, hidden_size).cpu()\n\n    return q_proj, k_proj, v_proj\n\n\n@io.state_transform(\n    source_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n    target_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n)\ndef _import_linear_fc1(down, gate):\n    return torch.cat((down, gate), axis=0).float()\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n    target_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n)\ndef _export_linear_fc1(linear_fc1):\n    gate_proj, up_proj = torch.chunk(linear_fc1, 2, dim=0)\n\n    return gate_proj, up_proj\n\n\n__all__ = [\n    \"GemmaConfig\",\n    \"GemmaConfig2B\",\n    \"GemmaConfig7B\",\n    \"CodeGemmaConfig2B\",\n    \"CodeGemmaConfig7B\",\n    \"GemmaModel\",\n]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/gemma.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:56:42.638854Z","iopub.execute_input":"2024-07-02T19:56:42.639209Z","iopub.status.idle":"2024-07-02T19:56:42.654526Z","shell.execute_reply.started":"2024-07-02T19:56:42.639184Z","shell.execute_reply":"2024-07-02T19:56:42.653629Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"write = '''\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Callable, List, Optional\n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn.functional as F\nfrom typing_extensions import Annotated\n\nfrom nemo.collections.llm.gpt.model.base import GPTConfig, GPTModel\nfrom nemo.collections.llm.utils import Config\nfrom nemo.lightning import io, teardown\nfrom nemo.lightning.pytorch.optim import MyOptimizerModule\n\nif TYPE_CHECKING:\n    from transformers import MistralConfig, MistralForCausalLM\n\n    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n    from nemo.collections.common.tokenizers.tokenizer_spec import TokenizerSpec\n\n\n@dataclass\nclass MistralConfig7B(GPTConfig):\n    normalization: str = \"RMSNorm\"\n    activation_func: Callable = F.silu\n    position_embedding_type: str = \"rope\"\n    add_bias_linear: bool = False\n    gated_linear_unit: bool = True\n    apply_query_key_layer_scaling: bool = False  # TODO: Should this be True?\n\n    num_layers: int = 32\n    hidden_size: int = 4096\n    num_attention_heads: int = 32\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 14336\n    seq_length: int = 32768\n\n    init_method_std: float = 0.02\n    layernorm_epsilon: float = 1e-5\n    window_size: List[int] = field(default_factory=lambda: [4096, 0])\n\n\nclass MistralModel(GPTModel):\n    def __init__(\n        self,\n        config: Annotated[Optional[MistralConfig7B], Config[MistralConfig7B]] = None,\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__(config or MistralConfig7B(), optim=optim, tokenizer=tokenizer)\n\n\n@io.model_importer(MistralModel, \"hf\")\nclass HFMistralImporter(io.ModelConnector[\"MistralForCausalLM\", MistralModel]):\n    def init(self) -> MistralModel:\n        return MistralModel(self.config, tokenizer=self.tokenizer)\n\n    def apply(self, output_path: Path) -> Path:\n        from transformers import MistralForCausalLM\n\n        source = MistralForCausalLM.from_pretrained(str(self))\n        target = self.init()\n        trainer = self.nemo_setup(target)\n        self.convert_state(source, target)\n        self.nemo_save(output_path, trainer)\n\n        print(f\"Converted Mistral 7B model to Nemo, model saved to {output_path}\")\n\n        teardown(trainer, target)\n        del trainer, target\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"model.embed_tokens.weight\": \"embedding.word_embeddings.weight\",\n            \"model.layers.*.self_attn.o_proj.weight\": \"decoder.layers.*.self_attention.linear_proj.weight\",\n            \"model.layers.*.mlp.down_proj.weight\": \"decoder.layers.*.mlp.linear_fc2.weight\",\n            \"model.layers.*.input_layernorm.weight\": \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n            \"model.layers.*.post_attention_layernorm.weight\": \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\",\n            \"model.norm.weight\": \"decoder.final_layernorm.weight\",\n            \"lm_head.weight\": \"output_layer.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_import_qkv, _import_linear_fc1])\n\n    @property\n    def tokenizer(self) -> \"AutoTokenizer\":\n        from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n        return AutoTokenizer(str(self))\n\n    @property\n    def config(self) -> MistralConfig7B:\n        from transformers import MistralConfig\n\n        source = MistralConfig.from_pretrained(str(self))\n\n        def make_vocab_size_divisible_by(mistral_vocab_size):\n            base = 128\n            while mistral_vocab_size % base != 0:\n                base //= 2\n            return base\n\n        output = MistralConfig7B(\n            seq_length=source.sliding_window,\n            num_layers=source.num_hidden_layers,\n            hidden_size=source.hidden_size,\n            ffn_hidden_size=source.intermediate_size,\n            num_attention_heads=source.num_attention_heads,\n            # max_position_embeddings=source.max_position_embeddings,\n            init_method_std=source.initializer_range,\n            layernorm_epsilon=source.rms_norm_eps,\n            num_query_groups=source.num_key_value_heads,\n            rotary_base=source.rope_theta,\n            gated_linear_unit=True,\n            make_vocab_size_divisible_by=make_vocab_size_divisible_by(source.vocab_size),\n            window_size=[source.sliding_window, 0],\n            share_embeddings_and_output_weights=False,\n        )\n\n        return output\n\n\n@io.model_exporter(MistralModel, \"hf\")\nclass HFMistralExporter(io.ModelConnector[MistralModel, \"MistralForCausalLM\"]):\n    def init(self) -> \"MistralForCausalLM\":\n        from transformers import AutoModelForCausalLM\n\n        return AutoModelForCausalLM.from_config(self.config)\n\n    def apply(self, output_path: Path) -> Path:\n        # TODO: Make it work with lazy init\n        # with torch.device(\"meta\"):\n        #     target = self.init()\n        target = self.init()\n        source, _ = self.nemo_load(str(self))\n        target = self.convert_state(source, target)\n\n        # TODO: Make sure we don't need to do this\n        target = target.cpu()\n        target.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"embedding.word_embeddings.weight\": \"model.embed_tokens.weight\",\n            \"decoder.layers.*.self_attention.linear_proj.weight\": \"model.layers.*.self_attn.o_proj.weight\",\n            \"decoder.layers.*.mlp.linear_fc2.weight\": \"model.layers.*.mlp.down_proj.weight\",\n            \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\": \"model.layers.*.input_layernorm.weight\",\n            \"decoder.layers.*.mlp.linear_fc1.layer_norm_weight\": \"model.layers.*.post_attention_layernorm.weight\",\n            \"decoder.final_layernorm.weight\": \"model.norm.weight\",\n            \"output_layer.weight\": \"lm_head.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_export_qkv, _export_linear_fc1])\n\n    @property\n    def tokenizer(self):\n        return io.load_ckpt(str(self)).model.tokenizer.tokenizer\n\n    @property\n    def config(self) -> \"MistralConfig\":\n        source: MistralConfig7B = io.load_ckpt(str(self)).model.config\n\n        from transformers import MistralConfig as HfMistralConfig\n\n        return HfMistralConfig(\n            sliding_window=source.window_size[0],\n            num_hidden_layers=source.num_layers,\n            hidden_size=source.hidden_size,\n            intermediate_size=source.ffn_hidden_size,\n            num_attention_heads=source.num_attention_heads,\n            max_position_embeddings=source.seq_length,\n            initializer_range=source.init_method_std,\n            rms_norm_eps=source.layernorm_epsilon,\n            num_key_value_heads=source.num_query_groups,\n            rope_theta=source.rotary_base,\n            vocab_size=self.tokenizer.vocab_size,\n        )\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n    target_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n)\ndef _import_qkv(ctx: io.TransformCTX, q, k, v):\n    megatron_config = ctx.target.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n\n    old_tensor_shape = q.size()\n    new_q_tensor_shape = (head_num, head_size) + old_tensor_shape[1:]\n    new_kv_tensor_shape = (num_query_groups, head_size) + old_tensor_shape[1:]\n\n    q = q.view(*new_q_tensor_shape)\n    k = k.view(*new_kv_tensor_shape)\n    v = v.view(*new_kv_tensor_shape)\n\n    qkv_weights_l = []\n    for i in range(num_query_groups):\n        qkv_weights_l.append(q[i * heads_per_group : (i + 1) * heads_per_group, :, :])\n        qkv_weights_l.append(k[i : i + 1, :, :])\n        qkv_weights_l.append(v[i : i + 1, :, :])\n    qkv_weights = torch.cat(qkv_weights_l)\n    assert qkv_weights.ndim == 3, qkv_weights.shape\n    assert qkv_weights.shape[0] == (heads_per_group + 2) * num_query_groups, qkv_weights.shape\n    assert qkv_weights.shape[1] == head_size, qkv_weights.shape\n    assert qkv_weights.shape[2] == old_tensor_shape[1], qkv_weights.shape\n\n    qkv_weights = qkv_weights.reshape([head_size * (head_num + 2 * num_query_groups), hidden_size])\n\n    return qkv_weights\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n    target_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n)\ndef _export_qkv(ctx: io.TransformCTX, linear_qkv):\n    megatron_config = ctx.source.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n    qkv_total_dim = head_num + 2 * num_query_groups\n\n    linear_qkv = linear_qkv.reshape([qkv_total_dim, head_size, hidden_size])\n    q_slice = torch.cat(\n        [\n            torch.arange((heads_per_group + 2) * i, (heads_per_group + 2) * i + heads_per_group)\n            for i in range(num_query_groups)\n        ]\n    )\n    k_slice = torch.arange(heads_per_group, qkv_total_dim, (heads_per_group + 2))\n    v_slice = torch.arange(heads_per_group + 1, qkv_total_dim, (heads_per_group + 2))\n\n    q_proj = linear_qkv[q_slice].reshape(-1, hidden_size).cpu()\n    k_proj = linear_qkv[k_slice].reshape(-1, hidden_size).cpu()\n    v_proj = linear_qkv[v_slice].reshape(-1, hidden_size).cpu()\n\n    return q_proj, k_proj, v_proj\n\n\n@io.state_transform(\n    source_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n    target_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n)\ndef _import_linear_fc1(down, gate):\n    return torch.cat((down, gate), axis=0).float()\n\n\n@io.state_transform(\n    source_key=\"decoder.layers.*.mlp.linear_fc1.weight\",\n    target_key=(\"model.layers.*.mlp.gate_proj.weight\", \"model.layers.*.mlp.up_proj.weight\"),\n)\ndef _export_linear_fc1(linear_fc1):\n    gate_proj, up_proj = torch.chunk(linear_fc1, 2, dim=0)\n\n    return gate_proj, up_proj\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/mistral.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:56:52.828969Z","iopub.execute_input":"2024-07-02T19:56:52.829296Z","iopub.status.idle":"2024-07-02T19:56:52.844669Z","shell.execute_reply.started":"2024-07-02T19:56:52.829271Z","shell.execute_reply":"2024-07-02T19:56:52.843532Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"write = '''\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Dict, Literal, Optional\n\nimport pytorch_lightning as L\nimport torch\nimport torch.distributed\nfrom megatron.core.optimizer import OptimizerConfig\nfrom megatron.core.transformer.transformer_config import TransformerConfig\n\nfrom nemo.collections.llm import fn\nfrom nemo.lightning import get_vocab_size, io\nfrom nemo.lightning.megatron_parallel import MaskedTokenLossReduction\nfrom nemo.lightning.pytorch.optim import MegatronOptimizerModule, MyOptimizerModule\n\nif TYPE_CHECKING:\n    from megatron.core.models.gpt.gpt_model import GPTModel as MCoreGPTModel\n\n    from nemo.collections.common.tokenizers.tokenizer_spec import TokenizerSpec\n\n\n@dataclass\nclass GPTConfig(TransformerConfig, io.IOMixin):\n    # From megatron.core.models.gpt.gpt_model.GPTModel\n    fp16_lm_cross_entropy: bool = False\n    parallel_output: bool = True\n    share_embeddings_and_output_weights: bool = True\n    make_vocab_size_divisible_by: int = 128\n    position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\"\n    rotary_base: int = 10000\n    rotary_percent: float = 1.0\n    seq_len_interpolation_factor: Optional[float] = None\n    seq_length: int = 1024\n\n    # TODO: Move this to better places?\n    get_attention_mask_from_fusion: bool = False\n\n    def configure_model(self, tokenizer) -> \"MCoreGPTModel\":\n        vp_size = self.virtual_pipeline_model_parallel_size\n        if vp_size:\n            p_size = self.pipeline_model_parallel_size\n            assert (\n                self.num_layers // p_size\n            ) % vp_size == 0, \"Make sure the number of model chunks is the same across all pipeline stages.\"\n\n        from megatron.core import parallel_state\n        from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_with_transformer_engine_spec\n        from megatron.core.models.gpt.gpt_model import GPTModel as MCoreGPTModel\n\n        return MCoreGPTModel(\n            self,\n            transformer_layer_spec=get_gpt_layer_with_transformer_engine_spec(self.num_moe_experts),\n            vocab_size=get_vocab_size(self, tokenizer.vocab_size, self.make_vocab_size_divisible_by),\n            max_sequence_length=self.seq_length,\n            fp16_lm_cross_entropy=self.fp16_lm_cross_entropy,\n            parallel_output=self.parallel_output,\n            share_embeddings_and_output_weights=self.share_embeddings_and_output_weights,\n            position_embedding_type=self.position_embedding_type,\n            rotary_percent=self.rotary_percent,\n            rotary_base=self.rotary_base,\n            seq_len_interpolation_factor=self.seq_len_interpolation_factor,\n            pre_process=parallel_state.is_pipeline_first_stage(),\n            post_process=parallel_state.is_pipeline_last_stage(),\n        )\n\n\nclass GPTModel(L.LightningModule, io.IOMixin, io.ConnectorMixin, fn.FNMixin):\n    def __init__(\n        self,\n        config: GPTConfig,\n        # TODO: Add transformer_layer_spec when we update mcore\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.tokenizer = tokenizer\n        self.optim = optim or MegatronOptimizerModule(config=OptimizerConfig(lr=1e-4, use_distributed_optimizer=True))\n        self.optim.connect(self)  # This will bind the `configure_optimizers` method\n\n    def configure_model(self) -> None:\n        if not hasattr(self, \"module\"):\n            self.module = self.config.configure_model(self.tokenizer)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        labels: Optional[torch.Tensor] = None,\n        decoder_input: Optional[torch.Tensor] = None,\n        inference_params=None,\n    ) -> torch.Tensor:\n        output_tensor = self.module(\n            input_ids,\n            position_ids,\n            attention_mask,\n            decoder_input=decoder_input,\n            labels=labels,\n            inference_params=inference_params,\n        )\n\n        return output_tensor\n\n    def data_step(self, dataloader_iter) -> Dict[str, torch.Tensor]:\n        return gpt_data_step(dataloader_iter)\n\n    def forward_step(self, batch) -> torch.Tensor:\n        return gpt_forward_step(self, batch)\n\n    def training_step(self, batch, batch_idx=None) -> torch.Tensor:\n        # In mcore the loss-function is part of the forward-pass (when labels are provided)\n\n        return self.forward_step(batch)\n\n    def validation_step(self, batch, batch_idx=None) -> torch.Tensor:\n        # In mcore the loss-function is part of the forward-pass (when labels are provided)\n\n        return self.forward_step(batch)\n\n    def training_loss_reduction(self) -> MaskedTokenLossReduction:\n        return MaskedTokenLossReduction()\n\n    def validation_loss_reduction(self) -> MaskedTokenLossReduction:\n        return MaskedTokenLossReduction(validation_step=True)\n\n\ndef gpt_data_step(dataloader_iter) -> Dict[str, torch.Tensor]:\n    from megatron.core import parallel_state\n\n    # Based on: https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py#L87\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L828-L842\n\n    batch = next(dataloader_iter)\n\n    _batch: dict\n    if isinstance(batch, tuple) and len(batch) == 3:\n        _batch = batch[0]\n    else:\n        _batch = batch\n\n    required_keys = set()\n    required_keys.add(\"attention_mask\")\n    if parallel_state.is_pipeline_first_stage():\n        required_keys.update((\"tokens\", \"position_ids\"))\n    if parallel_state.is_pipeline_last_stage():\n        required_keys.update((\"labels\", \"loss_mask\"))\n    # if self.get_attention_mask_from_fusion:\n    #     required_keys.remove('attention_mask')\n\n    _batch = {key: val.cuda(non_blocking=True) if key in required_keys else None for key, val in _batch.items()}\n    # slice batch along sequence dimension for context parallelism\n    output = get_batch_on_this_context_parallel_rank(_batch)\n\n    return output\n\n\ndef gpt_forward_step(model, batch) -> torch.Tensor:\n    forward_args = {\n        \"input_ids\": batch[\"tokens\"],\n        \"position_ids\": batch[\"position_ids\"],\n        \"attention_mask\": batch[\"attention_mask\"],\n        \"labels\": batch[\"labels\"],\n    }\n\n    if 'cu_seqlens' in batch:\n        forward_args['packed_seq_params'] = get_packed_seq_params(batch)\n\n    return model(**forward_args)\n\n\ndef get_batch_on_this_context_parallel_rank(batch):\n    from megatron.core import parallel_state\n\n    if (cp_size := parallel_state.get_context_parallel_world_size()) > 1:\n        num_valid_tokens_in_ub = None\n        if 'loss_mask' in batch and batch['loss_mask'] is not None:\n            num_valid_tokens_in_ub = batch['loss_mask'].sum()\n\n        cp_rank = parallel_state.get_context_parallel_rank()\n        for key, val in batch.items():\n            if val is not None:\n                seq_dim = 1 if key != 'attention_mask' else 2\n                _val = val.view(\n                    *val.shape[0:seq_dim],\n                    2 * cp_size,\n                    val.shape[seq_dim] // (2 * cp_size),\n                    *val.shape[(seq_dim + 1) :],\n                )\n                index = torch.tensor([cp_rank, (2 * cp_size - cp_rank - 1)], device=\"cpu\", pin_memory=True).cuda(\n                    non_blocking=True\n                )\n                _val = _val.index_select(seq_dim, index)\n                _val = _val.view(*val.shape[0:seq_dim], -1, *_val.shape[(seq_dim + 2) :])\n                batch[key] = _val\n        batch['num_valid_tokens_in_ub'] = num_valid_tokens_in_ub\n    return batch\n\n\ndef get_packed_seq_params(batch):\n    from megatron.core.packed_seq_params import PackedSeqParams\n\n    cu_seqlens = batch['cu_seqlens'].squeeze()  # remove batch size dimension (mbs=1)\n    # remove -1 \"paddings\" added in collate_fn\n    if (cu_seqlens_argmin := batch.get('cu_seqlens_argmin', None)) is not None:\n        # pre-compute cu_seqlens_argmin in dataset class for perf\n        cu_seqlens = cu_seqlens[: cu_seqlens_argmin.item()]\n    else:\n        cu_seqlens = cu_seqlens[: torch.argmin(cu_seqlens)]\n\n    # pre-compute max_seqlens in dataset class for perf\n    max_seqlen = batch['max_seqlen'].squeeze() if 'max_seqlen' in batch else None\n\n    # these args are passed eventually into TEDotProductAttention.forward()\n    return PackedSeqParams(\n        cu_seqlens_q=cu_seqlens,\n        cu_seqlens_kv=cu_seqlens,\n        max_seqlen_q=max_seqlen,\n        max_seqlen_kv=max_seqlen,\n        qkv_format='thd',\n    )\n\n\n__all__ = [\"GPTModel\", \"GPTConfig\", \"gpt_data_step\", \"gpt_forward_step\"]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/base.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:56:59.448966Z","iopub.execute_input":"2024-07-02T19:56:59.449558Z","iopub.status.idle":"2024-07-02T19:56:59.466881Z","shell.execute_reply.started":"2024-07-02T19:56:59.449522Z","shell.execute_reply":"2024-07-02T19:56:59.465912Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"write = '''\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Callable, Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom nemo.collections.llm.gpt.model.base import GPTConfig, GPTModel\nfrom nemo.lightning import io, teardown\nfrom nemo.lightning.pytorch.optim import MyOptimizerModule\n\nif TYPE_CHECKING:\n    from transformers import MistralConfig, MistralForCausalLM\n\n    from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n\n@dataclass\nclass MixtralConfig8x7B(GPTConfig):\n    \"\"\"\n    Config for Mixtral-8x7B model\n    Official announcement: https://mistral.ai/news/mixtral-of-experts/\n    \"\"\"\n\n    normalization: str = \"RMSNorm\"\n    activation_func: Callable = F.silu\n    position_embedding_type: str = \"rope\"\n    add_bias_linear: bool = False\n    gated_linear_unit: bool = True\n    apply_query_key_layer_scaling: bool = False  # TODO: Should this be True?\n\n    num_layers: int = 32\n    hidden_size: int = 4096\n    num_attention_heads: int = 32\n    num_query_groups: int = 8\n    ffn_hidden_size: int = 14336\n    max_position_embeddings: int = 4096  # 32768\n    seq_length: int = 4096  # 32768\n    # MoE\n    num_moe_experts: int = 8\n    moe_router_topk: int = 1\n\n    init_method_std: float = 0.02\n    layernorm_epsilon: float = 1e-5\n    # rotary\n    rotary_percent: float = 0.5\n    rotary_base: float = 10000\n\n\nclass MixtralModel(GPTModel):\n    def __init__(\n        self,\n        config: Optional[MixtralConfig8x7B] = None,\n        optim: Optional[MyOptimizerModule] = None,\n        tokenizer: Optional[\"TokenizerSpec\"] = None,\n    ):\n        super().__init__(config or MixtralConfig8x7B(), optim=optim, tokenizer=tokenizer)\n\n\n@io.model_importer(MixtralModel, ext=\"hf\")\nclass HFMixtralImporter(io.ModelConnector[\"MixtralForCausalLM\", MixtralModel]):\n    def init(self) -> MixtralModel:\n        return MixtralModel(self.config, tokenizer=self.tokenizer)\n\n    def apply(self, output_path: Path) -> Path:\n        from transformers import MixtralForCausalLM\n\n        source = MixtralForCausalLM.from_pretrained(str(self))\n        target = self.init()\n        trainer = self.nemo_setup(target)\n        self.convert_state(source, target)\n        self.nemo_save(output_path, trainer)\n\n        teardown(trainer, target)\n        del trainer, target\n\n        return output_path\n\n    def convert_state(self, source, target):\n        mapping = {\n            \"model.embed_tokens.weight\": \"embedding.word_embeddings.weight\",\n            \"model.layers.*.self_attn.o_proj.weight\": \"decoder.layers.*.self_attention.linear_proj.weight\",\n            \"model.layers.*.input_layernorm.weight\": \"decoder.layers.*.self_attention.linear_qkv.layer_norm_weight\",\n            \"model.layers.*.post_attention_layernorm.weight\": \"decoder.layers.*.pre_mlp_layernorm.weight\",\n            # MoE\n            \"model.layers.*.block_sparse_moe.experts.*.w2.weight\": \"decoder.layers.*.mlp.experts.local_experts.*.linear_fc2.weight\",\n            \"model.layers.*.block_sparse_moe.gate.weight\": \"decoder.layers.*.mlp.router.weight\",\n            # lm-head\n            \"model.norm.weight\": \"decoder.final_layernorm.weight\",\n            \"lm_head.weight\": \"output_layer.weight\",\n        }\n\n        return io.apply_transforms(source, target, mapping=mapping, transforms=[_import_qkv, _import_moe_w1_w3])\n\n    @property\n    def tokenizer(self) -> \"AutoTokenizer\":\n        from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer\n\n        return AutoTokenizer(str(self))\n\n    @property\n    def config(self) -> MixtralConfig8x7B:\n        from transformers import MixtralConfig as HfMixtralConfig\n\n        config = HfMixtralConfig.from_pretrained(str(self))\n        return MixtralConfig8x7B(\n            activation_func=F.silu,\n            # network\n            num_layers=config.num_hidden_layers,\n            hidden_size=config.hidden_size,\n            ffn_hidden_size=config.intermediate_size,\n            max_position_embeddings=config.max_position_embeddings,  # TODO\n            seq_length=config.max_position_embeddings,\n            # RoPE\n            position_embedding_type='rope',\n            rotary_base=config.rope_theta,\n            # Transformer config\n            num_attention_heads=config.num_attention_heads,\n            num_query_groups=config.num_key_value_heads,\n            num_moe_experts=config.num_local_experts,\n            moe_router_topk=config.num_experts_per_tok,\n            # norm\n            normalization='RMSNorm',\n            layernorm_epsilon=config.rms_norm_eps,\n            # Init\n            init_method_std=config.initializer_range,\n            gated_linear_unit=True,\n            # Vocab\n            make_vocab_size_divisible_by=128,\n        )\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.self_attn.q_proj.weight\",\n        \"model.layers.*.self_attn.k_proj.weight\",\n        \"model.layers.*.self_attn.v_proj.weight\",\n    ),\n    target_key=\"decoder.layers.*.self_attention.linear_qkv.weight\",\n)\ndef _import_qkv(ctx: io.TransformCTX, q, k, v):\n    megatron_config = ctx.target.config\n\n    head_num = megatron_config.num_attention_heads\n    num_query_groups = megatron_config.num_query_groups\n    heads_per_group = head_num // num_query_groups\n    hidden_size = megatron_config.hidden_size\n    head_num = megatron_config.num_attention_heads\n    head_size = hidden_size // head_num\n\n    old_tensor_shape = q.size()\n    new_q_tensor_shape = (head_num, head_size) + old_tensor_shape[1:]\n    new_kv_tensor_shape = (num_query_groups, head_size) + old_tensor_shape[1:]\n\n    q = q.view(*new_q_tensor_shape)\n    k = k.view(*new_kv_tensor_shape)\n    v = v.view(*new_kv_tensor_shape)\n\n    qkv_weights_l = []\n    for i in range(num_query_groups):\n        qkv_weights_l.append(q[i * heads_per_group : (i + 1) * heads_per_group, :, :])\n        qkv_weights_l.append(k[i : i + 1, :, :])\n        qkv_weights_l.append(v[i : i + 1, :, :])\n    qkv_weights = torch.cat(qkv_weights_l)\n    assert qkv_weights.ndim == 3, qkv_weights.shape\n    assert qkv_weights.shape[0] == (heads_per_group + 2) * num_query_groups, qkv_weights.shape\n    assert qkv_weights.shape[1] == head_size, qkv_weights.shape\n    assert qkv_weights.shape[2] == old_tensor_shape[1], qkv_weights.shape\n\n    qkv_weights = qkv_weights.reshape([head_size * (head_num + 2 * num_query_groups), hidden_size])\n\n    return qkv_weights\n\n\n@io.state_transform(\n    source_key=(\n        \"model.layers.*.block_sparse_moe.experts.*.w1.weight\",\n        \"model.layers.*.block_sparse_moe.experts.*.w3.weight\",\n    ),\n    target_key=\"decoder.layers.*.mlp.experts.local_experts.*.linear_fc1.weight\",\n)\ndef _import_moe_w1_w3(gate_proj, up_proj):\n    return torch.cat((gate_proj, up_proj), axis=0)\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/collections/llm/gpt/model/mixtral.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:57:08.346025Z","iopub.execute_input":"2024-07-02T19:57:08.346409Z","iopub.status.idle":"2024-07-02T19:57:08.357507Z","shell.execute_reply.started":"2024-07-02T19:57:08.346383Z","shell.execute_reply":"2024-07-02T19:57:08.356631Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"write = '''from nemo.lightning.pytorch.optim.base import LRSchedulerModule, MyOptimizerModule\nfrom nemo.lightning.pytorch.optim.lr_scheduler import (\n    CosineAnnealingScheduler,\n    InverseSquareRootAnnealingScheduler,\n    NoamAnnealingScheduler,\n    NoamHoldAnnealingScheduler,\n    PolynomialDecayAnnealingScheduler,\n    PolynomialHoldDecayAnnealingScheduler,\n    SquareAnnealingScheduler,\n    SquareRootAnnealingScheduler,\n    T5InverseSquareRootAnnealingScheduler,\n    WarmupAnnealingScheduler,\n    WarmupHoldPolicyScheduler,\n    WarmupPolicyScheduler,\n)\nfrom nemo.lightning.pytorch.optim.megatron import MegatronOptimizerModule\n\n__all__ = [\n    \"MyOptimizerModule\",\n    \"LRSchedulerModule\",\n    \"MegatronOptimizerModule\",\n    \"WarmupPolicyScheduler\",\n    \"WarmupHoldPolicyScheduler\",\n    \"SquareAnnealingScheduler\",\n    \"SquareRootAnnealingScheduler\",\n    \"NoamAnnealingScheduler\",\n    \"NoamHoldAnnealingScheduler\",\n    \"WarmupAnnealingScheduler\",\n    \"InverseSquareRootAnnealingScheduler\",\n    \"T5InverseSquareRootAnnealingScheduler\",\n    \"PolynomialDecayAnnealingScheduler\",\n    \"PolynomialHoldDecayAnnealingScheduler\",\n    \"CosineAnnealingScheduler\",\n]\n        '''\n\nwith open(\"/kaggle/working/NeMo/nemo/lightning/pytorch/optim/__init__.py\", \"w\") as file:\n    file.write(write)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T19:57:15.405596Z","iopub.execute_input":"2024-07-02T19:57:15.405986Z","iopub.status.idle":"2024-07-02T19:57:15.412164Z","shell.execute_reply.started":"2024-07-02T19:57:15.405960Z","shell.execute_reply":"2024-07-02T19:57:15.411238Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"!ls \"/kaggle/working/results/Some name of our experiment/checkpoints\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:25:03.984058Z","iopub.execute_input":"2024-07-02T20:25:03.984408Z","iopub.status.idle":"2024-07-02T20:25:04.937036Z","shell.execute_reply.started":"2024-07-02T20:25:03.984375Z","shell.execute_reply":"2024-07-02T20:25:04.936079Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"'Some name of our experiment--val_wer=0.4855-epoch=38.ckpt'\n'Some name of our experiment--val_wer=0.4943-epoch=36.ckpt'\n'Some name of our experiment--val_wer=0.4971-epoch=28.ckpt'\n'Some name of our experiment--val_wer=0.4992-epoch=40.ckpt'\n'Some name of our experiment--val_wer=0.5496-epoch=35.ckpt'\n'Some name of our experiment--val_wer=0.5661-epoch=37.ckpt'\n'Some name of our experiment--val_wer=0.5668-epoch=38.ckpt'\n'Some name of our experiment--val_wer=0.5694-epoch=39.ckpt'\n'Some name of our experiment.nemo'\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -r \"/kaggle/working/results/Some name of our experiment/checkpoints/Some name of our experiment--val_wer=0.5694-epoch=39-last.ckpt\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:25:02.225297Z","iopub.execute_input":"2024-07-02T20:25:02.226026Z","iopub.status.idle":"2024-07-02T20:25:03.394336Z","shell.execute_reply.started":"2024-07-02T20:25:02.225987Z","shell.execute_reply":"2024-07-02T20:25:03.393093Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"!mv \"/kaggle/working/results/Some name of our experiment/checkpoints/Some name of our experiment--val_wer=0.4992-epoch=40.ckpt\" \"/kaggle/working/results/Some name of our experiment/checkpoints/Some name of our experiment--val_wer=0.4992-epoch=40-last.ckpt\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:25:58.311743Z","iopub.execute_input":"2024-07-02T20:25:58.312700Z","iopub.status.idle":"2024-07-02T20:25:59.267748Z","shell.execute_reply.started":"2024-07-02T20:25:58.312657Z","shell.execute_reply":"2024-07-02T20:25:59.266532Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"params.model.train_ds.batch_size=4 # this set precision of the model to values [32,16]\nOmegaConf.save(params, \"/kaggle/working/NeMo/examples/asr/conf/conformer/conformer_ctc_bpe.yaml\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:26:01.264179Z","iopub.execute_input":"2024-07-02T20:26:01.264834Z","iopub.status.idle":"2024-07-02T20:26:01.286363Z","shell.execute_reply.started":"2024-07-02T20:26:01.264798Z","shell.execute_reply":"2024-07-02T20:26:01.285447Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"TOKENIZER='/kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128'\n#TRAIN_MANIFEST='/kaggle/input/aic-manifests/train_manifest.json'\n#VAL_MANIFEST='/kaggle/input/aic-manifests/test_manifest.json'\nNEMO_ROOT='/kaggle/working/NeMo'\n\n! HYDRA_FULL_ERROR=1 python /kaggle/working/NeMo/examples/asr/asr_ctc/speech_to_text_ctc_bpe.py \\\n  --config-path=../conf/conformer/ \\\n  --config-name=conformer_ctc_bpe \\\n  exp_manager.name=\"Some name of our experiment\" \\\n  exp_manager.resume_if_exists=true \\\n  exp_manager.resume_ignore_no_checkpoint=true \\\n  exp_manager.exp_dir=results/ \\\n  model.tokenizer.dir=$TOKENIZER \\\n  model.train_ds.manifest_filepath=$TRAIN_MANIFEST \\\n  model.validation_ds.manifest_filepath=$VAL_MANIFEST \\\n#  exp_manager.create_wandb_logger=True \\\n#  exp_manager.wandb_logger_kwargs.project=\"'ASR'\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T20:49:05.030327Z","iopub.execute_input":"2024-07-02T20:49:05.030829Z","iopub.status.idle":"2024-07-02T20:50:00.205766Z","shell.execute_reply.started":"2024-07-02T20:49:05.030796Z","shell.execute_reply":"2024-07-02T20:50:00.204183Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"[NeMo W 2024-07-02 20:49:17 nemo_logging:349] /opt/conda/lib/python3.10/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n      ret = run_job(\n    \n[NeMo I 2024-07-02 20:49:17 speech_to_text_ctc_bpe:78] Hydra config: name: Conformer-CTC-BPE\n    model:\n      sample_rate: 16000\n      log_prediction: true\n      ctc_reduction: mean_batch\n      skip_nan_grad: false\n      train_ds:\n        manifest_filepath: /kaggle/input/adapt-split-manifest/data_85.json\n        sample_rate: ${model.sample_rate}\n        batch_size: 4\n        shuffle: true\n        num_workers: 8\n        pin_memory: true\n        max_duration: 16.7\n        min_duration: 0.1\n        is_tarred: false\n        tarred_audio_filepaths: null\n        shuffle_n: 2048\n        bucketing_strategy: synced_randomized\n        bucketing_batch_size: null\n      validation_ds:\n        manifest_filepath: /kaggle/input/adapt-split-manifest/data_15.json\n        sample_rate: ${model.sample_rate}\n        batch_size: 16\n        shuffle: false\n        use_start_end_token: false\n        num_workers: 8\n        pin_memory: true\n      test_ds:\n        manifest_filepath: null\n        sample_rate: ${model.sample_rate}\n        batch_size: 16\n        shuffle: false\n        use_start_end_token: false\n        num_workers: 8\n        pin_memory: true\n      tokenizer:\n        dir: /kaggle/working/tokinzers/sus/tokenizer_spe_unigram_v128\n        type: bpe\n      preprocessor:\n        _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n        sample_rate: ${model.sample_rate}\n        normalize: per_feature\n        window_size: 0.025\n        window_stride: 0.01\n        window: hann\n        features: 80\n        n_fft: 512\n        log: true\n        frame_splicing: 1\n        dither: 1.0e-05\n        pad_to: 0\n        pad_value: 0.0\n      spec_augment:\n        _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n        freq_masks: 2\n        time_masks: 10\n        freq_width: 27\n        time_width: 0.05\n      encoder:\n        _target_: nemo.collections.asr.modules.ConformerEncoder\n        feat_in: ${model.preprocessor.features}\n        feat_out: -1\n        n_layers: 18\n        d_model: 512\n        subsampling: striding\n        subsampling_factor: 4\n        subsampling_conv_channels: -1\n        causal_downsampling: false\n        ff_expansion_factor: 4\n        self_attention_model: rel_pos\n        n_heads: 8\n        att_context_size:\n        - -1\n        - -1\n        att_context_style: regular\n        xscaling: true\n        untie_biases: true\n        pos_emb_max_len: 5000\n        conv_kernel_size: 31\n        conv_norm_type: batch_norm\n        conv_context_size: null\n        dropout: 0.2\n        dropout_pre_encoder: 0.1\n        dropout_emb: 0.1\n        dropout_att: 0.1\n        stochastic_depth_drop_prob: 0.0\n        stochastic_depth_mode: linear\n        stochastic_depth_start_layer: 1\n      decoder:\n        _target_: nemo.collections.asr.modules.ConvASRDecoder\n        feat_in: null\n        num_classes: -1\n        vocabulary: []\n      interctc:\n        loss_weights: []\n        apply_at_layers: []\n      optim:\n        name: adamw\n        lr: 2.0\n        betas:\n        - 0.9\n        - 0.98\n        weight_decay: 0.001\n        sched:\n          name: NoamAnnealing\n          d_model: ${model.encoder.d_model}\n          warmup_steps: 10000\n          warmup_ratio: null\n          min_lr: 1.0e-06\n    trainer:\n      devices: -1\n      num_nodes: 1\n      max_epochs: 1000\n      max_steps: -1\n      val_check_interval: 1.0\n      accelerator: auto\n      strategy: ddp\n      accumulate_grad_batches: 1\n      gradient_clip_val: 0.0\n      precision: 32\n      log_every_n_steps: 10\n      enable_progress_bar: true\n      num_sanity_val_steps: 0\n      check_val_every_n_epoch: 1\n      sync_batchnorm: true\n      enable_checkpointing: false\n      logger: false\n      benchmark: false\n    exp_manager:\n      exp_dir: results/\n      name: Some name of our experiment\n      create_tensorboard_logger: true\n      create_checkpoint_callback: true\n      checkpoint_callback_params:\n        monitor: val_wer\n        mode: min\n        save_top_k: 5\n        always_save_nemo: true\n      resume_if_exists: true\n      resume_ignore_no_checkpoint: true\n      create_wandb_logger: false\n      wandb_logger_kwargs:\n        name: null\n        project: null\n    \nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n[NeMo W 2024-07-02 20:49:18 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n[NeMo W 2024-07-02 20:49:18 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :results/Some name of our experiment/checkpoints. Training from scratch.\n[NeMo I 2024-07-02 20:49:18 exp_manager:396] Experiments will be logged at results/Some name of our experiment\n[NeMo I 2024-07-02 20:49:18 exp_manager:856] TensorboardLogger has been set up\n[NeMo I 2024-07-02 20:49:18 mixins:172] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n[NeMo I 2024-07-02 20:49:18 ctc_bpe_models:68] \n    Replacing placeholder number of classes (-1) with actual number of classes - 128\n[NeMo I 2024-07-02 20:49:19 collections:196] Dataset loaded with 1779 files totalling 2.04 hours\n[NeMo I 2024-07-02 20:49:19 collections:197] 89 files were filtered totalling 0.53 hours\n[NeMo W 2024-07-02 20:49:19 nemo_logging:349] /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n      warnings.warn(_create_warning_msg(\n    \n[NeMo I 2024-07-02 20:49:19 collections:196] Dataset loaded with 331 files totalling 0.42 hours\n[NeMo I 2024-07-02 20:49:19 collections:197] 0 files were filtered totalling 0.00 hours\n[NeMo W 2024-07-02 20:49:19 audio_to_text_dataset:830] Could not load dataset as `manifest_filepath` was None. Provided config : {'manifest_filepath': None, 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n[NeMo I 2024-07-02 20:49:19 features:305] PADDING: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\n2024-07-02 20:49:43.329573: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-02 20:49:43.329651: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-02 20:49:43.331240: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[NeMo I 2024-07-02 20:49:45 modelPT:770] Optimizer config = AdamW (\n    Parameter Group 0\n        amsgrad: False\n        betas: [0.9, 0.98]\n        capturable: False\n        differentiable: False\n        eps: 1e-08\n        foreach: None\n        fused: None\n        lr: 2.0\n        maximize: False\n        weight_decay: 0.001\n    )\n[NeMo I 2024-07-02 20:49:45 lr_scheduler:923] Scheduler \"<nemo.core.optim.lr_scheduler.NoamAnnealing object at 0x7dd5e7813c10>\" \n    will be used during training (effective maximum steps = 223000) - \n    Parameters : \n    (d_model: 512\n    warmup_steps: 10000\n    warmup_ratio: null\n    min_lr: 1.0e-06\n    max_steps: 223000\n    )\n\n  | Name              | Type                              | Params\n------------------------------------------------------------------------\n0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n1 | encoder           | ConformerEncoder                  | 121 M \n2 | decoder           | ConvASRDecoder                    | 66.2 K\n3 | loss              | CTCLoss                           | 0     \n4 | spec_augmentation | SpectrogramAugmentation           | 0     \n5 | wer               | WER                               | 0     \n------------------------------------------------------------------------\n121 M     Trainable params\n0         Non-trainable params\n121 M     Total params\n486.005   Total estimated model params size (MB)\n[NeMo W 2024-07-02 20:49:45 nemo_logging:349] /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n      warnings.warn(_create_warning_msg(\n    \n[NeMo W 2024-07-02 20:49:45 nemo_logging:349] /opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n      self.pid = os.fork()\n    \nEpoch 0:   4%| | 9/223 [00:06<02:26,  1.46it/s, v_num=0, train_step_timing in s=[NeMo I 2024-07-02 20:49:52 wer:334] \n    \n[NeMo I 2024-07-02 20:49:52 wer:335] reference:طب لو قولت يا عم الحج\n[NeMo I 2024-07-02 20:49:52 wer:336] predicted:جدا أين ماخ نٱe> نز> أ إيهها حط ح جدا جو ح ك ح[ ح\nEpoch 0:   9%| | 19/223 [00:12<02:09,  1.57it/s, v_num=0, train_step_timing in s[NeMo I 2024-07-02 20:49:58 wer:334] \n    \n[NeMo I 2024-07-02 20:49:58 wer:335] reference:غصب عن عنيك الجوز على دماغك هترفعني أنا الناس اللي عاشت تحت خط الفقر وانا الناس اللي ماتت ملتجتش القبر فسيبك من اللي بيجاملك وبيهيص\n[NeMo I 2024-07-02 20:49:58 wer:336] predicted:لب ينح لم لخ إنينب لجطp ل خطؤ لم لينطةط ل فين أت لض حين لم لين تت لة١ لينم ل  ج ل حم ك÷ لت لطاء ح ق أٱ ل حطح حوم١ ح لٱ لط ل ق حط وطضينٱينلا ل[ط لٱ لو بتت لطة ل لطين إمةين ل  ع قطصواينطتمة حطصين ح إم ٱ لتطإ ل خ ل[ط ل خط ل  لطِينط ل  ل خينِ ل? لم ح ل أ لطٱ  لu ل  لe ل ما لنا دي ل فجح١ لط ح لo ج ل ح لeة أنا كينصط جدا لم ل ج ما لت ل  ل  ل  لe لب لينوط لجiينهاذة لأ ف أ لط لص ل ف لط لد ف ج لاءمط ليند ل ك ل مع ل خeت ل فٱ لط لحط ق ل?ٱت ل حم ل حاء\nEpoch 0:   9%| | 21/223 [00:13<02:06,  1.60it/s, v_num=0, train_step_timing in s^C\n[NeMo W 2024-07-02 20:49:59 nemo_logging:349] /opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n    \n","output_type":"stream"}]},{"cell_type":"code","source":"TRAIN_MANIFEST='/kaggle/input/adapt-split-manifest/data_85.json'\nVAL_MANIFEST='/kaggle/input/adapt-split-manifest/data_15.json'","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T20:13:14.895148Z","iopub.execute_input":"2024-07-02T20:13:14.895780Z","iopub.status.idle":"2024-07-02T20:13:14.899819Z","shell.execute_reply.started":"2024-07-02T20:13:14.895750Z","shell.execute_reply":"2024-07-02T20:13:14.898895Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = '/kaggle/working/results/Some name of our experiment/checkpoints/Some name of our experiment.nemo'","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:44:53.879697Z","iopub.execute_input":"2024-07-02T20:44:53.880063Z","iopub.status.idle":"2024-07-02T20:44:53.885194Z","shell.execute_reply.started":"2024-07-02T20:44:53.880036Z","shell.execute_reply":"2024-07-02T20:44:53.883954Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"first_asr_model = nemo_asr.models.EncDecCTCModelBPE.restore_from(checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:44:55.150033Z","iopub.execute_input":"2024-07-02T20:44:55.150410Z","iopub.status.idle":"2024-07-02T20:44:58.001073Z","shell.execute_reply.started":"2024-07-02T20:44:55.150380Z","shell.execute_reply":"2024-07-02T20:44:58.000118Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"[NeMo I 2024-07-02 20:44:55 mixins:172] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n","output_type":"stream"},{"name":"stderr","text":"[NeMo W 2024-07-02 20:44:56 modelPT:176] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n    Train config : \n    manifest_filepath: /kaggle/input/adapt-split-manifest/data_85.json\n    sample_rate: 16000\n    batch_size: 4\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    max_duration: 16.7\n    min_duration: 0.1\n    is_tarred: false\n    tarred_audio_filepaths: null\n    shuffle_n: 2048\n    bucketing_strategy: synced_randomized\n    bucketing_batch_size: null\n    \n[NeMo W 2024-07-02 20:44:56 modelPT:183] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n    Validation config : \n    manifest_filepath: /kaggle/input/adapt-split-manifest/data_15.json\n    sample_rate: 16000\n    batch_size: 16\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n    \n[NeMo W 2024-07-02 20:44:56 modelPT:189] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n    Test config : \n    manifest_filepath: null\n    sample_rate: 16000\n    batch_size: 16\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n    \n","output_type":"stream"},{"name":"stdout","text":"[NeMo I 2024-07-02 20:44:56 features:305] PADDING: 0\n[NeMo I 2024-07-02 20:44:57 save_restore_connector:263] Model EncDecCTCModelBPE was successfully restored from /kaggle/working/results/Some name of our experiment/checkpoints/Some name of our experiment.nemo.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport csv\n\n# Placeholder for your ASR model initialization\n# first_asr_model = YourASRModel()\n\n# Path to the directory containing .wav files\ndata_dir = '/kaggle/input/test-data/test'\n\n# List all .wav files in the directory\nwav_files = [f for f in os.listdir(data_dir) if f.endswith('.wav')]\n\n# Prepare the list of audio paths\naudio_paths = [os.path.join(data_dir, wav) for wav in wav_files]\n\n# Transcribe the audio files in batches (assuming batch_size=4)\nbatch_size = 4\ntranscriptions = []\n\nfor i in range(0, len(audio_paths), batch_size):\n    batch_paths = audio_paths[i:i + batch_size]\n    transcripts = first_asr_model.transcribe(audio=batch_paths, batch_size=len(batch_paths))\n    transcriptions.extend(transcripts)\n\n# Prepare data for CSV\ncsv_data = []\nfor wav, transcript in zip(wav_files, transcriptions):\n    audio_name = os.path.splitext(wav)[0]\n    csv_data.append([audio_name, transcript])\n\n# Write to CSV\ncsv_file = 'transcriptions.csv'\nwith open(csv_file, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['audio', 'transcript'])\n    writer.writerows(csv_data)\n\nprint(f\"Transcriptions saved to {csv_file}\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T20:44:58.070430Z","iopub.execute_input":"2024-07-02T20:44:58.071133Z","iopub.status.idle":"2024-07-02T20:45:48.805753Z","shell.execute_reply.started":"2024-07-02T20:44:58.071105Z","shell.execute_reply":"2024-07-02T20:45:48.804143Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"Transcribing: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.93it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.73it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.42it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.01it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.60it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.04it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.28it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 16.87it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.42it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 18.84it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.19it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.01it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.33it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.65it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.66it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.48it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.31it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.23it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.03it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 18.20it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.97it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.27it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.86it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.68it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.97it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 22.14it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.19it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.94it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.54it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.02it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.32it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 15.48it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 17.21it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 16.68it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 11.63it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.69it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.49it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.33it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 18.75it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.86it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.10it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.15it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.12it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 22.17it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.95it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.24it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.14it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.55it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.65it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.38it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.20it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.32it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.14it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.08it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.25it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.43it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.55it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.81it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 18.56it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.11it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.96it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.74it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.77it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.12it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.22it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.41it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 12.99it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.41it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.37it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.49it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.76it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.67it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 16.16it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.41it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.86it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.47it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.12it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.10it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.06it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.61it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.38it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.96it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.76it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.16it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.81it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.69it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 22.28it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.20it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.95it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.06it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.95it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.83it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 17.11it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.85it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.54it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 18.40it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.01it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.86it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.83it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.30it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.49it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.24it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.60it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.14it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 11.02it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.51it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.55it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.24it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.19it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.55it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 15.84it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.29it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.05it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.13it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.58it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.17it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  5.80it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.67it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 17.77it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 10.15it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.50it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.54it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 17.29it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 14.62it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 19.12it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 13.15it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 21.47it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00, 20.18it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nTranscribing: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\nTranscribing:   0%|          | 0/1 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[77], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(audio_paths), batch_size):\n\u001b[1;32m     21\u001b[0m     batch_paths \u001b[38;5;241m=\u001b[39m audio_paths[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 22\u001b[0m     transcripts \u001b[38;5;241m=\u001b[39m \u001b[43mfirst_asr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     transcriptions\u001b[38;5;241m.\u001b[39mextend(transcripts)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Prepare data for CSV\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nemo/collections/asr/models/ctc_models.py:158\u001b[0m, in \u001b[0;36mEncDecCTCModel.transcribe\u001b[0;34m(self, audio, batch_size, return_hypotheses, num_workers, channel_selector, augmentor, verbose, override_config)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     audio: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor, np\u001b[38;5;241m.\u001b[39mndarray, DataLoader],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     override_config: Optional[TranscribeConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TranscriptionReturnType:\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    If modify this function, please remember update transcribe_partial_audio() in\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    nemo/collections/asr/parts/utils/trancribe_utils.py\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m        A list of transcriptions (or raw log probabilities if logprobs is True) in the same order as paths2audio_files\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_hypotheses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_hypotheses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchannel_selector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannel_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43maugmentor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmentor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nemo/collections/asr/parts/mixins/transcription.py:277\u001b[0m, in \u001b[0;36mTranscriptionMixin.transcribe\u001b[0;34m(self, audio, batch_size, return_hypotheses, num_workers, channel_selector, augmentor, verbose, override_config, **config_kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscribe_generator(audio, override_config\u001b[38;5;241m=\u001b[39mtranscribe_cfg)\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m processed_outputs \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;66;03m# Create a results of the same type as each element in processed_outputs\u001b[39;00m\n\u001b[1;32m    281\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nemo/collections/asr/parts/mixins/transcription.py:389\u001b[0m, in \u001b[0;36mTranscriptionMixin.transcribe_generator\u001b[0;34m(self, audio, override_config)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# Run forward pass\u001b[39;00m\n\u001b[1;32m    388\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transcribe_forward(test_batch, transcribe_cfg)\n\u001b[0;32m--> 389\u001b[0m processed_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transcribe_output_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscribe_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# clear up memory\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m test_batch, model_outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nemo/collections/asr/models/ctc_models.py:692\u001b[0m, in \u001b[0;36mEncDecCTCModel._transcribe_output_processing\u001b[0;34m(self, outputs, trcfg)\u001b[0m\n\u001b[1;32m    689\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    690\u001b[0m logits_len \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits_len\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 692\u001b[0m current_hypotheses, all_hyp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_decoder_predictions_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_hypotheses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_hypotheses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trcfg\u001b[38;5;241m.\u001b[39mreturn_hypotheses:\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# See comment in\u001b[39;00m\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;66;03m# ctc_greedy_decoding.py::GreedyCTCInfer::forward() to\u001b[39;00m\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# understand this idiom.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/ctc_decoding.py:369\u001b[0m, in \u001b[0;36mAbstractCTCDecoding.ctc_decoder_predictions_tensor\u001b[0;34m(self, decoder_outputs, decoder_lengths, fold_consecutive, return_hypotheses)\u001b[0m\n\u001b[1;32m    365\u001b[0m     fold_consecutive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoding\u001b[38;5;241m.\u001b[39moverride_fold_consecutive_value\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# Resolve the forward step of the decoding strategy\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     hypotheses_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_lengths\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: List[List[Hypothesis]]\u001b[39;00m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;66;03m# extract the hypotheses\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     hypotheses_list \u001b[38;5;241m=\u001b[39m hypotheses_list[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: List[Hypothesis]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/ctc_greedy_decoding.py:519\u001b[0m, in \u001b[0;36mGreedyBatchedCTCInfer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nemo/core/classes/common.py:1064\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m instance\u001b[38;5;241m.\u001b[39m_validate_input_types(input_types\u001b[38;5;241m=\u001b[39minput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[0;32m-> 1064\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m instance\u001b[38;5;241m.\u001b[39m_attach_and_validate_output_types(\n\u001b[1;32m   1067\u001b[0m     output_types\u001b[38;5;241m=\u001b[39moutput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, out_objects\u001b[38;5;241m=\u001b[39moutputs\n\u001b[1;32m   1068\u001b[0m )\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/ctc_greedy_decoding.py:412\u001b[0m, in \u001b[0;36mGreedyBatchedCTCInfer.forward\u001b[0;34m(self, decoder_output, decoder_lengths)\u001b[0m\n\u001b[1;32m    410\u001b[0m     hypotheses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_greedy_decode_labels_batched(decoder_output, decoder_lengths)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     hypotheses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_decode_logprobs_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m packed_result \u001b[38;5;241m=\u001b[39m pack_hypotheses(hypotheses, input_decoder_lengths)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (packed_result,)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/ctc_greedy_decoding.py:439\u001b[0m, in \u001b[0;36mGreedyBatchedCTCInfer._greedy_decode_logprobs_batched\u001b[0;34m(self, x, out_len)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Sum the non-blank labels to compute the score of the\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# transcription. This follows from Eq. (3) of \"Connectionist\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# Temporal Classification: Labelling Unsegmented Sequence Data\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# with Recurrent Neural Networks\".\u001b[39;00m\n\u001b[1;32m    437\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(non_blank_ids_mask, predictions_logprobs, \u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 439\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m predictions_labels \u001b[38;5;241m=\u001b[39m predictions_labels\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    441\u001b[0m out_len \u001b[38;5;241m=\u001b[39m out_len\u001b[38;5;241m.\u001b[39mcpu()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"len","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import Levenshtein as lev\nimport csv\n\n# Setup the test data loader and make sure the model is on GPU\nfirst_asr_model.setup_test_data(test_data_config=params)\nfirst_asr_model.cuda()\nfirst_asr_model.eval()\n\n# We remove some preprocessing artifacts which benefit training\nfirst_asr_model.preprocessor.featurizer.pad_to = 0\nfirst_asr_model.preprocessor.featurizer.dither = 0.0\n\n# Initialize a list to store audio file IDs and their transcriptions\npredictions = []\n\n# Function to convert token indices to strings using the model's tokenizer\ndef tokens_to_text(tokens, tokenizer):\n    texts = []\n    vocab_size = tokenizer.vocab_size  # Get the vocabulary size\n    for token_seq in tokens:\n        # Filter out-of-range token IDs\n        valid_tokens = [token for token in token_seq.tolist() if token < vocab_size]\n        text = tokenizer.ids_to_text(valid_tokens)\n        texts.append(text)\n    return texts\n\n# Loop over all test batches\nfor test_batch in first_asr_model.test_dataloader():\n    test_batch = [x.cuda() for x in test_batch]\n    audio_file_ids = test_batch[3]  # Assuming the audio file IDs are in the 5th position\n    \n    targets = test_batch[2]\n    targets_lengths = test_batch[3]\n    log_probs, encoded_len, greedy_predictions = first_asr_model(\n        input_signal=test_batch[0], input_signal_length=test_batch[1]\n    )\n\n    # Convert predictions to text\n    pred_texts = tokens_to_text(greedy_predictions, first_asr_model.tokenizer)\n    \n    # Collect predictions with their corresponding audio file IDs\n    for audio_file_id, pred_text in zip(audio_file_ids, pred_texts):\n        predictions.append((audio_file_id, pred_text))\n\n# Write the predictions to a CSV file\nwith open('submission.csv', mode='w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['audio', 'transcript'])\n    for audio_file_id, transcript in predictions:\n        writer.writerow([audio_file_id, transcript])\n\nprint(\"Submission file 'submission.csv' created successfully.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:51:01.824668Z","iopub.execute_input":"2024-07-02T20:51:01.825452Z","iopub.status.idle":"2024-07-02T20:52:31.397124Z","shell.execute_reply.started":"2024-07-02T20:51:01.825418Z","shell.execute_reply":"2024-07-02T20:52:31.396000Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"[NeMo I 2024-07-02 20:51:01 collections:196] Dataset loaded with 1726 files totalling 2.50 hours\n[NeMo I 2024-07-02 20:51:01 collections:197] 0 files were filtered totalling 0.00 hours\nSubmission file 'submission.csv' created successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"audio_file_path","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:47:54.551280Z","iopub.execute_input":"2024-07-02T20:47:54.551718Z","iopub.status.idle":"2024-07-02T20:47:54.558494Z","shell.execute_reply.started":"2024-07-02T20:47:54.551688Z","shell.execute_reply":"2024-07-02T20:47:54.557636Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"tensor(38784)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}